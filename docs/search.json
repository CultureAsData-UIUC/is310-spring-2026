[
  {
    "objectID": "assets/files/is310IntroTextAnalysis.html",
    "href": "assets/files/is310IntroTextAnalysis.html",
    "title": "Introduction to Text Analysis Notebook",
    "section": "",
    "text": "You are welcome to either work locally or in this Google Colab notebook. Below you will find code to replicate creating the dataset and some code to start you exploring NER and TF-IDF. Please reach out to the instructors if you have questions or concerns. To use this notebook, you will need to install the following packages:"
  },
  {
    "objectID": "assets/files/is310IntroTextAnalysis.html#install-and-import-libraries",
    "href": "assets/files/is310IntroTextAnalysis.html#install-and-import-libraries",
    "title": "Introduction to Text Analysis Notebook",
    "section": "Install and Import Libraries",
    "text": "Install and Import Libraries\n\n# Importing libraries\nimport spacy\nfrom spacy import displacy\nimport pandas as pd\nfrom tqdm import tqdm\nimport altair as alt\nimport gutenbergpy.textget\nimport requests\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Load Spacy's multilingual model (can be replaced with a larger model if needed)\nmulti_nlp = spacy.load('xx_ent_wiki_sm')\neng_nlp = spacy.load('en_core_web_sm')"
  },
  {
    "objectID": "assets/files/is310IntroTextAnalysis.html#load-datasets",
    "href": "assets/files/is310IntroTextAnalysis.html#load-datasets",
    "title": "Introduction to Text Analysis Notebook",
    "section": "Load Datasets",
    "text": "Load Datasets\nYou either have the option of using the premade dataset available in Google Drive (though you will need to change the path to the file) or running the code below to remake the dataset from scratch.\nBe warned, this file is quite large because of the size of the novels, so you may want to use a subset of the novels to test this code.\n\nGoogle Drive Dataset\nYou can download this dataset here https://drive.google.com/file/d/1LkaRtYph_lWtMPRyzZpECuzEMD3WPx26/view?usp=sharing and it’s very larger so make sure you don’t push it up to GitHub.\n\ncombined_novels_nyt_df = pd.read_csv(\"combined_novels_nyt_with_text.csv\")\n\n\n\nRerun Dataset Creation Code\n\nnovels_df = pd.read_csv(\"https://raw.githubusercontent.com/melaniewalsh/responsible-datasets-in-context/main/datasets/top-500-novels/library_top_500.csv\")\nnyt_bestsellers_df = pd.read_csv(\"https://raw.githubusercontent.com/ecds/post45-datasets/main/nyt_full.tsv\", sep='\\t')\ncombined_novels_nyt_df = novels_df.merge(nyt_bestsellers_df, how='left', on=['author', 'title'])\n\ndef get_text(url):\n    if pd.notna(url):\n        try:\n            response = requests.get(url)\n            if response.status_code == 200:\n                return response.text\n        except Exception as e:\n            return None\n            \n    return None\n\nfrom tqdm import tqdm\n\ntqdm.pandas(desc=\"Progress\")\ncombined_novels_nyt_df.loc[:, 'pg_eng_text'] = combined_novels_nyt_df.pg_eng_url.progress_apply(get_text)\ncombined_novels_nyt_df.loc[:, 'pg_orig_text'] = combined_novels_nyt_df['pg_orig_url'].progress_apply(get_text)\n\ncombined_novels_nyt_df['pg_eng_tokens'] = combined_novels_nyt_df['pg_eng_text'].str.split()\ncombined_novels_nyt_df['pg_orig_tokens'] = combined_novels_nyt_df['pg_orig_text'].str.split()\n\ncombined_novels_nyt_df['pg_eng_text_len'] = combined_novels_nyt_df.pg_eng_text.str.len()\ncombined_novels_nyt_df['pg_orig_text_len'] = combined_novels_nyt_df.pg_orig_text.str.len()\ncombined_novels_nyt_df['pg_eng_token_len'] = combined_novels_nyt_df.pg_eng_tokens.str.len()\ncombined_novels_nyt_df['pg_orig_token_len'] = combined_novels_nyt_df.pg_orig_tokens.str.len()\n\ndef clean_book(url):\n    # This gets a book by its gutenberg id number\n    if pd.notna(url):\n        pg_id = url.split('/pg')[-1].split('.')[0]\n        try:\n            raw_book = gutenbergpy.textget.get_text_by_id(pg_id) # with headers\n            clean_book = gutenbergpy.textget.strip_headers(raw_book) # without headers\n            return clean_book\n        except Exception as e:\n            return None\n\ncombined_novels_nyt_df.loc[:, 'cleaned_pg_eng_text'] = combined_novels_nyt_df.pg_eng_url.apply(clean_book)\ncombined_novels_nyt_df.loc[:, 'cleaned_pg_orig_text'] = combined_novels_nyt_df.pg_orig_url.apply(clean_book)"
  },
  {
    "objectID": "assets/files/is310IntroTextAnalysis.html#ner-code",
    "href": "assets/files/is310IntroTextAnalysis.html#ner-code",
    "title": "Introduction to Text Analysis Notebook",
    "section": "NER Code",
    "text": "NER Code\nThere are many libraries for doing NER but today we’ll be using spaCy, which is one of the most popular. You can visit the documentation here https://spacy.io/usage.\nNow spaCy is a much more complex library than we have used before so let’s try out an example and then talk through some the documentation. Let’s start with an example from their spaCy 101 guide https://spacy.io/usage/spacy-101#annotations-ner and try copying some code.\ndoc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n\nfor ent in doc.ents:\n    print(ent.text, ent.start_char, ent.end_char, ent.label_)\nIn our case we’ll change nlp to be eng_nlp. Let’s try running this code and see what happens.\n\ndoc = eng_nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n\nfor ent in doc.ents:\n    print(ent.text, ent.start_char, ent.end_char, ent.label_)\n\nApple 0 5 ORG\nU.K. 27 31 GPE\n$1 billion 44 54 MONEY\n\n\nSo what exactly does this output mean? Well first let’s explore a bit of the spaCy API. First how could we check out what our doc variable contains?\nWe should see spacy.tokens.doc.Doc so let’s take a look at the Doc https://spacy.io/api/doc. The documentation is pretty dense, but we should get a sense that a Doc is a collection of Token classes, and that it has a number of built-in methods.\nWe can list these methods with the following code:\n# Let's dig into what this Class gives us\n[prop for prop in dir(doc) if not prop.startswith('_')]\n\nfirst_word = doc[0]\ntype(first_word)\n\n[prop for prop in dir(first_word) if not prop.startswith('__')]\nTake a look at the similiarity and ents methods. What do these methods/attributes do?\nPart of understanding what they are doing, requires understanding how spaCy works. Below is a figure of their pipeline:\n\n\n\nspacy pipeline\n\n\nThis gives us a bit of a sense of how this is working (recognize tokenizer?) but let’s also read their broad overview page https://spacy.io/usage/facts-figures.\nLooking at their comparison usage, what do you think are the benefits and limitations of spaCy? How does spaCy create their models and what are these models exactly?\nReturning our example above, let’s try using one of spaCy’s built-in visualizations to understand how this is working.\nfrom spacy import displacy\ndisplacy.render(doc, style=\"dep\")\nWe should now see something that looks like this:\n\n\n\nspacy pos\n\n\nWhat this visualization is highlighting is essentially how spaCy works, which is with something called Parts of Speech Tagging.\nFrom the spaCy docs:\n\nAfter tokenization, spaCy can parse and tag a given Doc. This is where the trained pipeline and its statistical models come in, which enable spaCy to make predictions of which tag or label most likely applies in this context. A trained component includes binary data that is produced by showing a system enough examples for it to make predictions that generalize across the language – for example, a word following “the” in English is most likely a noun.\n\nSo the key thing to understand is that this super powerful library also comes with a lot of built-in assumptions about how language in your corpus is structured.\nNow that we’ve considered some of the pros and cons, let’s try out spaCy’s NER with some of our data.\nspaCy does have a limit on how much text it can process at once, so we’ll need to break up our text into smaller chunks. Let’s try a small subset first:\n\ntext = combined_novels_nyt_df.cleaned_pg_eng_text[0][0:1000]\ndoc = eng_nlp(text)\n\nfor ent in doc.ents[0:10]:\n    print(ent.text, ent.start_char, ent.end_char, ent.label_)\n\n152K)\\n\\n\\nFull 73 88 CARDINAL\nMiguel de Cervantes\\n\\n\\n\\n Translated 129 167 PERSON\nJohn Ormsby\\n\\n\\n\\n\\nEbook 171 197 PERSON\nNote\\n\\n\\n\\nThe 218 233 ORG\nOrmsby 320 326 PERSON\nJ. W. Clark 391 402 PERSON\nGustave Dor\\xc3\\xa9 419 438 PERSON\nClark 440 445 PERSON\nEnglish 491 498 LANGUAGE\n\\xe2\\x80\\x98Don Quixote\\xe2\\x80\\x99 507 542 ORG\n\n\nWe could visualize this using displacy as well.\n\nfrom spacy import displacy\n\ndisplacy.render(doc, style=\"ent\", jupyter=True)\n\nb'\\n\\n\\n\\nbookcover.jpg\\n\\n\\nFull Size\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nspine.jpg (\n\n    152K)\\n\\n\\nFull\n    CARDINAL\n\n Size\\n\\n\\n\\n\\n\\n\\nDon Quixote\\n\\n\\n\\nby \n\n    Miguel de Cervantes\\n\\n\\n\\n Translated\n    PERSON\n\n by \n\n    John Ormsby\\n\\n\\n\\n\\nEbook\n    PERSON\n\n Editor\\xe2\\x80\\x99s \n\n    Note\\n\\n\\n\\nThe\n    ORG\n\n book cover and spine above and the images which follow were not\\npart of the original \n\n    Ormsby\n    PERSON\n\n translation\\xe2\\x80\\x94they are taken from the 1880\\nedition of \n\n    J. W. Clark\n    PERSON\n\n, illustrated by \n\n    Gustave Dor\\xc3\\xa9\n    PERSON\n\n. \n\n    Clark\n    PERSON\n\n in his\\nedition states that, \\xe2\\x80\\x9cThe \n\n    English\n    LANGUAGE\n\n text of \n\n    \\xe2\\x80\\x98Don Quixote\\xe2\\x80\\x99\n    ORG\n\n adopted in this\\nedition is that of \n\n    Jarvis\n    PERSON\n\n, with occasional corrections from Motteaux.\\xe2\\x80\\x9d\\nSee in the introduction below \n\n    John\n    PERSON\n\n Ormsby\\xe2\\x80\\x99s critique of both the Jarvis\\nand \n\n    Motteaux\n    PERSON\n\n translations. It has been elected in the present \n\n    Project\\nGutenberg\n    PERSON\n\n edition to attach the famous engravings of \n\n    Gustave Dor\\xc3\\xa9\n    PERSON\n\n to\\nthe \n\n    Ormsby\n    PERSON\n\n translation instead of \n\n    the Jarvis/Motteaux\n    ORG\n\n. The detail of\\nmany of the \n\n    Dor\\xc3\\xa9\n    ORG\n\n engravings can be fully appreci\n\n\nYou’ll notice there’s some weird looking characters in the text, like \\xe2\\x80\\x99s or \\n. These are unicode characters and for example \\n is a newline character.\nMore importantly we are starting to see how NER works and what entities it identifies. Below we can see the number of entities that exist in spaCy.\n\n\n\nner entites\n\n\nWe could for example be interested in what places are mentioned in these novels, so we would want to subset to only GPE entities.\nfor ent in doc.ents:\n    if ent.label_ == 'GPE':\n        print(ent.text, ent.start_char, ent.end_char, ent.label_)\nSo let’s try writing code that gets all the GPE entities from our novels.\n\ndef chunk_text(text, n=10000):\n    return (text[i:i+n] for i in range(0, len(text), n))\n\ndef get_entities(text, nlp, n=10000, entity_type=\"GPE\"):\n    entities = []\n    chunks = list(chunk_text(text, n))\n    for chunk in tqdm(chunks, desc=\"Processing Text Chunks\"):\n        doc = nlp(chunk)\n        entities.extend(ent.text for ent in doc.ents if ent.label_ == entity_type)\n    return entities\n\n# Assuming combined_novels_nyt_df and eng_nlp are already defined\ntest_df = combined_novels_nyt_df[0:1]\n# tqdm.pandas(desc=\"Identifying Entities\")\ntest_df['pg_eng_gpe'] = test_df.cleaned_pg_eng_text.apply(get_entities, nlp=eng_nlp, n=10000, entity_type=\"GPE\")\n\nProcessing Chunks: 100%|██████████| 251/251 [02:01&lt;00:00,  2.06it/s]\n\n\n\n# explode the list of entities into separate rows\nexploded_test_df = test_df.explode('pg_eng_gpe')\n# group by the entities and count the number of times they appear\ngpe_counts = exploded_test_df.groupby('pg_eng_gpe').size().reset_index(name='counts')\n# sort the entities by the number of times they appear\ngpe_counts = gpe_counts.sort_values(by='counts', ascending=False)\n# plot the top 10 entities\nalt.Chart(gpe_counts[0:10]).mark_bar().encode(\n    y=alt.Y('pg_eng_gpe', sort='-x'),\n    x='counts'\n)\n\n\n\n\n\n\n\nNow we are starting to see how NER might help us start to explore our cultural data. You’ll notice that there is se\\xc3\\xb1or which is a unicode character for ñ and c3 and b1 are the hex values for that character, which means it stands for senor. It’s unclear why spaCy is recognizing this as a GPE entity, but it’s likely because of some issues in the model (remember we’re using an English model). We could try experimenting with other spaCy models to see if this changes, but fundamentally this shows both the power and danger of relying on these models.\n\nWhat to try next?\n\nTry using a different spaCy model and see how the results change.\nTry running it on the full dataset and see what you find.\nTry spaCy’s parts of speech tagging and see if you can find any interesting patterns, like which pronouns are most common in these novels."
  },
  {
    "objectID": "assets/files/is310IntroTextAnalysis.html#tf-idf-code",
    "href": "assets/files/is310IntroTextAnalysis.html#tf-idf-code",
    "title": "Introduction to Text Analysis Notebook",
    "section": "TF-IDF Code",
    "text": "TF-IDF Code\nTo run TF-IDF we’ll be using the scikit-learn library. You can find the documentation here https://scikit-learn.org/stable/. In particular, we’ll be using the TfidfVectorizer class https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html.\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nvectorizer = TfidfVectorizer()\n\n# Fit and transform the text data\ntfidf_matrix = vectorizer.fit_transform(combined_novels_nyt_df.cleaned_pg_eng_text.fillna(''))\n\n# Convert the TF-IDF matrix to a DataFrame for better readability\ntfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n\n# Add the titles to the DataFrame\ntfidf_df['title'] = combined_novels_nyt_df['title'].values\n\n# Melt the DataFrame to get a long format DataFrame with terms and scores\nmelted_tfidf_df = tfidf_df.melt(id_vars=['title'], var_name='term', value_name='score')\n\n# Sort the DataFrame by score in descending order\nsorted_tfidf_df = melted_tfidf_df.sort_values(by='score', ascending=False)\n\n# Display the top 10 results\nsorted_tfidf_df.head(10)\n\n\n\n\n\n\n\n\ntitle\nterm\nscore\n\n\n\n\n115304890\nThe Last Days of Pompeii\nthe\n0.730242\n\n\n115304767\nGerminal\nthe\n0.700247\n\n\n115304868\nNostromo\nthe\n0.699570\n\n\n115304932\nDeath In Venice\nthe\n0.687305\n\n\n115304523\nThe Grapes of Wrath\nthe\n0.670729\n\n\n115304601\nThe War of the Worlds\nthe\n0.668319\n\n\n115304796\nThe Phantom of the Opera\nthe\n0.667619\n\n\n115304750\nDeath Comes for the Archbishop\nthe\n0.659896\n\n\n115304525\nThe Last of the Mohicans\nthe\n0.659860\n\n\n115304564\nA Journey to the Center of the Earth\nthe\n0.653512\n\n\n\n\n\n\n\nYou’ll notice that our top results are all the words the. This is exactly the issue with common words and why stop words are so popular. So we can try removing the stop words and see what results we get.\nIndeed, TF-IDF has a number of parameters that you can set. Specifically it includes parameters for dealing with stop words, max_df, min_df, and ngram_range. Let’s experiment with some of these parameters.\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Let's try out some different parameters\ntfidf = TfidfVectorizer(stop_words='english', max_df=0.5, min_df=2, ngram_range=(1, 2))\ntfidf_matrix = tfidf.fit_transform(df['text'])\n\nvectorizer = TfidfVectorizer(stop_words=\"english\")\n\n# Fit and transform the text data\ntfidf_matrix = vectorizer.fit_transform(combined_novels_nyt_df.cleaned_pg_eng_text.fillna(''))\n\n# Convert the TF-IDF matrix to a DataFrame for better readability\ntfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n\n# Add the titles to the DataFrame\ntfidf_df['title'] = combined_novels_nyt_df['title'].values\n\n# Melt the DataFrame to get a long format DataFrame with terms and scores\nmelted_tfidf_df = tfidf_df.melt(id_vars=['title'], var_name='term', value_name='score')\n\n# Sort the DataFrame by score in descending order\nsorted_tfidf_no_stopwords_df = melted_tfidf_df.sort_values(by='score', ascending=False)\n\n# Display the top 10 results\nsorted_tfidf_no_stopwords_df.head(10)\n\n\n\n\n\n\n\n\ntitle\nterm\nscore\n\n\n\n\n38042168\nHeidi\nheidi\n0.821331\n\n\n107126913\nA Christmas Carol\nscrooge\n0.819125\n\n\n8154752\nThis Side of Paradise\namory\n0.786431\n\n\n16185827\nCandide\ncandide\n0.748896\n\n\n65341057\nResurrection\nnekhludoff\n0.732123\n\n\n124364958\nHeart of Darkness\nx80\n0.684902\n\n\n141660306\nHeart of Darkness\nxe2\n0.684902\n\n\n141659937\nUncle Tom's Cabin\nxe2\n0.684411\n\n\n124364589\nUncle Tom's Cabin\nx80\n0.684411\n\n\n124364572\nThe Adventures of Tom Sawyer\nx80\n0.684342\n\n\n\n\n\n\n\nNow we see we are getting words that are more specific to each novel, though we still have some unicode characters like x80 and x99 which are € and ’ respectively. We could try removing these characters from our text or use more of the TF-IDF parameters to make them less important. We can also add a custom token pattern that removes these characters.\ntfidf = TfidfVectorizer(stop_words='english', max_df=0.5, min_df=2, ngram_range=(1, 2), token_pattern=r'(?u)\\b\\w+\\b')\ntfidf_matrix = tfidf.fit_transform(df['text'])\nAlternatively though we could actually use spaCy to help us clean this text. Let’s try using spaCy to clean our text and see if we get better results.\n# Load the spaCy English model\nnlp = spacy.load(\"en_core_web_sm\")\n\n# Define a function to clean the text with spaCy\ndef clean_text_spacy(text):\n    # Process the text with spaCy\n    doc = nlp(text)\n    # Remove stopwords, punctuation, and non-alphabetic tokens\n    tokens = [token.lemma_ for token in doc if token.is_alpha and not token.is_stop]\n    # Join tokens back to a single string\n    return ' '.join(tokens)\n\n# Apply the cleaning function to the text data\ncombined_novels_nyt_df['cleaned_text'] = combined_novels_nyt_df['cleaned_pg_eng_text'].fillna('').apply(clean_text_spacy)\n\ndef chunk_text(text, n=10000):\n    return (text[i:i+n] for i in range(0, len(text), n))\n\n# Define a function to clean text with spaCy, processing in chunks\ndef clean_text_spacy(text, nlp, chunk_size=10000):\n    cleaned_chunks = []\n    chunks = list(chunk_text(text, chunk_size))\n    \n    # Chunk the text and process each chunk\n    for chunk in tqdm(chunks, desc=\"Processing Chunks\", leave=False):\n        doc = nlp(chunk)  # Process each chunk with spaCy\n        # Remove stopwords, punctuation, and non-alphabetic tokens, and apply lemmatization\n        tokens = [token.lemma_ for token in doc if token.is_alpha and not token.is_stop]\n        # Join tokens of the current chunk and add to cleaned chunks\n        cleaned_chunks.append(' '.join(tokens))\n    \n    # Join all chunks into a single string (optional, based on your needs)\n    return ' '.join(cleaned_chunks)\n\n# Assuming combined_novels_nyt_df and eng_nlp are already defined\ntqdm.pandas(desc=\"Cleaning Text\")\ncombined_novels_nyt_df['cleaned_pg_eng_text_spacy'] = combined_novels_nyt_df.cleaned_pg_eng_text.fillna('').progress_apply(clean_text_spacy, nlp=eng_nlp)\n\n\nvectorizer = TfidfVectorizer(stop_words=\"english\", min_df=1, max_df=0.7,)\n\n# Fit and transform the text data\ntfidf_matrix = vectorizer.fit_transform(combined_novels_nyt_df.cleaned_pg_eng_text_spacy.fillna(''))\n\n# Convert the TF-IDF matrix to a DataFrame for better readability\ntfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n\n# Add the titles to the DataFrame\ntfidf_df['title'] = combined_novels_nyt_df['title'].values\n\n# Melt the DataFrame to get a long format DataFrame with terms and scores\nmelted_tfidf_df = tfidf_df.melt(id_vars=['title'], var_name='term', value_name='score')\n\n# Sort the DataFrame by score in descending order\nsorted_tfidf_no_stopwords_min_max_df = melted_tfidf_df.sort_values(by='score', ascending=False)\n\n# Display the top 10 results\nsorted_tfidf_no_stopwords_min_max_df.head(10)\n\n\n\n\n\n\n\n\ntitle\nterm\nscore\n\n\n\n\n44672498\nThe Hunchback of Notre Dame\nn\n0.992497\n\n\n44672555\nThe Hound of the Baskervilles\nn\n0.988642\n\n\n44672495\nThe Count of Monte Cristo\nn\n0.951079\n\n\n44672602\nThe Sun Also Rises\nn\n0.950076\n\n\n44672542\nSons and Lovers\nn\n0.946033\n\n\n44672446\nJane Eyre\nn\n0.944493\n\n\n44672469\nLes Misérables\nn\n0.941966\n\n\n44672520\nA Journey to the Center of the Earth\nn\n0.941441\n\n\n44672492\nUlysses\nn\n0.939559\n\n\n44672491\nUlysses\nn\n0.939559\n\n\n\n\n\n\n\nYou’ll notice that I’ve set the parameter min_df=1 and max_df=0.7. If we go to the scikit-learn documentation for TfidfVectorizer we can see that min_df is the minimum document frequency and max_df is the maximum document frequency. This means that we are only including words that appear in at least 2 documents and in at most 70% of the documents. So you can imagine that these parameters can determine a lot of our analysis.\n\nvectorizer = TfidfVectorizer(stop_words=\"english\", min_df=1, max_df=0.9, ngram_range=(1, 2), max_features=1000)\n\n# Fit and transform the text data\ntfidf_matrix = vectorizer.fit_transform(combined_novels_nyt_df.cleaned_pg_eng_text_spacy.fillna(''))\n\n# Convert the TF-IDF matrix to a DataFrame for better readability\ntfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n\n# Add the titles to the DataFrame\ntfidf_df['title'] = combined_novels_nyt_df['title'].values\n\n# Melt the DataFrame to get a long format DataFrame with terms and scores\nmelted_tfidf_df = tfidf_df.melt(id_vars=['title'], var_name='term', value_name='score')\n\n# Sort the DataFrame by score in descending order\nsorted_tfidf_no_stopwords_min_max_ngram_df = melted_tfidf_df.sort_values(by='score', ascending=False)\n\n# Display the top 10 results\nsorted_tfidf_no_stopwords_min_max_ngram_df.head(10)"
  },
  {
    "objectID": "slides/01-introducing-computing-in-the-humanities.html#who-are-your-instructors",
    "href": "slides/01-introducing-computing-in-the-humanities.html#who-are-your-instructors",
    "title": "Introductions & Installations",
    "section": "Who are your instructors?",
    "text": "Who are your instructors?\nLead Instructor: Dr. Zoe LeBlanc (You can call me Prof. LeBlanc or Prof. Zoe)\nPronouns: She/Her\nEmail: zleblanc@illinois.edu\nWebsite: zoeleblanc.com\nContact me via Slack please I will try to learn your names starting next week. Please correct me if I get it wrong. Thank you!!"
  },
  {
    "objectID": "slides/01-introducing-computing-in-the-humanities.html#who-are-your-instructors-1",
    "href": "slides/01-introducing-computing-in-the-humanities.html#who-are-your-instructors-1",
    "title": "Introductions & Installations",
    "section": "Who are your instructors?",
    "text": "Who are your instructors?\nTeaching Assistant: Jessica Frye (Prefers Jess)\nPronouns: She/Her\nEmail: jrfrye2@illinois.edu"
  },
  {
    "objectID": "slides/01-introducing-computing-in-the-humanities.html#course-description",
    "href": "slides/01-introducing-computing-in-the-humanities.html#course-description",
    "title": "Introductions & Installations",
    "section": "Course Description",
    "text": "Course Description\nCurrently the iSchool catalogue lists this following course description:\n\nExplores use and application of technology to scholarly activity in the humanities, including projects that put classic texts on the web or create multimedia applications on humanities topics.\n\nThis isn’t necessarily wrong, but not quite descriptive enough for this version of the course."
  },
  {
    "objectID": "slides/01-introducing-computing-in-the-humanities.html#what-is-culture-as-data",
    "href": "slides/01-introducing-computing-in-the-humanities.html#what-is-culture-as-data",
    "title": "Introductions & Installations",
    "section": "What is “Culture as Data”?",
    "text": "What is “Culture as Data”?\nOur goal is to understand how culture can be represented as data and studied with computation.\n\nBy culture, we mean what’s usually associated with the Humanities:\n\nPopular fiction & literature\nNewspapers & government documents\nOnline communities (Reddit, TikTok subcultures)"
  },
  {
    "objectID": "slides/01-introducing-computing-in-the-humanities.html#culture-is-increasingly-digital",
    "href": "slides/01-introducing-computing-in-the-humanities.html#culture-is-increasingly-digital",
    "title": "Introductions & Installations",
    "section": "Culture is Increasingly Digital",
    "text": "Culture is Increasingly Digital\nCulture is an intrinsic part of being human.\n\nToday, that culture is increasingly both digital and datafied.\n\n\nBut representing our cultural heritage is rarely straightforward or without tradeoffs."
  },
  {
    "objectID": "slides/01-introducing-computing-in-the-humanities.html#humanities-can-change-computing",
    "href": "slides/01-introducing-computing-in-the-humanities.html#humanities-can-change-computing",
    "title": "Introductions & Installations",
    "section": "Humanities Can Change Computing",
    "text": "Humanities Can Change Computing\nWe will explore how humanities can change how we think about computing.\n\n\nHistories of data collection and computation are fundamentally political\nInterpretation becomes “baked” into technologies\nThese forces shape both scholarship and society"
  },
  {
    "objectID": "slides/01-introducing-computing-in-the-humanities.html#what-well-do",
    "href": "slides/01-introducing-computing-in-the-humanities.html#what-well-do",
    "title": "Introductions & Installations",
    "section": "What We’ll Do",
    "text": "What We’ll Do\nWe will investigate these topics through:\n\n\nWeekly readings and assignments\n\n\n\n\nA semester-long project\n\n\n\n\nExperiencing the full process: from humanistic questions → collecting data → analyzing → communicating findings"
  },
  {
    "objectID": "slides/01-introducing-computing-in-the-humanities.html#bigger-questions",
    "href": "slides/01-introducing-computing-in-the-humanities.html#bigger-questions",
    "title": "Introductions & Installations",
    "section": "Bigger Questions",
    "text": "Bigger Questions\nThrough weekly assignments and projects, we will debate:\n\nData ethics and privacy\nSustainability and curation of digital projects\nPossibilities and limitations of computational methods\nHow computing in the humanities connects to global conversations about data and society"
  },
  {
    "objectID": "slides/01-introducing-computing-in-the-humanities.html#objective-1-explore",
    "href": "slides/01-introducing-computing-in-the-humanities.html#objective-1-explore",
    "title": "Introductions & Installations",
    "section": "Objective 1: Explore",
    "text": "Objective 1: Explore\nExplore computing in the humanities as a research field.\n\nThrough class discussions, readings, selected projects, and assignments, this course will provide an overview of the:\n\nHistory\nDebates\nCurrent trends"
  },
  {
    "objectID": "slides/01-introducing-computing-in-the-humanities.html#objective-2-experiment",
    "href": "slides/01-introducing-computing-in-the-humanities.html#objective-2-experiment",
    "title": "Introductions & Installations",
    "section": "Objective 2: Experiment",
    "text": "Objective 2: Experiment\nExperiment with computing in the humanities as a research praxis.\n\nThrough learning coding, data analysis, and project management, this course will provide a foundation for:\n\nMaking projects that blend culture and coding\nEngaging with debates over how to maintain and evaluate this type of scholarship"
  },
  {
    "objectID": "slides/01-introducing-computing-in-the-humanities.html#what-comes-after-this-course",
    "href": "slides/01-introducing-computing-in-the-humanities.html#what-comes-after-this-course",
    "title": "Introductions & Installations",
    "section": "What Comes After This Course?",
    "text": "What Comes After This Course?\nMuch depends on your interests, but you will be well equipped to continue undertaking substantive and innovative research on culture using computation and data. These skills are useful whether you aim to be a:\n\nData scientist or developer\nComputational or Data Journalist\nData or DH Librarian\nHCI or UX researcher\n\nOr just someone who understands how technology and information shape our world"
  },
  {
    "objectID": "slides/01-introducing-computing-in-the-humanities.html#ideally",
    "href": "slides/01-introducing-computing-in-the-humanities.html#ideally",
    "title": "Introductions & Installations",
    "section": "Ideally…",
    "text": "Ideally…\nI hope each of you continues to work on your final project and share your research long after the course ends."
  },
  {
    "objectID": "slides/01-introducing-computing-in-the-humanities.html#pre--and-co-requisites",
    "href": "slides/01-introducing-computing-in-the-humanities.html#pre--and-co-requisites",
    "title": "Introductions & Installations",
    "section": "Pre- and Co-Requisites",
    "text": "Pre- and Co-Requisites\nThere is no required prerequisite.\n\nHowever, students should have some previous experience equivalent to a semester of programming, ideally in Python.\n\n\nRelevant courses include IS205 and IS107."
  },
  {
    "objectID": "slides/01-introducing-computing-in-the-humanities.html#how-much-coding-experience",
    "href": "slides/01-introducing-computing-in-the-humanities.html#how-much-coding-experience",
    "title": "Introductions & Installations",
    "section": "How much coding experience?",
    "text": "How much coding experience?\nI get this question every semester and there is no one answer. Part of the reason it depends is because AI is revolutionizing how we code. So even if you have limited experience, you might actually get further than you expect.\n\nHowever, I would say that if you have limited interest in coding, you might consider switching to Dr. Roland’s section of IS310 (Section A). Happy to assist if you decide that’s what is best for you!"
  },
  {
    "objectID": "slides/01-introducing-computing-in-the-humanities.html#questions",
    "href": "slides/01-introducing-computing-in-the-humanities.html#questions",
    "title": "Introductions & Installations",
    "section": "Questions?",
    "text": "Questions?\nInterested students should contact the instructor if they have any questions.\n\nLet’s get started!\n\n\nBy exploring our course website ✨"
  },
  {
    "objectID": "slides/01-introducing-computing-in-the-humanities.html#weekly-participation-25",
    "href": "slides/01-introducing-computing-in-the-humanities.html#weekly-participation-25",
    "title": "Introductions & Installations",
    "section": "Weekly Participation (25%)",
    "text": "Weekly Participation (25%)\nTwo components:\n\nIndividual engagement with course materials\nCollaborative group presentations\n\nBoth ensure you’re deepening your own understanding AND learning from peers."
  },
  {
    "objectID": "slides/01-introducing-computing-in-the-humanities.html#seminar-discussion-annotations-15",
    "href": "slides/01-introducing-computing-in-the-humanities.html#seminar-discussion-annotations-15",
    "title": "Introductions & Installations",
    "section": "Seminar Discussion & Annotations (15%)",
    "text": "Seminar Discussion & Annotations (15%)\nBefore each class, engage with assigned materials:\n\nBook chapters or articles\nData visualizations\nDocumentation\n\nThese lay the groundwork for class discussions and your semester project."
  },
  {
    "objectID": "slides/01-introducing-computing-in-the-humanities.html#questions-to-consider",
    "href": "slides/01-introducing-computing-in-the-humanities.html#questions-to-consider",
    "title": "Introductions & Installations",
    "section": "Questions to Consider",
    "text": "Questions to Consider\nWhen engaging with weekly materials, ask yourself:\n\nWhat is the main argument? Is there one?\nHow does the author support their argument?\nWhat evidence or data did they use?\nWhat is the likely audience?"
  },
  {
    "objectID": "slides/01-introducing-computing-in-the-humanities.html#more-questions-to-consider",
    "href": "slides/01-introducing-computing-in-the-humanities.html#more-questions-to-consider",
    "title": "Introductions & Installations",
    "section": "More Questions to Consider",
    "text": "More Questions to Consider\n\nWhat connections or tensions exist across materials?\nWhat was confusing or unclear?\nHow does this connect to previous weeks?\n\n\nGoal: Summarize central points in a few lines, not detail everything!"
  },
  {
    "objectID": "slides/01-introducing-computing-in-the-humanities.html#demonstrating-engagement",
    "href": "slides/01-introducing-computing-in-the-humanities.html#demonstrating-engagement",
    "title": "Introductions & Installations",
    "section": "Demonstrating Engagement",
    "text": "Demonstrating Engagement\nOption 1: Seminar Discussions\n\nAsk questions about complex concepts\nShare connections across materials\nRespond thoughtfully to peers\n\n. . .\nGoal: Create a thoughtful and respectful intellectual community"
  },
  {
    "objectID": "slides/01-introducing-computing-in-the-humanities.html#demonstrating-engagement-1",
    "href": "slides/01-introducing-computing-in-the-humanities.html#demonstrating-engagement-1",
    "title": "Introductions & Installations",
    "section": "Demonstrating Engagement",
    "text": "Demonstrating Engagement\nOption 2: Asynchronous Annotations\nUsing Hypothesis annotation platform:\n\nShare thoughts on weekly materials\nEngage with peers’ annotations\nMay be asked to expand during discussions\n\nSign up: hypothes.is/signup and more details available in our course website"
  },
  {
    "objectID": "slides/01-introducing-computing-in-the-humanities.html#grading-seminar-discussions",
    "href": "slides/01-introducing-computing-in-the-humanities.html#grading-seminar-discussions",
    "title": "Introductions & Installations",
    "section": "Grading: Seminar Discussions",
    "text": "Grading: Seminar Discussions\nTo receive full credit:\n\nBe present in person\nActively participate in discussion or activities\n\n\nSimply attending class is not enough!"
  },
  {
    "objectID": "slides/01-introducing-computing-in-the-humanities.html#grading-annotations",
    "href": "slides/01-introducing-computing-in-the-humanities.html#grading-annotations",
    "title": "Introductions & Installations",
    "section": "Grading: Annotations",
    "text": "Grading: Annotations\n\nSubmit via Hypothesis by midnight the day before class\nTag appropriately in our Hypothesis group\nDemonstrate thoughtful engagement:\n\nSummarize key points\nRaise questions\nMake connections"
  },
  {
    "objectID": "slides/01-introducing-computing-in-the-humanities.html#group-presentations-10",
    "href": "slides/01-introducing-computing-in-the-humanities.html#group-presentations-10",
    "title": "Introductions & Installations",
    "section": "Group Presentations (10%)",
    "text": "Group Presentations (10%)\nGroups assigned in first two weeks based on:\n\nStudent interests\nBackgrounds (music, literature, social media, gaming, etc.)"
  },
  {
    "objectID": "slides/01-introducing-computing-in-the-humanities.html#group-work-structure",
    "href": "slides/01-introducing-computing-in-the-humanities.html#group-work-structure",
    "title": "Introductions & Installations",
    "section": "Group Work Structure",
    "text": "Group Work Structure\nWeekly Prompts and Activities:\n\nFind digital objects\nExplore datasets\nApply concepts from readings"
  },
  {
    "objectID": "slides/01-introducing-computing-in-the-humanities.html#in-class-presentations",
    "href": "slides/01-introducing-computing-in-the-humanities.html#in-class-presentations",
    "title": "Introductions & Installations",
    "section": "In-Class Presentations",
    "text": "In-Class Presentations\n\nNot every group presents every week\nEven if not presenting:\n\nSubmit work to GitHub\nBe prepared to discuss\n\n\n\nPresentations should be clear, concise, and demonstrate understanding."
  },
  {
    "objectID": "slides/01-introducing-computing-in-the-humanities.html#documentation-requirements",
    "href": "slides/01-introducing-computing-in-the-humanities.html#documentation-requirements",
    "title": "Introductions & Installations",
    "section": "Documentation Requirements",
    "text": "Documentation Requirements\nAll group work must be documented on GitHub:\n\nSummary of activities\nHow labor was divided\nReflections on the process\n\n\nClear documentation is crucial for grading and project progress!"
  },
  {
    "objectID": "slides/01-introducing-computing-in-the-humanities.html#grading-group-work",
    "href": "slides/01-introducing-computing-in-the-humanities.html#grading-group-work",
    "title": "Introductions & Installations",
    "section": "Grading: Group Work",
    "text": "Grading: Group Work\nBased on:\n\nActive participation in presentations\nQuality of contributions\nEffectiveness of collaboration\n\n\nGrade not impacted if a member is absent (but group must pivot)."
  },
  {
    "objectID": "slides/01-introducing-computing-in-the-humanities.html#group-work-deadlines",
    "href": "slides/01-introducing-computing-in-the-humanities.html#group-work-deadlines",
    "title": "Introductions & Installations",
    "section": "Group Work Deadlines",
    "text": "Group Work Deadlines\nSubmit to GitHub by midnight before class.\n\nLate submissions: Half credit (with explanation)\nRepeated late submissions: Meeting with Instructor"
  },
  {
    "objectID": "slides/01-introducing-computing-in-the-humanities.html#weekly-coding-assignments-25",
    "href": "slides/01-introducing-computing-in-the-humanities.html#weekly-coding-assignments-25",
    "title": "Introductions & Installations",
    "section": "Weekly Coding Assignments (25%)",
    "text": "Weekly Coding Assignments (25%)\n\nComplete and share via GitHub\nPair programming encouraged!\n\n\nWork together, but make sure you understand the concepts — they build quickly!"
  },
  {
    "objectID": "slides/01-introducing-computing-in-the-humanities.html#grading-coding-assignments",
    "href": "slides/01-introducing-computing-in-the-humanities.html#grading-coding-assignments",
    "title": "Introductions & Installations",
    "section": "Grading: Coding Assignments",
    "text": "Grading: Coding Assignments\n\nDue: Midnight before class\nLate submissions: Half credit (if before final class)\n\n\nQuestions encouraged — in person and online!"
  },
  {
    "objectID": "slides/01-introducing-computing-in-the-humanities.html#required-texts-resources",
    "href": "slides/01-introducing-computing-in-the-humanities.html#required-texts-resources",
    "title": "Introductions & Installations",
    "section": "Required Texts & Resources",
    "text": "Required Texts & Resources\nGood news: Almost all materials available free online!\n\nCourse website\nCanvas\n\n\nNo required purchases for software or books."
  },
  {
    "objectID": "slides/01-introducing-computing-in-the-humanities.html#computer-access",
    "href": "slides/01-introducing-computing-in-the-humanities.html#computer-access",
    "title": "Introductions & Installations",
    "section": "Computer Access",
    "text": "Computer Access\nYou will need access to a computer.\n\nIf this is an issue, let me know early — we’ll find solutions!"
  },
  {
    "objectID": "slides/01-introducing-computing-in-the-humanities.html#summary",
    "href": "slides/01-introducing-computing-in-the-humanities.html#summary",
    "title": "Introductions & Installations",
    "section": "Summary",
    "text": "Summary\n\n\n\nComponent\nWeight\n\n\n\n\nSeminar Discussion & Annotations\n15%\n\n\nGroup Presentations\n10%\n\n\nWeekly Coding Assignments\n25%\n\n\nTotal\n50%"
  },
  {
    "objectID": "slides/01-introducing-computing-in-the-humanities.html#questions-1",
    "href": "slides/01-introducing-computing-in-the-humanities.html#questions-1",
    "title": "Introductions & Installations",
    "section": "Questions?",
    "text": "Questions?\nRemember:\n\nEngage thoughtfully\nCollaborate with peers\nAsk questions early and often!"
  },
  {
    "objectID": "slides/01-introducing-computing-in-the-humanities.html#inspired-by-rdc-project",
    "href": "slides/01-introducing-computing-in-the-humanities.html#inspired-by-rdc-project",
    "title": "Introductions & Installations",
    "section": "Inspired by RDC Project",
    "text": "Inspired by RDC Project\nThe project is modeled on the Responsible Datasets in Context Project:\nresponsible-datasets-in-context.com\n\nCreated to help students “work with data responsibly.”"
  },
  {
    "objectID": "slides/01-introducing-computing-in-the-humanities.html#why-context-matters",
    "href": "slides/01-introducing-computing-in-the-humanities.html#why-context-matters",
    "title": "Introductions & Installations",
    "section": "Why Context Matters",
    "text": "Why Context Matters\n\n“Data cannot be analyzed responsibly without deep knowledge of its social and historical context, provenance, and limitations.”\n\n\n\n“In classes, it is very common for students to use datasets that they find on websites like Kaggle, datasets that are poorly documented and that students thus don’t fully understand. This is a recipe for irresponsible data work.”"
  },
  {
    "objectID": "slides/01-introducing-computing-in-the-humanities.html#your-goal",
    "href": "slides/01-introducing-computing-in-the-humanities.html#your-goal",
    "title": "Introductions & Installations",
    "section": "Your Goal",
    "text": "Your Goal\nYou will work collaboratively to create a first draft of what could eventually be part of the RDC Project.\n\nNot as polished or extensive — but a meaningful contribution."
  },
  {
    "objectID": "slides/01-introducing-computing-in-the-humanities.html#timeline-overview",
    "href": "slides/01-introducing-computing-in-the-humanities.html#timeline-overview",
    "title": "Introductions & Installations",
    "section": "Timeline Overview",
    "text": "Timeline Overview\n\n\n\nMilestone\nDue Date\nWeight\n\n\n\n\nGroup & Individual Topic Selection\nFeb 5\nPass/Fail\n\n\nInitial Dataset\nMar 12\n15%\n\n\nData Demo Day\nApr 30 or May 5\n5%\n\n\nFinal Submission\nMay 15\n30%"
  },
  {
    "objectID": "slides/01-introducing-computing-in-the-humanities.html#group-formation",
    "href": "slides/01-introducing-computing-in-the-humanities.html#group-formation",
    "title": "Introductions & Installations",
    "section": "Group Formation",
    "text": "Group Formation\nIn the first two weeks, you will be assigned to a group based on:\n\nShared interests\nComplementary skill sets"
  },
  {
    "objectID": "slides/01-introducing-computing-in-the-humanities.html#planning-document",
    "href": "slides/01-introducing-computing-in-the-humanities.html#planning-document",
    "title": "Introductions & Installations",
    "section": "Planning Document",
    "text": "Planning Document\nYour first task is to collaboratively determine:\n\n\nGroup Theme: Shared area of interest\n\n\n\n\nIndividual Ideas: Specific datasets each member is considering\n\n\n\n\nCollaboration Plan: Communication and GitHub organization strategy"
  },
  {
    "objectID": "slides/01-introducing-computing-in-the-humanities.html#format-submission",
    "href": "slides/01-introducing-computing-in-the-humanities.html#format-submission",
    "title": "Introductions & Installations",
    "section": "Format & Submission",
    "text": "Format & Submission\n\nMarkdown file (planning.md) in your GitHub repo\n500–750 words\nUse headings, bullet points, links, images, or tables"
  },
  {
    "objectID": "slides/01-introducing-computing-in-the-humanities.html#why-start-small",
    "href": "slides/01-introducing-computing-in-the-humanities.html#why-start-small",
    "title": "Introductions & Installations",
    "section": "Why Start Small?",
    "text": "Why Start Small?\nCreate approximately 50-100 data items through close, interpretive work.\n\nWhy small? This is where you learn that every dataset embeds interpretive choices.\n\n\nWhen you manually work through items, deciding what counts and how to categorize, you experience the intellectual and ethical labor that gets hidden at scale."
  },
  {
    "objectID": "slides/01-introducing-computing-in-the-humanities.html#computation-required",
    "href": "slides/01-introducing-computing-in-the-humanities.html#computation-required",
    "title": "Introductions & Installations",
    "section": "Computation Required",
    "text": "Computation Required\nWhile your dataset is small, you must use computational tools to assist.\n\nThis is not about automation—it’s about understanding how computation can augment even bespoke data work."
  },
  {
    "objectID": "slides/01-introducing-computing-in-the-humanities.html#approach-1-create-from-scratch",
    "href": "slides/01-introducing-computing-in-the-humanities.html#approach-1-create-from-scratch",
    "title": "Introductions & Installations",
    "section": "Approach 1: Create from Scratch",
    "text": "Approach 1: Create from Scratch\nTransform complex cultural materials into structured data.\n\nExamples:\n\nDigitize lesser-known children’s literature and decide what to capture\nAnnotate TikToks mentioning AI to track recurring themes\nExtract and categorize visual elements from historical documents"
  },
  {
    "objectID": "slides/01-introducing-computing-in-the-humanities.html#approach-2-audit-augment",
    "href": "slides/01-introducing-computing-in-the-humanities.html#approach-2-audit-augment",
    "title": "Introductions & Installations",
    "section": "Approach 2: Audit & Augment",
    "text": "Approach 2: Audit & Augment\nCritically engage with an existing dataset that lacks documentation or transparency.\n\nExamples:\n\nTrace a Kaggle movie dataset back to its source\nCompare original reviews to what appears in the dataset\nAdd missing metadata or flag inconsistencies"
  },
  {
    "objectID": "slides/01-introducing-computing-in-the-humanities.html#submission-components",
    "href": "slides/01-introducing-computing-in-the-humanities.html#submission-components",
    "title": "Introductions & Installations",
    "section": "Submission Components",
    "text": "Submission Components\n\nInitial Dataset (~50-100 items) in structured format\n\n\n\nInitial Documentation explaining process and interpretive choices\n\n\n\n\nNext Steps plan for scaling computationally"
  },
  {
    "objectID": "slides/01-introducing-computing-in-the-humanities.html#focus-on-whats-next",
    "href": "slides/01-introducing-computing-in-the-humanities.html#focus-on-whats-next",
    "title": "Introductions & Installations",
    "section": "Focus on What’s Next",
    "text": "Focus on What’s Next\nThis presentation is primarily about speculation:\n\n\nWhat users do you envision?\nWhat computational methods might you try?\nWhat patterns might emerge at scale?\nWhat future data would you collect?"
  },
  {
    "objectID": "slides/01-introducing-computing-in-the-humanities.html#work-in-progress",
    "href": "slides/01-introducing-computing-in-the-humanities.html#work-in-progress",
    "title": "Introductions & Installations",
    "section": "Work in Progress",
    "text": "Work in Progress\nThis is work-in-progress, not a final polished product.\n\nBe creative and ambitious in your presentations!"
  },
  {
    "objectID": "slides/01-introducing-computing-in-the-humanities.html#what-youll-have-done",
    "href": "slides/01-introducing-computing-in-the-humanities.html#what-youll-have-done",
    "title": "Introductions & Installations",
    "section": "What You’ll Have Done",
    "text": "What You’ll Have Done\nBy this point, you will have:\n\nCreated data manually with computational assistance\nAugmented it at scale\nExperimented with methods\nPresented speculative visions"
  },
  {
    "objectID": "slides/01-introducing-computing-in-the-humanities.html#individual-component-25",
    "href": "slides/01-introducing-computing-in-the-humanities.html#individual-component-25",
    "title": "Introductions & Installations",
    "section": "Individual Component (25%)",
    "text": "Individual Component (25%)\nCulture As Dataset:\n\nBespoke manual work + computational augmentation\nStructured format, thoughtfully organized\nRelevant documentation"
  },
  {
    "objectID": "slides/01-introducing-computing-in-the-humanities.html#individual-component-cont.",
    "href": "slides/01-introducing-computing-in-the-humanities.html#individual-component-cont.",
    "title": "Introductions & Installations",
    "section": "Individual Component (cont.)",
    "text": "Individual Component (cont.)\nCulture As Documentation:\nA final data essay that tells the story of your dataset:\n\nHow you made it\nWhat it represents\nWhat it reveals and conceals"
  },
  {
    "objectID": "slides/01-introducing-computing-in-the-humanities.html#data-essay-focus-areas",
    "href": "slides/01-introducing-computing-in-the-humanities.html#data-essay-focus-areas",
    "title": "Introductions & Installations",
    "section": "Data Essay Focus Areas",
    "text": "Data Essay Focus Areas\nYour essay should address:\n\nHow computation played a role\nHow scale shaped the data\nLimitations and qualifications\nEthical or privacy considerations\nLessons learned\nConnection to peer-reviewed scholarship"
  },
  {
    "objectID": "slides/01-introducing-computing-in-the-humanities.html#collective-component-5",
    "href": "slides/01-introducing-computing-in-the-humanities.html#collective-component-5",
    "title": "Introductions & Installations",
    "section": "Collective Component (5%)",
    "text": "Collective Component (5%)\nCollective Principles & Documentation\n\nSynthesize what your group collectively learned about working with your particular type of cultural data."
  },
  {
    "objectID": "slides/01-introducing-computing-in-the-humanities.html#think-of-it-as",
    "href": "slides/01-introducing-computing-in-the-humanities.html#think-of-it-as",
    "title": "Introductions & Installations",
    "section": "Think of It As…",
    "text": "Think of It As…\nWriting the documentation you wish had existed when you started.\n\n\nWhat should someone know before representing music as data? Or social media? Or gaming culture?\n\n\n\n\nWhat principles emerged from your group’s diverse approaches?"
  },
  {
    "objectID": "slides/01-introducing-computing-in-the-humanities.html#grade-breakdown",
    "href": "slides/01-introducing-computing-in-the-humanities.html#grade-breakdown",
    "title": "Introductions & Installations",
    "section": "Grade Breakdown",
    "text": "Grade Breakdown\n\n\n\nComponent\nWeight\n\n\n\n\nTopic Selection\nPass/Fail\n\n\nInitial Dataset\n15%\n\n\nData Demo Day\n5%\n\n\nFinal Submission (Individual)\n25%\n\n\nFinal Submission (Collective)\n5%\n\n\nTotal\n50%"
  },
  {
    "objectID": "slides/01-introducing-computing-in-the-humanities.html#questions-2",
    "href": "slides/01-introducing-computing-in-the-humanities.html#questions-2",
    "title": "Introductions & Installations",
    "section": "Questions?",
    "text": "Questions?\nRemember:\n\nStart with close, interpretive work\nUse computation to augment, not replace\nDocument your decisions and process\nThink about context and responsibility\n\n\nMore details throughout the semester! Though requirements are subject to change depending on speed and AI-usage."
  },
  {
    "objectID": "slides/01-introducing-computing-in-the-humanities.html#general-note-on-grading",
    "href": "slides/01-introducing-computing-in-the-humanities.html#general-note-on-grading",
    "title": "Introductions & Installations",
    "section": "General Note on Grading",
    "text": "General Note on Grading\nI tend to use GitHub to have as much transparency when it comes to grading and feedback.\n\nGenerally, as far as I’m concerned you are all A+ humans. It’s just about ensuring that effort, creativity, labor, and other core principles are correctly evaluated. If you have concerns over grades, I am always happy to discuss them."
  },
  {
    "objectID": "slides/01-introducing-computing-in-the-humanities.html#if-you-need-to-miss-class",
    "href": "slides/01-introducing-computing-in-the-humanities.html#if-you-need-to-miss-class",
    "title": "Introductions & Installations",
    "section": "If You Need to Miss Class",
    "text": "If You Need to Miss Class\nIf you are feeling unwell or have been exposed to illness, please stay home and prioritize your health.\n\nI do not require doctor’s notes for absences.\n\n\nHowever, please keep in mind that if you miss a substantial portion of class meetings, it will be difficult to make up missed content and that you need to coordinate with your group members who depend on your contributions."
  },
  {
    "objectID": "slides/01-introducing-computing-in-the-humanities.html#when-you-miss-class",
    "href": "slides/01-introducing-computing-in-the-humanities.html#when-you-miss-class",
    "title": "Introductions & Installations",
    "section": "When You Miss Class",
    "text": "When You Miss Class\n\n\nMessage me on Slack as soon as possible\n\n\n\n\nInform your group members so they can plan accordingly\n\n\n\n\nCheck in with me about making up missed content\n\n\n\n\nReview materials on the course website"
  },
  {
    "objectID": "slides/01-introducing-computing-in-the-humanities.html#information-overload-days",
    "href": "slides/01-introducing-computing-in-the-humanities.html#information-overload-days",
    "title": "Introductions & Installations",
    "section": "Information Overload Days",
    "text": "Information Overload Days\nSometimes you need a break from the workload.\n\nInstead of missing class outright, let me know you need an information overload day."
  },
  {
    "objectID": "slides/01-introducing-computing-in-the-humanities.html#how-information-overload-days-work",
    "href": "slides/01-introducing-computing-in-the-humanities.html#how-information-overload-days-work",
    "title": "Introductions & Installations",
    "section": "How Information Overload Days Work",
    "text": "How Information Overload Days Work\n\n\nYou are excused from assigned materials and discussion\n\n\n\n\nYou actively listen during class\n\n\n\n\nConsult with instructor about make-up assignments later\n\n\n\n\nInform your group and help them adjust\n\n\n\nTwo free, no questions asked — after that, let’s talk!"
  },
  {
    "objectID": "slides/01-introducing-computing-in-the-humanities.html#worst-case-scenarios",
    "href": "slides/01-introducing-computing-in-the-humanities.html#worst-case-scenarios",
    "title": "Introductions & Installations",
    "section": "Worst Case Scenarios",
    "text": "Worst Case Scenarios\nIf you or someone close to you becomes ill:\n\nFinal grade based on existing work\nOption to move course to pass/fail\n\n\nWhen you can, please get in touch. Your wellbeing comes first."
  },
  {
    "objectID": "slides/01-introducing-computing-in-the-humanities.html#respect-in-all-communication",
    "href": "slides/01-introducing-computing-in-the-humanities.html#respect-in-all-communication",
    "title": "Introductions & Installations",
    "section": "Respect in All Communication",
    "text": "Respect in All Communication\n\n\nIn-person interactions\n\n\n\n\nSlack messages\n\n\n\n\nEmails\n\n\n\nThis course is experimental with students from varied backgrounds. Every opinion, question, and idea deserves a respectful response."
  },
  {
    "objectID": "slides/01-introducing-computing-in-the-humanities.html#rule-of-thumb",
    "href": "slides/01-introducing-computing-in-the-humanities.html#rule-of-thumb",
    "title": "Introductions & Installations",
    "section": "Rule of Thumb",
    "text": "Rule of Thumb\nWhen in doubt, ask questions and over-communicate — but do so respectfully!"
  },
  {
    "objectID": "slides/01-introducing-computing-in-the-humanities.html#the-short-version",
    "href": "slides/01-introducing-computing-in-the-humanities.html#the-short-version",
    "title": "Introductions & Installations",
    "section": "The Short Version",
    "text": "The Short Version\nDon’t cheat.\n\nIf you need help, see the instructor.\n\n\nI would rather you turn in work late than have to report you for plagiarism."
  },
  {
    "objectID": "slides/01-introducing-computing-in-the-humanities.html#citation-as-practice",
    "href": "slides/01-introducing-computing-in-the-humanities.html#citation-as-practice",
    "title": "Introductions & Installations",
    "section": "Citation as Practice",
    "text": "Citation as Practice\nWe’ll discuss what constitutes plagiarism (it gets thorny with code).\n\nRule of thumb: Cite as much as possible.\n\n\nAll scholarship is a collective endeavor."
  },
  {
    "objectID": "slides/01-introducing-computing-in-the-humanities.html#why-citation-matters",
    "href": "slides/01-introducing-computing-in-the-humanities.html#why-citation-matters",
    "title": "Introductions & Installations",
    "section": "Why Citation Matters",
    "text": "Why Citation Matters\n\n“Citation is how we acknowledge our debt to those who came before; those who helped us find our way when the way was obscured because we deviated from the paths we were told to follow.”\n— Sara Ahmed, Living a Feminist Life\n\n\n\n“Acknowledging and establishing feminist genealogies is part of the work of producing more just forms of knowledge and intellectual practice.”\n— Beverly Weber, Digital Feminist Collective"
  },
  {
    "objectID": "slides/01-introducing-computing-in-the-humanities.html#why-citation-matters-1",
    "href": "slides/01-introducing-computing-in-the-humanities.html#why-citation-matters-1",
    "title": "Introductions & Installations",
    "section": "Why Citation Matters",
    "text": "Why Citation Matters\nAcknowledging sources is both intellectually and politically imperative."
  },
  {
    "objectID": "slides/01-introducing-computing-in-the-humanities.html#your-course-workflow",
    "href": "slides/01-introducing-computing-in-the-humanities.html#your-course-workflow",
    "title": "Introductions & Installations",
    "section": "Your Course Workflow",
    "text": "Your Course Workflow\nIn the first two weeks, submit your Course Workflow addressing:\n\nUsing AI? Which tools? For what purposes?\nNot using AI? What’s your alternative workflow?\nUsing local models? Consult with instructor for setup."
  },
  {
    "objectID": "slides/01-introducing-computing-in-the-humanities.html#if-your-workflow-changes",
    "href": "slides/01-introducing-computing-in-the-humanities.html#if-your-workflow-changes",
    "title": "Introductions & Installations",
    "section": "If Your Workflow Changes",
    "text": "If Your Workflow Changes\nAI use is iterative and experimental.\n\nIf your approach changes during the semester, simply update your workflow statement.\n\n\nNo judgment — experimentation is encouraged!"
  },
  {
    "objectID": "slides/01-introducing-computing-in-the-humanities.html#using-agentic-ai",
    "href": "slides/01-introducing-computing-in-the-humanities.html#using-agentic-ai",
    "title": "Introductions & Installations",
    "section": "Using Agentic AI",
    "text": "Using Agentic AI\nIf you use highly agentic AI tools, you’ll be held to higher standards:\n\nMore sophisticated analysis\nRigorous documentation of AI use\nDeeper engagement with scholarship\n\n\nIf AI does more technical work, you show more intellectual work."
  },
  {
    "objectID": "slides/01-introducing-computing-in-the-humanities.html#what-counts-as-agentic-ai",
    "href": "slides/01-introducing-computing-in-the-humanities.html#what-counts-as-agentic-ai",
    "title": "Introductions & Installations",
    "section": "What Counts as Agentic AI?",
    "text": "What Counts as Agentic AI?\n\nTools that autonomously write, debug, and execute complex code\nTools that independently conduct multi-step research\nTools that generate substantial written work with minimal input\n\n\nIf you’re unsure, ask!"
  },
  {
    "objectID": "slides/01-introducing-computing-in-the-humanities.html#ai-access-equity",
    "href": "slides/01-introducing-computing-in-the-humanities.html#ai-access-equity",
    "title": "Introductions & Installations",
    "section": "AI Access & Equity",
    "text": "AI Access & Equity\nWe use free tools:\n\nGitHub Copilot (free with GitHub Student Developer Pack)\nOpen-source local models\n\n\nIf using paid tools, disclose and check for education discounts."
  },
  {
    "objectID": "slides/01-introducing-computing-in-the-humanities.html#ai-for-coding",
    "href": "slides/01-introducing-computing-in-the-humanities.html#ai-for-coding",
    "title": "Introductions & Installations",
    "section": "AI for Coding",
    "text": "AI for Coding\nYou may use AI to help write, debug, and understand code.\n\nBut make sure you understand what the code does.\n\n\nIf code breaks, you need to fix it."
  },
  {
    "objectID": "slides/01-introducing-computing-in-the-humanities.html#ai-for-written-work",
    "href": "slides/01-introducing-computing-in-the-humanities.html#ai-for-written-work",
    "title": "Introductions & Installations",
    "section": "AI for Written Work",
    "text": "AI for Written Work\nYou may use AI for brainstorming, outlining, drafting, or editing.\n\nYour ideas, arguments, and voice should be yours.\n\n\nIf an essay reads like it was primarily AI-generated or if you cannot answer questions about your submissions, you will be dinged points."
  },
  {
    "objectID": "slides/01-introducing-computing-in-the-humanities.html#ai-for-data-work",
    "href": "slides/01-introducing-computing-in-the-humanities.html#ai-for-data-work",
    "title": "Introductions & Installations",
    "section": "AI for Data Work",
    "text": "AI for Data Work\nYou may use AI for data collection, cleaning, analysis, or documentation.\n\nYou make the interpretive decisions.\n\n\nYour work must demonstrate you understand your methodology deeply."
  },
  {
    "objectID": "slides/01-introducing-computing-in-the-humanities.html#the-bottom-line",
    "href": "slides/01-introducing-computing-in-the-humanities.html#the-bottom-line",
    "title": "Introductions & Installations",
    "section": "The Bottom Line",
    "text": "The Bottom Line\nAI should make you more capable, not less thoughtful.\n\nIt should amplify your learning, not replace it.\n\n\nIf you can’t explain what your code does or why your essay makes certain arguments, stop and reassess."
  },
  {
    "objectID": "slides/01-introducing-computing-in-the-humanities.html#example-citation",
    "href": "slides/01-introducing-computing-in-the-humanities.html#example-citation",
    "title": "Introductions & Installations",
    "section": "Example Citation?",
    "text": "Example Citation?\nWhen using AI tools like Claude Code, cite them as you would software:\n\nAnthropic. (2026). Claude Code (Version 4.5) [AI coding assistant]. https://claude.ai/claude-code\n\nOr in prose: “These slides were created with assistance from Claude Code (Anthropic, 2026), an AI coding assistant used for formatting Quarto/Reveal.js syntax.”"
  },
  {
    "objectID": "slides/01-introducing-computing-in-the-humanities.html#questions-3",
    "href": "slides/01-introducing-computing-in-the-humanities.html#questions-3",
    "title": "Introductions & Installations",
    "section": "Questions?",
    "text": "Questions?\nRemember:\n\nCommunicate early and often\nEngage thoughtfully with AI\nRespect the land and each other\n\n\nFull policies available on the course website and in the syllabus on Canvas."
  },
  {
    "objectID": "slides/01-introducing-computing-in-the-humanities.html#traditional-territory",
    "href": "slides/01-introducing-computing-in-the-humanities.html#traditional-territory",
    "title": "Introductions & Installations",
    "section": "Traditional Territory",
    "text": "Traditional Territory\nThese lands were the traditional territory of these Native Nations prior to their forced removal;\n\nThese lands continue to carry the stories of these Nations and their struggles for survival and identity."
  },
  {
    "objectID": "slides/01-introducing-computing-in-the-humanities.html#our-responsibility",
    "href": "slides/01-introducing-computing-in-the-humanities.html#our-responsibility",
    "title": "Introductions & Installations",
    "section": "Our Responsibility",
    "text": "Our Responsibility\nAs a land-grant institution, the University of Illinois has a particular responsibility to acknowledge the peoples of these lands, as well as the histories of dispossession that have allowed for the growth of this institution for the past 150 years. We are also obligated to reflect on and actively address these histories and the role that this university has played in shaping them. This acknowledgement and the centering of Native peoples is a start as we move forward for the next 150 years."
  },
  {
    "objectID": "slides/01-introducing-computing-in-the-humanities.html#beyond-platitudes-what-does-this-mean",
    "href": "slides/01-introducing-computing-in-the-humanities.html#beyond-platitudes-what-does-this-mean",
    "title": "Introductions & Installations",
    "section": "Beyond Platitudes: What does this mean?",
    "text": "Beyond Platitudes: What does this mean?\nWhile this acknowledgement is important, I find that these words can be difficult to understand or visualize.\n\nLet’s look at the first two digital humanities projects in this course to help us understand."
  },
  {
    "objectID": "slides/01-introducing-computing-in-the-humanities.html#native-land-digital",
    "href": "slides/01-introducing-computing-in-the-humanities.html#native-land-digital",
    "title": "Introductions & Installations",
    "section": "Native Land Digital",
    "text": "Native Land Digital\nVisualizes indigenous lands worldwide, built by a Canadian non-for-profit, that helps us see how this dispossession has shaped our very understanding of geography and political identity."
  },
  {
    "objectID": "slides/01-introducing-computing-in-the-humanities.html#native-land-digital-1",
    "href": "slides/01-introducing-computing-in-the-humanities.html#native-land-digital-1",
    "title": "Introductions & Installations",
    "section": "Native Land Digital",
    "text": "Native Land Digital\n\n\n\nNative Lands Map of Illinois"
  },
  {
    "objectID": "slides/01-introducing-computing-in-the-humanities.html#land-grab-universities",
    "href": "slides/01-introducing-computing-in-the-humanities.html#land-grab-universities",
    "title": "Introductions & Installations",
    "section": "Land Grab Universities",
    "text": "Land Grab Universities\nHow the Morrill Act dispossessed tribal lands, signed in 1862, dispossessed tribal lands to fund the creation of public state universities. The project was built by Robert Lee, Tristane Ahtone, Margaret Pearce, Kalen Goodluck, Geoff McGhee, and Cody Leff and published by High Country News"
  },
  {
    "objectID": "slides/01-introducing-computing-in-the-humanities.html#land-grab-universities-1",
    "href": "slides/01-introducing-computing-in-the-humanities.html#land-grab-universities-1",
    "title": "Introductions & Installations",
    "section": "Land Grab Universities",
    "text": "Land Grab Universities\n\n\n\nLand Grab Universities - Illinois"
  },
  {
    "objectID": "slides/01-introducing-computing-in-the-humanities.html#final-discussion-questions",
    "href": "slides/01-introducing-computing-in-the-humanities.html#final-discussion-questions",
    "title": "Introductions & Installations",
    "section": "Final Discussion Questions",
    "text": "Final Discussion Questions\nWhat is computing in the humanities?\n\nWhat is culture as data?\n\n\nVisit What is Digital Humanities? and refresh a few times\n\n\nAlso visit Open Syllabus Project"
  },
  {
    "objectID": "policies.html",
    "href": "policies.html",
    "title": "Course Policies",
    "section": "",
    "text": "Note\n\n\n\nAny statement with an * has been copied from the iSchool Syllabus Template."
  },
  {
    "objectID": "policies.html#grades",
    "href": "policies.html#grades",
    "title": "Course Policies",
    "section": "Grades",
    "text": "Grades\n\nIncomplete grades\nAn exceptional request for an incomplete grade is most often granted to students encountering a medical emergency or other extraordinary circumstances beyond their control. Students must request an incomplete grade from the instructor. The instructor and student will agree on a due date for completion of coursework. The student must submit an Incomplete Form signed by the student, the instructor, and the student’s academic advisor to the front office: https://uofi.app.box.com/s/sx7arobhr0gfw12teaetmp1qq32ifdrd Please see the Student Code for full details: http://studentcode.illinois.edu/article3/part1/3-104/\n\n\nGrading Scale\nInclude the grading scale used in the course. If you permit A+ grades, or have other measures for grading please include them here. Suggested grading scale:\n\n\n\nPercentage Range\nGrade\n\n\n\n\n94-100\nA\n\n\n90-93\nA-\n\n\n87-89\nB+\n\n\n83-86\nB\n\n\n80-82\nB-\n\n\n77-79\nC+\n\n\n73-76\nC\n\n\n70-72\nC-\n\n\n67-69\nD+\n\n\n63-66\nD\n\n\n60-62\nD-\n\n\n59 and below\nF"
  },
  {
    "objectID": "policies.html#attendance",
    "href": "policies.html#attendance",
    "title": "Course Policies",
    "section": "Attendance*",
    "text": "Attendance*\nThe iSchool expects students to attend all classes except in cases of emergency. Student Code on Attendance: http://studentcode.illinois.edu/article1/part5/1-501/\nI believe that in-person attendance is important for your learning and for building a collaborative classroom community. However, I also recognize that life happens—illness, family emergencies, mental health needs, and other circumstances sometimes require missing class.\nIf You Need to Miss Class: If you are feeling unwell or have been exposed to illness, please stay home and prioritize your health. I do not require doctor’s notes for absences. However, please keep in mind that if you miss a substantial portion of class meetings, it will be difficult to make up missed content and that you need to coordinate with your group members who depend on your contributions.\nWhen you miss class:\n\nMessage me on Slack as soon as possible to let me know\nInform your group members so they can plan accordingly\nCheck in with me about making up missed content and assignments\nReview materials posted on the course website\n\nI am happy to help you catch up on content within reason, though I cannot reteach multiple weeks of material. If you are facing challenges that require extended absences, please reach out so we can discuss accommodations.\n\nInformation Overload Days:\nAt the same time, at points throughout the semester you might simply need a break. Instead of missing class because you are overwhelmed with your workload, please let me know how you are feeling and please consider taking an information overload day rather than missing class outright. If you let me know in advance that you need more bandwidth for whatever reason, you will be excused from the assigned materials and in-class discussion. Instead you will be expected to actively listen to the discussion and then consult with the instructor about how to make up any relevant assignments at a later date. You will also have to inform your group of your status and help them decide how to make up for missed work. But hopefully showing each other grace and kindness can further foster your collaboration.\nI realize the workload for this course is significant and that you are balancing a number of other commitments at the same time, so information overload days are intended to help you manage your stress and at the same time help you not fall behind in the course. You are welcome to two of these without any questions, but after that point it would be helpful to have a conversation about your situation so that you don’t fall behind.1\n\n\nWorst case scenarios:\nIf you or someone close to you becomes ill and you are no longer able to attend class or complete the final project, I will in consultation with you determine a final grade based on your existing work, and/or advocate for you to be able to move the course to pass/fail.\nFor groups that lose a member(s) in such a scenario, the Instructor will work with you to pivot and determine how to best complete your final project. Members of the group will not be penalized for the missing member, but instead evaluated on their ability to manage the situation.\nDepending on the urgency of your situation, I realize that communicating with your instructor is a low priority, but please get in touch when you can to let me know if anything comes up."
  },
  {
    "objectID": "policies.html#communication-and-respect",
    "href": "policies.html#communication-and-respect",
    "title": "Course Policies",
    "section": "Communication and Respect",
    "text": "Communication and Respect\nWe will be meeting in person, barring any emergencies, but we will also be using Slack, an asynchronous communication platform, to encourage communication beyond our in-person meetings. Our Slack server is the DH@UIUC Slack (DH stands for digital humanities) and you can access the link in Canvas. Once you have joined our Slack, you should join our #is310-spring-2026 channel (or ask one of the instructors for assistance joining). We will discuss more about Slack procedures in class, and you can find more information about the platform on our course website here\nYou are also welcome to set up meetings with the instructor using this Calendly link or to email the instructor at zleblanc@illinois.edu.\nMuch of this course is experimental, and students often have a variety of backgrounds and previous relevant experience. This diversity is what makes our course so unique and enriching. Consequently, respect is a core component of our classroom culture, both in in-person interactions and on virtual platforms like Slack. Embracing a respectful attitude in all forms of communication is crucial, as it ensures a productive and positive learning environment. Whether in class discussions, Slack messages, or emails, being considerate and understanding of different perspectives and experiences is key. Remember, every opinion, question, and idea deserves a respectful response and consideration.\nRule of thumb from here on out is when in doubt, ask questions and over-communicate, but do so respectfully!"
  },
  {
    "objectID": "policies.html#disruptive-behavior",
    "href": "policies.html#disruptive-behavior",
    "title": "Course Policies",
    "section": "Disruptive behavior*",
    "text": "Disruptive behavior*\nBehavior that persistently or grossly interferes with classroom activities is considered disruptive behavior and may be subject to disciplinary action. Such behavior inhibits other students’ ability to learn and an instructor’s ability to teach. A student responsible for disruptive behavior may be required to leave class pending discussion and resolution of the problem and may be reported to the Office for Student Conflict Resolution (https://conflictresolution.illinois.edu; conflictresolution@illinois.edu; 333-3680) for disciplinary action."
  },
  {
    "objectID": "policies.html#statement-of-inclusion",
    "href": "policies.html#statement-of-inclusion",
    "title": "Course Policies",
    "section": "Statement of Inclusion*",
    "text": "Statement of Inclusion*\nhttps://diversity.illinois.edu/about/senate-diversity-resolution/ As the state’s premier public university, the University of Illinois at Urbana-Champaign’s core mission is to serve the interests of the diverse people of the state of Illinois and beyond. The institution thus values inclusion and a pluralistic learning and research environment, one which we respect the varied perspectives and lived experiences of a diverse community and global workforce. We support diversity of worldviews, histories, and cultural knowledge across a range of social groups including race, ethnicity, gender identity, sexual orientation, abilities, economic class, religion, and their intersections."
  },
  {
    "objectID": "policies.html#religious-observances",
    "href": "policies.html#religious-observances",
    "title": "Course Policies",
    "section": "Religious Observances*",
    "text": "Religious Observances*\nIn keeping with our Statement of Inclusion and Illinois law, the University is required to reasonably accommodate its students’ religious beliefs, observances, and practices in regard to admissions, class attendance, and the scheduling of examinations and work requirements.\nReligious Observance Accommodation Request form: https://cm.maxient.com/reportingform.php?UnivofIllinois&layout_id=19 Other accommodations may be available."
  },
  {
    "objectID": "policies.html#accessibility-statement",
    "href": "policies.html#accessibility-statement",
    "title": "Course Policies",
    "section": "Accessibility Statement*",
    "text": "Accessibility Statement*\nTo ensure disability-related concerns are properly addressed from the beginning of the semester, I request that students with disabilities who require assistance to participate in this class contact me as soon as possible to discuss your needs and any concerns you may have. The University of Illinois may be able to provide additional resources to assist you in your studies through the office of Disability Resources and Educational Services (DRES). This office can assist you with disability-related academic adjustments and/or auxiliary aids. Please contact them as soon as possible by visiting the office in person: 1207 S. Oak St., Champaign; visiting the website: http://disability.illinois.edu; calling (217) 333-4603 (V/TTY); or via e-mail disability@illinois.edu. NOTE: I do not require a letter from DRES in order to discuss your requested accommodations."
  },
  {
    "objectID": "policies.html#community-of-care",
    "href": "policies.html#community-of-care",
    "title": "Course Policies",
    "section": "Community of Care*",
    "text": "Community of Care*\nAs members of the Illinois community, we each have a responsibility to express care and concern for one another. If you come across a classmate whose behavior concerns you, whether in regards to their well-being or yours, we encourage you to refer this behavior to the Student Assistance Center (217-333-0050 or http://odos.illinois.edu/community-of-care/referral/). Based on your report, the staff in the Student Assistance Center reaches out to students to make sure they have the support they need to be healthy and safe.\nFurther, as a Community of Care, we want to support you in your overall wellness. We know that students sometimes face challenges that can impact academic performance (examples include mental health concerns, food insecurity, homelessness, personal emergencies). Should you find that you are managing such a challenge and that it is interfering with your coursework, you are encouraged to contact the Student Assistance Center (SAC) in the Office of the Dean of Students for support and referrals to campus and/or community resources.\n\nMental Health Resources*\nSignificant stress, mood changes, excessive worry, substance/alcohol misuse or interferences in eating or sleep can have an impact on academic performance, social development, and emotional wellbeing. The University of Illinois offers a variety of confidential services including individual and group counseling, crisis intervention, psychiatric services, and specialized screenings which are covered through the Student Health Fee. If you or someone you know experiences any of the above mental health concerns, it is strongly encouraged to contact or visit any of the University’s resources provided below. Getting help is a smart and courageous thing to do for yourself and for those who care about you.\n\nCounseling Center (217) 333-3704\nMcKinley Health Center (217) 333-2700\nNational Suicide Prevention Lifeline (800) 273-8255\nRosecrance Crisis Line (217) 359-4141 (available 24/7, 365 days a year)\n\nIf you are in immediate danger, call 911."
  },
  {
    "objectID": "policies.html#emergency-response-recommendations",
    "href": "policies.html#emergency-response-recommendations",
    "title": "Course Policies",
    "section": "Emergency Response Recommendations*",
    "text": "Emergency Response Recommendations*\nEmergency response recommendations and campus building floor plans can be found at the following website: https://police.illinois.edu/em/run-hide-fight/. I encourage you to review this website within the first 10 days of class."
  },
  {
    "objectID": "policies.html#additional-resources",
    "href": "policies.html#additional-resources",
    "title": "Course Policies",
    "section": "Additional Resources*",
    "text": "Additional Resources*\nStudents experiencing economic hardships resulting in food insecurity, housing insecurity, homelessness, or other issues that may affect the quality of their work, are encouraged to reach out to iSchool Assistant Dean for Student Affairs, April Carter at acart22@illinois.edu or call 217-333-0532."
  },
  {
    "objectID": "policies.html#sexual-misconduct-reporting-obligation",
    "href": "policies.html#sexual-misconduct-reporting-obligation",
    "title": "Course Policies",
    "section": "Sexual Misconduct Reporting Obligation*",
    "text": "Sexual Misconduct Reporting Obligation*\nThe University of Illinois is committed to combating sexual misconduct. Faculty and staff members are required to report any instances of sexual misconduct to the University’s Title IX Office. In turn, an individual with the Title IX Office will provide information about rights and options, including accommodations, support services, the campus disciplinary process, and law enforcement options.\nA list of the designated University employees who, as counselors, confidential advisors, and medical professionals, do not have this reporting responsibility and can maintain confidentiality, can be found here: wecare.illinois.edu/resources/students/#confidential. Other information about resources and reporting is available here: wecare.illinois.edu."
  },
  {
    "objectID": "policies.html#library-resources",
    "href": "policies.html#library-resources",
    "title": "Course Policies",
    "section": "Library Resources*",
    "text": "Library Resources*\nhttps://www.library.illinois.edu/infosci/"
  },
  {
    "objectID": "policies.html#the-ischool-academic-support-center",
    "href": "policies.html#the-ischool-academic-support-center",
    "title": "Course Policies",
    "section": "The iSchool Academic Support Center*",
    "text": "The iSchool Academic Support Center*\nThe iSchool Academic Support Center offers academic success workshops; coordinates with faculty and instructors to provide support for specific courses or exams; and curates and shares academic support resources. Appointments are available online and in person in room 4020 (Academic Support Center) in the 614 E. Daniel iSchool building. For more information including hours of operation, please visit https://ischool.illinois.edu/student-life/academic-support-center."
  },
  {
    "objectID": "policies.html#academic-and-self-integrity",
    "href": "policies.html#academic-and-self-integrity",
    "title": "Course Policies",
    "section": "Academic and Self Integrity*",
    "text": "Academic and Self Integrity*\nThe iSchool has the responsibility for maintaining academic integrity so as to protect the quality of education and research in our school and to protect those who depend on our integrity. Consequences of academic integrity infractions may be serious, ranging from a written warning to a failing grade for the course or dismissal from the University. See the student code for academic integrity requirements: http://studentcode.illinois.edu/article1/part4/1-401/\nLong story, short: don’t cheat. If you need help, see the instructor. I would rather you turn in work late, than have you plagiarize materials and have to report you for violating the student code.\nWe’ll discuss what constitutes plagiarism for the course (it gets thorny around coding sometimes), but a good rule of thumb is to cite as much as possible. All scholarship is a collective endeavor, and acknowledging those whose work has influenced you is both intellectually and politically imperative.\nSara Ahmed writes that citation serves as feminist bricks and feminist memory: &gt; “Citation is how we acknowledge our debt to those who came before; those who helped us find our way when the way was obscured because we deviated from the paths we were told to follow. In this book, I cite feminists of color who have contributed to the project of naming and dismantling the institutions of patriarchal whiteness” (Ahmed 17). Acknowledging and establishing feminist genealogies is part of the work of producing more just forms of knowledge and intellectual practice. —Beverly Weber, Digital Feminist Collective2"
  },
  {
    "objectID": "policies.html#ai-policy",
    "href": "policies.html#ai-policy",
    "title": "Course Policies",
    "section": "AI Policy",
    "text": "AI Policy\nThis course explicitly allows the use of so-called AI tools, and we will be experimenting primarily with GitHub Co-Pilot throughout the semester for our programming related research and learning. Such a choice might seem strange given current discourses around AI, but this course’s approach to AI is premised on the belief that these tools are not going anyways and that we need to engage with them critically.\nTo that end, students are allowed and encouraged to use AI tools for various assignments and projects, but you are required to explicitly share your preferred workflow in this course. In the first two weeks of class, you will submit your preferred Course Workflow as your first assignment where you will address the following questions:\n\nIf you plan to use AI tools: Describe how you intend to integrate them into your work. Which tools will you use? For what purposes? (Examples: coding assistance, brainstorming, proofreading, data analysis, literature search, etc.)\nIf you prefer not to use AI tools: Explain your alternative workflow and how you plan to complete assignments without AI assistance. You will need to document your process differently (see below).\nIf you plan to use local models: Indicate this and consult with the instructor for setup assistance. Running models locally gives you more control and privacy.\n\n\nIf Your Workflow Changes\nAI use is iterative and experimental. If your approach changes during the semester (e.g., you decide to start using AI after initially opting out, or vice versa), simply update your workflow statement. No judgment—experimentation is encouraged.\n\n\nOpting Out: Alternative Documentation Requirements\nIf you choose not to use AI tools, you should contact the Instructor either in Class or via Slack to determine the best approach to document your workflows. This may involve using versioning functionality in git and Google Drive, or other tools. The goal is ensuring I can assess your learning process without AI assistance as evidence.\n\n\nUsing Agentic AI: Higher Expectations\nIf you choose to use highly agentic AI tools (tools that can autonomously complete complex tasks with minimal guidance), you’ll be held to higher standards for the course, but specifically the final project, including more more sophisticated analysis in your data essay; more rigorous documentation of how you used AI; more extensive engagement with scholarship; and deeper reflection on how automation influenced your final project. Essentially, if AI is doing more of the technical work, you need to demonstrate more sophisticated intellectual work. This is not intended as a penalty or to dissuade you from using these tools—it is recognizing that using powerful tools requires showing you understand their implications and raising expectations so that you can maximize your learning in this course.\n\nWhat “Agentic AI” Means:\nFor this course, agentic AI includes:\n\nTools that can autonomously write, debug, and execute complex code with minimal guidance\nTools that can independently conduct multi-step research or analysis tasks\nTools that can generate substantial portions of written work with minimal human input\n\nIf you are unsure whether a tool counts as “agentic,” ask. Better to clarify than assume.\n\n\n\nAccess & Equity with AI\nTo ensure equal access, we will primarily utilize AI tools that are free of charge, specifically GitHub Copilot (free for students with GitHub Student Developer Pack) or open-source local models (instructor can provide setup guidance). You will need to disclose if you are using a paid tool, and I would strongly suggest first checking if whatever tool you are using has an education discount.\nStudents are encouraged to explore using local AI models (models that run on your own computer rather than in the cloud). Benefits include:\n\nGreater privacy—your data does not leave your machine\nNo usage limits or costs\nOpportunity to understand how models actually work\nExperience with a different paradigm of AI use\n\nThe instructor can provide setup instructions for local models like Llama, Mistral, or others. If you are interested, let me know either in Class or via Slack.\n\n\nSharing Experiences\nThroughout the semester, you are encouraged to share either in Class, Slack, or via assignments:\n\nWhich AI tools you find helpful for specific tasks\nPrompts or workflows that work particularly well\nFailures or frustrations with AI tools\nEthical concerns or unexpected consequences\n\nLearning from each other’s experiments will hopefully make everyone’s work stronger and help us collectively navigate this changing landscape.\n\n\nWhat This Means for Your Work\nFor coding assignments: You may use AI to help write, debug, and understand code, but make sure you understand what the code does—you are responsible for explaining it. If code breaks or does not work as expected, you need to fix it. A helpful paradigm shift may be trying to think about understanding code not through building it, but breaking it even.\nFor written work: You may use AI for brainstorming, outlining, drafting, or editing, but your ideas, arguments, and voice should be yours. If an essay reads like it was primarily AI-generated, you will be dinged points. In particular, if you plan to use AI then I will be grading grammar, syntax, and flow. Regardless though, this course encourages you to be creative, original, and ambitious.\nFor data work: You may use AI to help with data collection, cleaning, analysis, or documentation, but you need to make the interpretive decisions about what matters. Your assignments and data essays must demonstrate you understand your methodology deeply.\n\n\nAI Academic Integrity\nUsing AI is not cheating in this course—it is expected. However, plagiarism is still plagiarism. Do not submit work generated by others (human or AI) as if it is entirely your own intellectual contribution. You are expected to be honest and responsible for your submissions. If AI generates incorrect information or problematic code, you are responsible for catching and fixing it.\nThe Bottom Line: AI should make you more capable, not less thoughtful. It should amplify your learning, not replace it. If you find yourself mindlessly accepting AI suggestions without understanding them, or if you cannot explain what your code does or why your essay makes certain arguments, stop and reassess. That is a sign AI is replacing your learning rather than supporting it, and our goal is to experiment with how AI can augment our work. If you are unsure of how best to work, please reach out to the Instructor and we can discuss options.\n\n\nWhy This Policy? Questions?\nUltimately, AI is increasingly permeating every aspect of academia, as well as our day-to-day lives. Rather than shy away from this fact, we will explore how to integrate AI into our work in sustainable and responsible ways, reflecting on the impact of these technologies and using them to further your learning experience.\nIf you have concerns about this policy, questions about what is allowed, or uncertainty about whether your approach is appropriate, please ask. Office hours, email, and Slack are all good venues. There are no wrong questions about AI—we are all figuring this out together. This course is about becoming thoughtful, critical users of tools like AI that are reshaping our world. And that requires engaging with them directly, not avoiding them."
  },
  {
    "objectID": "policies.html#land-acknowledgement-statement",
    "href": "policies.html#land-acknowledgement-statement",
    "title": "Course Policies",
    "section": "Land acknowledgement Statement*",
    "text": "Land acknowledgement Statement*\nAdopted by the University of Illinois in 2018\nMore information: https://chancellor.illinois.edu/land_acknowledgement.html\nI/We would like to begin today by recognizing and acknowledging that we are on the lands of the Peoria, Kaskaskia, Piankashaw, Wea, Miami, Mascoutin, Odawa, Sauk, Mesquaki, Kickapoo, Potawatomi, Ojibwe, and Chickasaw Nations. These lands were the traditional territory of these Native Nations prior to their forced removal; these lands continue to carry the stories of these Nations and their struggles for survival and identity.3\nAs a land-grant institution, the University of Illinois has a particular responsibility to acknowledge the peoples of these lands, as well as the histories of dispossession that have allowed for the growth of this institution for the past 150 years. We are also obligated to reflect on and actively address these histories and the role that this university has played in shaping them. This acknowledgement and the centering of Native peoples is a start as we move forward for the next 150 years.\nWhile this acknowledgement is important, I find that these words can be difficult to understand or visualize. To help us understand this history, there are two phenomenal digital humanities projects. The first is Native Land Digital that actually visualizes the lands of these Native Nations, as well as all indigenous lands worldwide. Available at https://native-land.ca/ and built by a Canadian non-for-profit, I would highly encourage you to spend some time looking through the project to understand and see how this dispossession has shaped our very understanding of geography and political identity.\n\n\n\nNative Lands Map of Illinois\n\n\nThe second project is Land Grab Universities which looks specifically at how the Morill Act, signed in 1862, dispossessed tribal lands to fund the creation of public state universities. The project was built by Robert Lee, Tristane Ahtone, Margaret Pearce, Kalen Goodluck, Geoff McGhee, and Cody Leff and published by High Country News https://www.landgrabu.org/.\n\n\n\nLand Grab Universities - Illinois"
  },
  {
    "objectID": "policies.html#footnotes",
    "href": "policies.html#footnotes",
    "title": "Course Policies",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe concept of an information overload day is inspired and adapted from Ryan Cordell’s Building A Better Book Syllabus https://f19bbb.ryancordell.org/policies/↩︎\nBeverly Weber, “The Politics of Citation” Digital Feminist Collective https://digitalfeministcollective.net/index.php/2018/01/13/the-politics-of-citation/ and Ahmed, Sara. Living a Feminist Life. Duke University Press Books, 2017.↩︎\nFor those unfamiliar with these tribes, here is the correct phonetic pronunciations: Peoria (pee-OHR-ee-uh), Kaskaskia (kas-KAS-kee-uh), Piankashaw (pee-AN-kuh-shaw), Wea (WEE-uh), Miami (my-AM-ee), Mascoutin (mas-KOO-tin), Odawa (oh-DAH-wuh), Sauk (sawk), Mesquaki (mes-KWAH-kee), Kickapoo (KIK-uh-poo), Potawatomi (pot-uh-WAH-tuh-mee), Ojibwe (oh-JIB-way), Chickasaw (CHIK-uh-saw).↩︎"
  },
  {
    "objectID": "materials/introducing-humanities-computing/03-intro-versioning.html",
    "href": "materials/introducing-humanities-computing/03-intro-versioning.html",
    "title": "Introduction to Versioning & Git",
    "section": "",
    "text": "Note\n\n\n\n⚡️ This lesson has been adapted from the Digital Humanities Research Institute’s Workshop on git https://github.com/DHRI-Curriculum/DHRIFT/blob/main/workshops/git.md. Many thanks to the authors for sharing their materials!\nSo far, we have been using the command line in the terminal, installed GitHub Co-Pilot and git, but we have yet to use either for their main purpose: version control and collaborative coding. In this lesson, we will explore what version control is and how it works, as well as how to use GitHub to collaborate with others and share our code and datasets.",
    "crumbs": [
      "Resources",
      "Introductions & Installations",
      "Introduction to Versioning & Git"
    ]
  },
  {
    "objectID": "materials/introducing-humanities-computing/03-intro-versioning.html#what-is-git",
    "href": "materials/introducing-humanities-computing/03-intro-versioning.html#what-is-git",
    "title": "Introduction to Versioning & Git",
    "section": "What is Git?",
    "text": "What is Git?\nWhile we already discussed what GitHub is in-depth (link here), we have yet to discuss git much.\nGit is a software used for version control, that is keeping track of changes in your files and folders over time. Git requires changes to be saved (committed) before they can be combined (merged) with the main project. What that means in lay terms is that Git is the software that allows you to track changes in your project and collaborate with others. You are already familiar with this concept if you’ve ever used Google Docs or Microsoft Word’s “Track Changes” feature, but in those you get the version history automatically. With git, we have to tell it what to track and when to track it.\nCreated in 2005 by Linus Torvalds, the creator of Linux, the open-source operating system that powers much of the internet, Git is now the most popular version control system in the world.1 It is used by millions of developers and researchers to collaborate on projects and keep track of changes in their code and data\nA very important thing to understand is that while git and GitHub sound the same, they are two separate technologies. Git is a version control system, while GitHub is a platform for hosting git repositories (in git, a folder is called a repository). We’ll be using both in this course, but it’s important to realize they are not identical.\nSo here’s a few bullet points to remember this distinction:\n\n\n\n\n\n\n\n\n\nGit\nGitHub\n\n\n\n\nDescription\nis a version control system (aka a software that tracks changes in files and folders)\nis a remote platform for hosting git repositories (aka folders)\n\n\nLocation\nexists locally on your computer (you need to install it)\nis a website (you don’t need to install it)\n\n\n\n\nEven though these two technologies are separate, they are often used together. For example, you can use git to track changes in your files and folders locally on your computer, and then push those changes to a remote repository on GitHub. This is what we will be doing in this course.\n\nWhy is Git Useful for Computing in the Humanities?\nAlthough Git and version control systems are predominantly used in software development, they offer several advantages for scholars in Digital Humanities. Similar to how you might handle versions of a paper, essay, or data set, Git allows you to:\n\nKeep track of changes in plain text files\nCollaborate in real-time with other researchers\nEasily revert to earlier versions\nCreate different branches to explore new ideas without disturbing the main project, and see who added what to the project\n\nTo give a sense of what this version tracking looks like, here’s an example of an error I corrected in last week’s CLI tutorial, after one of your peers pointed it out to me:\n\nThis screenshot is from GitHub but shows what git tracks (confusing to combine both, but this visual is hopefully helpful!). You can see on the left, I deleted the previous erroneous line, and on the right, is the corrected version. This is a very simple example, but it gives you a sense of how git tracks changes.",
    "crumbs": [
      "Resources",
      "Introductions & Installations",
      "Introduction to Versioning & Git"
    ]
  },
  {
    "objectID": "materials/introducing-humanities-computing/03-intro-versioning.html#working-with-git",
    "href": "materials/introducing-humanities-computing/03-intro-versioning.html#working-with-git",
    "title": "Introduction to Versioning & Git",
    "section": "Working with Git",
    "text": "Working with Git\nFirst things first, let’s double check we all have git installed. To do this, we can use the git --version command. This will tell us what version of git we have installed.\ngit --version\nRemember that we run our commands in the terminal!\nHopefully that worked for everyone! If not, please let the instructors know.\nSince we have already installed git, we can start using it in our projects. But first, let’s create a new folder for our project in our home directory. We can do this by using the mkdir command and then the name of the folder we want to create. Let’s call our folder is310-first-repo.\nmkdir is310-first-repo\nNow let’s navigate into this folder using the cd command and then create a new file using the touch command (or if you’re on PowerShell New-Item -ItemType File -Path . -Name [file-name]). Let’s call this file first_file.txt.\nNext, we can add some text to this file using the following commands. Let’s add the following text to the file:\nUnix/Linux\necho \"This is my first file\" &gt; first_file.txt\nPowerShell\n\"This is my first file\" | Out-File -FilePath first_file.txt\nSo now we have a new folder and a new file. But what if we want to track the changes we make to this file? This is where git comes in.\n\nInitializing a Git Repository\nTo start tracking changes to our file, we need to initialize a git repository. We can do this by using the git init command. This will initialize a git repository in our current folder.\ngit init\nYou should see the following output:\nInitialized empty Git repository in /Users/YOU/is310-first-repo/.git/\nNow if we type ls -a or Get-ChildItem -Force we will see a new folder called .git. This is the folder that contains all of the information about our git repository. The .git folder is hidden by default, so we need to use the -a or -Force flag to see it. This means that if you try looking at your folder either in a file explorer or in the terminal with just ls, or even in the VS Code Explorer, you won’t see the .git folder.\nWith our git repository initialized, we can now start tracking changes to our file, but this is not automatic. Instead, we have to tell git which files to pay attention to and track, and also when to take a snapshot of our changes.\n\n\nAdding Files to the Staging Area\nTo do this, we need to add our file to the staging area. We can do this by using the git add command and then the name of the file we want to add. Let’s add our first_file.txt file.\ngit add first_file.txt\nNow if we type git status we will see that our file has been added to the staging area. This means that git is now tracking changes to our file.\ngit status\nWe should see the following output:\nOn branch main\n\nNo commits yet\n\nChanges to be committed:\n  (use \"git rm --cached &lt;file&gt;...\" to unstage)\n        new file:   first_file.txt\nThis is telling us that we are on the main branch, we have not made any commits yet, and we have one file in the staging area. You can run git status at any time to see the status of your repository.\n\n\nCommitting Changes\nNow that we have added our file to the staging area, git knows that we want it to track this file, but we have to tell it to finalize our snapshot of the file, which we do by committing our changes. To do this, we use the git commit command and then the -m flag to add a message to our commit. This message should describe the changes we made to the file. Let’s add the following message:\ngit commit -m \"Added first file to our repository\"\n\nCommit Messages\nIdeally, your commit messages should be short and descriptive. They should describe the changes you made to the file.\nIf you don’t add a message, git will automatically open a text editor in your terminal and ask you to add a message. You’ll likely see something that looks like the following:\n\nThis looks very intimidating, but it’s actually not that bad. This is the default text editor (think like VS Code, but much simpler) for git, which is called Vim. Vim is a very powerful text editor, but it has a steep learning curve. If you want to learn more about Vim, you can read this https://www.freecodecamp.org/news/vim-beginners-guide/.\nThe key thing to know is that if you get this screen you can either add a message or exit the text editor. To add a message, you need to press i on your keyboard. This will put you in insert mode, which means you can type. Then you can add your message. Once you have added your message, you need to press esc on your keyboard. This will take you out of insert mode. Then you need to type :wq and press enter. This will save your message and exit the text editor.\nIf you just want to exit the text editor, you can press esc on your keyboard. Then you need to type :q! and press enter. This will exit the text editor without saving your message. In which case, you’ll see the following message:\nAborting commit due to empty commit message.\nWhile doing this once or twice is fine, it can get annoying so it’s better to add a message to your commit with the -m flag. Be sure that you use quotations marks around your message, and that you use the same type of quotations marks. So if you use double quotes, use double quotes around your message. If you use single quotes, use single quotes around your message. If for some reason you end up mixing them, you’ll get a quote prompt in your terminal.\nquote&gt;\nThis is just git telling you that you didn’t close your quotation marks, so if you add the correct quotation marks and press enter, it will add your message.\nNow if we type git status again we will see that our file is no longer in the staging area. This means that our changes have been committed to our repository.\ngit status\nWe should see the following output:\nOn branch main\nnothing to commit, working tree clean\nThis is telling us that we are on the main branch, we have no changes to commit, and our working tree is clean. You can run git status at any time to see the status of your repository.\nThe DHRI lesson has the following metaphor/explanation for how we can conceptualize this process that I find very helpful:\n\nThinking of git as a camera taking a photo of your project at a particular point in time is a helpful way to understand how git works (also like thinking of it as a save point in a video game). Every time you commit your changes, you are taking a snapshot of your project. You can then go back to that snapshot at any time. But also sometimes you don’t want to save a snapshot or you want to redo it, git let’s you do that too.\n\n\n\nViewing the Version History\nNow that we have committed our changes, we can view the version history of our file. To do this, we use the git log command. This will show us the commit history of our file. We can see the commit message, the author, the date, and the commit hash. The commit hash is a unique identifier for each commit. It is a long string of letters and numbers that is used to identify each commit. We can use this commit hash to revert to a previous version of our file.\ngit log\nWhich should output something that looks like the following:\ncommit 8bb8306c1392eed52d4407eb16867a49b49a46ac (HEAD -&gt; main)\nAuthor: Your Name &lt;your-email-here@gmail.com&gt;\nDate:   Tue Jan 22 16:03:39 2024 -0400\n\n    Added first file to our repository\nThis is telling us that we have one commit, the commit is identified with that long string of letters and numbers (this is the commit hash), the commit is on the main branch. The Author and Date should correspond to your information and when you made the commit. Finally, we see our commit message.\nWe could now update the file and commit our changes again. If we did that, then our version history via git log would show the two commits.\nThese activities are the core features of git. The next step is to connect our local repository to a remote repository on GitHub.",
    "crumbs": [
      "Resources",
      "Introductions & Installations",
      "Introduction to Versioning & Git"
    ]
  },
  {
    "objectID": "materials/introducing-humanities-computing/03-intro-versioning.html#working-with-github",
    "href": "materials/introducing-humanities-computing/03-intro-versioning.html#working-with-github",
    "title": "Introduction to Versioning & Git",
    "section": "Working with GitHub",
    "text": "Working with GitHub\nBy now you should have your git configuration setup and have a GitHub account, but feel free to look back at our course tools lesson if you’re having any issues.\n\nCreating a New Repository\nTo get our local git repository into GitHub, the first thing we need is open a web browser and navigate to GitHub. The homepage is very full featured, and we’ll be discussing some of those later. First, we need to create a new repository. To do this, we need to click on the New button in the top right-hand corner of the screen. This will take us to a page where we can create a new repository.\n\nNow you’ll be asked to give your repository a name. Let’s create a repository called is310-first-repo. You can also add a description, choose whether it’s public or private, and add a license (this essentially tells others if they can reuse your materials or not). For us to see each other’s repositories, we need to select public.\n\nYou also have the option to initialize the repository with a README, which is a file that provides information about the project. We will be discussing this more later so for now leave this option unchecked. You can also add a .gitignore file, which is a file that tells Git to ignore certain files in the repository. Again, something we’ll discuss more in-depth in the coming weeks.\n\n\n\nFirst Commit\n\n\nOnce you create your repository it should like the photo above. If you selected the Initialize this repository with a README option, you will see a blank repository that looks like the photo below.\n\n\n\nFirst Commit With README\n\n\nTo understand what each button does feel free to browse through our advanced git and GitHub resource, but first let’s try to get our local repository onto GitHub.\n\n\nPushing to GitHub\nNow that we have created a new repository on GitHub, we need to connect it to our local repository. To do this, we need to add a remote repository. We can do this by using the git remote add command and then the name of the remote repository and the URL of the remote repository. Let’s call our remote repository origin and use the URL of our GitHub repository.\ngit remote add origin https://github.com/OWNER/REPOSITORY.git\nIn this case, we would replace OWNER with our GitHub username and REPOSITORY with the name of our repository. So for me, it would be ZoeLeBlanc/is310-first-repo.\nWe can check this by using the git remote -v command. This will show us the remote repositories we have added to our local repository.\ngit remote -v\nWhich should output something that looks like the following:\norigin  https://github.com/username/repository.git (fetch)\norigin  https://github.com/username/repository.git (push)\nNow that we can see that we have added our remote repository, we can push our local repository to our remote repository. We can do this by using the git push command.\nThis command has the following structure:\ngit push [remote repository] [flags] [local repository]\nSo all we need to do is specify the name of our remote repository and the name of our local repository. What’s a bit confusing is that we don’t need to write our GitHub URL here but instead can just write origin. If you look at the output from git remote -v again, you’ll notice it says origin next to our GitHub URL. This is because origin is the default name for our remote repository. We can change this, but for now we’ll just use origin.\nThen for our local repository, we don’t need to say our directory name but instead need to specify the name of the branch we want to push. We’ll discuss branches more later, but for now we can just use main. So our command will look like this:\ngit push origin main\nWhich should output something that looks like the following (though the exact details will be different depending on your repository, files, etc.):\nEnumerating objects: 3, done.\nCounting objects: 100% (3/3), done.\nWriting objects: 100% (3/3), 226 bytes | 226.00 KiB/s, done.\nTotal 3 (delta 0), reused 0 (delta 0), pack-reused 0\nTo github.com:ZoeLeBlanc/is310-first-repo.git\n * [new branch]      main -&gt; main\nIf you see a fatal error that looks like this:\nfatal: 'origin' does not appear to be a git repository\nfatal: Could not read from remote repository.\n\nPlease make sure you have the correct access rights\nand the repository exists.\nThat just means you forgot to add your remote repository. If you fix that and try again, it should work.\nNow if we go to our GitHub repository, we will see that our local repository has been pushed to our remote repository.\n\nConfiguring SSH Keys (Optional But Recommended)\n\nNow when you pushed to GitHub, you likely saw something similar to this image, where it was asking you to enter your username and password for your GitHub account. This is fine, but it can be annoying to have to do this every time you want to push changes from your local computer to your remote GitHub Repository.\nSo, instead we can configure SSH keys. SSH keys are a way to securely connect to a remote server without having to enter your username and password every time. To do this, we need to generate a new SSH key and then add it to our GitHub account.\nFirst, we should check for existing SSH keys. We can do this by using the ls command and then the name of the file we want to check.\nls ~/.ssh/id_*.pub\nNotice that we are using the ~ symbol. Remember, this is a shortcut for our home directory. So if we are using a Mac, this would be /Users/[USER NAME]/.ssh/id_*.pub. If we are using a Windows machine, this would be C:\\Users\\[USER NAME]\\.ssh\\id_*.pub. We can run this command from any directory and it will search our home directory for the file (which is why it’s important to know about relative and absolute file paths!).\nIf you see a file called id_rsa.pub or id_ed25519.pub then you already have an SSH key. If you don’t see this file, then you need to generate a new SSH key.\nTo generate a new SSH key, we need to use the ssh-keygen command and then the -t flag to specify the type of key we want to generate. Let’s generate a new RSA key.\nssh-keygen -t ed25519 -C \"your_email@example.com\"\nYou should replace your_email@example.com with the email you use for your GitHub account. Be sure to wrap your email in quotations, like in the example. If you get an error, you might be working with an older version of SSH. In that case, you can use the following command.\nssh-keygen -t rsa -b 4096 -C \"your_email@example.com\"\nBoth rsa and ed25519 are types of encryption, which is a way to secure your data. You can read more about encryption https://www.cloudflare.com/learning/ssl/what-is-encryption/.\nYou’ll be prompted to save your SSH key into a file. You can just press enter to save it in the default location, which is your home directory.\n&gt; Enter a file in which to save the key (/Users/YOU/.ssh/id_ALGORITHM): [Press enter]\nYou’ll then be prompted to enter a passphrase. You can enter a passphrase or leave it blank. If you enter a passphrase, you’ll have to enter it every time you use your SSH key. If you leave it blank, you won’t have to enter it every time you use your SSH key (I would recommend leaving it blank unless you’re very good at remembering/storing pass phrases, but it’s up to you!).\n\nNext we need to start the ssh-agent in the background. We can do this by using the eval command and then the ssh-agent command.\nFor Unix/Linux/WSL:\neval \"$(ssh-agent -s)\"\nFor PowerShell, make sure you are running PowerShell as an administrator and then run the following command:\nStart-Service ssh-agent\nWhich should output something that looks like the following:\n&gt; Agent pid 59566\nIf you are getting errors, you might need to use a slightly different command depending on your operating system. You can read more about this https://docs.github.com/en/authentication/connecting-to-github-with-ssh/generating-a-new-ssh-key-and-adding-it-to-the-ssh-agent#adding-your-ssh-key-to-the-ssh-agent.\n\nMac Users Configuration (Also for those running WSL on Windows)\nIf you are on a MacBook running Sierra 10.12.2 or later, you will need to do some manual configuration (you can check operating system version by clicking on the Apple icon in the top left-hand corner of your screen and then clicking on About This Mac).\nFirst, you’ll need to check if this file exists with this command:\nls -al ~/.ssh/config\nIf you don’t see a file called config, then you need to create one. You can do this by using the touch command and then the name of the file you want to create.\ntouch ~/.ssh/config\nThen you need to open it with a text editor. You can do this by using the open command and then the name of the file you want to open.\nopen ~/.ssh/config\nThis will open the file in your default text editor. Then you need to add the following lines to the file.\nHost *\n  AddKeysToAgent yes\n  IdentityFile  ~/.ssh/id_ed25519\nIf you added a passcode, you’ll need to add UseKeychain yes underneath the AddKeysToAgent yes.\nThen you need to save the file and exit the text editor. You can do this by pressing Command + S and then Command + Q.\n\n\nAdding SSH Key to SSH Agent\nNow we need to add our SSH key to the ssh-agent. We can do this by using the ssh-add command and then the name of the file we want to add.\nssh-add ~/.ssh/id_ed25519\nIf you added a passcode and are on a Macbook, you will need to alter the command slightly.\nssh-add --apple-use-keychain ~/.ssh/id_ed25519\nFinally, we need to add our SSH key to our GitHub account. To do this, we need to copy our SSH key to our clipboard.\nIf you are running either Unix/Linux, we can use the pbcopy command and then the name of the file we want to copy.\npbcopy &lt; ~/.ssh/id_ed25519.pub\nIf you are running WSL/PowerShell, we can use the clip command and then the name of the file we want to copy.\ncat ~/.ssh/id_ed25519.pub | clip.exe\nAnd now we can add our SSH key to our GitHub account. To do this, we need to go to our GitHub account and click on the Settings tab. Then we need to click on the SSH and GPG keys tab. Finally, we need to click on the New SSH key button and then paste our SSH key into the Key box. We can also add a title for our SSH key. Let’s call it My SSH Key.\nOnce we have add the SSH key, we can click on the Add SSH key button. Now we have added our SSH key to our GitHub account. Then we can just update our remote repository URL to use SSH instead of HTTPS. We can do this by using the git remote set-url command and then the name of the remote repository and the URL of the remote repository. Let’s call our remote repository origin and use the URL of our GitHub repository.\ngit remote set-url origin git@github.com:OWNER/REPOSITORY.git\nNow you should try pushing to GitHub again. The first time you do this, you might see something like the following:\nThe authenticity of host 'github.com (IP ADDRESS)' can't be established.\nRSA key fingerprint is SHA256:xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx.\nAre you sure you want to continue connecting (yes/no/[fingerprint])?\nThis is just git asking you if you want to connect to GitHub. You can type yes and press enter. Then you should see something like the following:\nWarning: Permanently added 'github.com,IP ADDRESS' (RSA) to the list of known hosts.\nThis is just git telling you that it has added GitHub and we can now push to our GitHub repository without having to enter our username and password every time 🥳.\nYou can read more about managing remote repositories here and managing SSH keys here.\n\n\n\n\nEditing Files in GitHub\nSo far, we have been editing our files locally and pushing them up to GitHub. However, we can also edit files in GitHub. This is a great way to make quick changes to a file or add a new file to a repository. Let’s try adding using the GitHub interface in the browser and add a special type of file called a README.md file.\nWe can do this through the web browser interface, by clicking the add file button and then selecting create new file.\n\nThen we need to give our file a name. Let’s call it README.md. The .md extension stands for markdown, which is a way to format text. We’ll be discussing this more later, but for now just know that .md files are used to format text. For now, let’s just add some text to our file. We can do this by typing in the text box. Let’s add the following text:\n# IS 310 Test Repository\n\nThis is my first repository.\nWe can see what this looks like by clicking on the Preview tab. This will show us a preview of our file.\n\nNow we need to commit our changes and file by clicking the green Commit changes box. This will prompt us for a commit message. Let’s add the following message:\nAdded README.md file\nFinally, we need to tell GitHub whether to create a new branch or commit to the main branch. Click main for now and then click Commit changes. Now we have added a new file to our repository and you should see it when you navigate to your repository.\n\nGitHub Dev (Optional, Additional Feature)\nBesides using the normal web interface, there’s also a new way to edit files in the GitHub browser interface, called GitHub Dev. This is a new feature that similar to what we just did, allows you to edit files in the browser and commit them to your repository. To access it, all you have to do is change github.com in your URL to github.dev.\nSo for us, we would change https://github.com/[USER NAME]/is310-first-repo to https://github.dev/[USER NAME]/is310-first-repo.\n\n\n\nGitHub Dev\n\n\nYou can also just hit the . key on your keyboard when you’re in a repository to open GitHub Dev.\nNow you should see a new interface that looks like this:\n\nYou’ll likely recognize this interface, since it’s an instance of Visual Studio Code (VS Code). This feature is completely optional, but can be helpful to know it exists, since you can also commit changes through this interface. You can read more about it https://docs.github.com/en/codespaces/the-githubdev-web-based-editor.\n\n\n\nPulling from GitHub\nNow we have two versions of our repository. One on our local computer and one on GitHub, and these two have separate files. How can we get this remote version into our local one? This is where pulling becomes useful.\nTo pull from GitHub, we need to use the git pull command and then the name of the remote repository and the name of the branch we want to pull. Let’s pull from our origin remote repository and the main branch.\ngit pull origin main\nWhich should output something that looks like the following:\nremote: Enumerating objects: 4, done.\nremote: Counting objects: 100% (4/4), done.\nremote: Compressing objects: 100% (2/2), done.\nremote: Total 3 (delta 0), reused 0 (delta 0), pack-reused 0\nUnpacking objects: 100% (3/3), 935 bytes | 233.00 KiB/s, done.\nFrom github.com:ZoeLeBlanc/is310-first-repo\n * branch            main       -&gt; FETCH_HEAD\n   d8dad7b..832b673  main       -&gt; origin/main\nUpdating d8dad7b..832b673\nFast-forward\n README.md | 1 +\n 1 file changed, 1 insertion(+)\n create mode 100644 README.md\nNow if we type ls we will see that we have a new file called README.md. This is the file we created in GitHub. We can also see the file in VS Code and edit it there.\nWe’ve officially closed the loop between our local and remote repositories. We can now edit our files locally and push them to GitHub, or edit our files in GitHub and pull them to our local repository.\n\nThis is the core workflow for using git and GitHub.",
    "crumbs": [
      "Resources",
      "Introductions & Installations",
      "Introduction to Versioning & Git"
    ]
  },
  {
    "objectID": "materials/introducing-humanities-computing/03-intro-versioning.html#homework-init-is310",
    "href": "materials/introducing-humanities-computing/03-intro-versioning.html#homework-init-is310",
    "title": "Introduction to Versioning & Git",
    "section": "Homework: Init IS310",
    "text": "Homework: Init IS310\nNow it’s your turn to try this out! Trying following the steps above to create a new GitHub repository called is310-coding-assignments in your GitHub account. For this assignment, I would highly recommend you use this git cheatsheet and also take a look at this overview of best practices, though we will be discussing this in-class as well.\nFor this assignment, you need to do the following:\n\nCreate a new GitHub repository called is310-coding-assignments.\nCreate a new directory in your local computer called is310-coding-assignments and enable it as a git repository.\nCreate a Markdown file called README.md within is310-coding-assignments.\nCreate a folder called images within is310-coding-assignments.\n\nThe final piece is updating your README.md file with evidence that you have completed all Required tool installations for the course, listed here (though you are welcome to post also proof of Optional tools). If you are using your own setup, please also post proof of your equivalent. You can do this by adding a list of the tools you have installed and adding screenshots of the tools running on your computer. You can use the images folder to store these screenshots.\nWe have yet to learn much about Markdown but you can use the following template to structure your README.md file:\n# Init IS310 Homework\n\n## Proof of Installation\n\n1. Python\n\n![Python Installation](images/python-installation.png)\n\n2. Git\n\n![Git Installation](images/git-installation.png)\n\n3. VS Code\n\n![VS Code Installation](images/vscode-installation.png)\nThis should also include your Hypothesis username.\nOnce you have created everything locally, you will need to add, commit, and push your changes to your GitHub repository. You can use the following commands to do this:\ngit add .\ngit commit -m \"Init IS310 Homework\"\ngit push origin main\nHowever, remember to connect your local repository with your GitHub repository (hint: git remote add origin if your GitHub repository is empty OR git remote set-url origin if you have files in your GitHub). If you get any errors, you can always ask for help in the Slack channel or reach out to the Instructors directly.\nYou are also encouraged to try out using GitHub Co-Pilot or any other AI chatbot to help you complete this assignment, and we also have a mored detailed lesson on git & GitHub that should cover most issues you might face.\nOnce you have completed this assignment, post the link to your GitHub repository in our first discussion forum https://github.com/cultureasdata-uiuc/is310-spring-2026/discussions/1.",
    "crumbs": [
      "Resources",
      "Introductions & Installations",
      "Introduction to Versioning & Git"
    ]
  },
  {
    "objectID": "materials/introducing-humanities-computing/03-intro-versioning.html#resources",
    "href": "materials/introducing-humanities-computing/03-intro-versioning.html#resources",
    "title": "Introduction to Versioning & Git",
    "section": "Resources",
    "text": "Resources\n\nOnce you’ve worked through this lesson, feel free to read through our Advanced git and GitHub resource. In particular, it has a helpful section on git cheatsheet that you’re welcome to refer to as you work through the assignments/throughout the semester.",
    "crumbs": [
      "Resources",
      "Introductions & Installations",
      "Introduction to Versioning & Git"
    ]
  },
  {
    "objectID": "materials/introducing-humanities-computing/03-intro-versioning.html#footnotes",
    "href": "materials/introducing-humanities-computing/03-intro-versioning.html#footnotes",
    "title": "Introduction to Versioning & Git",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nBefore Git, there were several generations of version control systems that evolved to meet the changing demands of software development. The first generation, represented by tools like Source Code Control System (SCCS) developed in 1972 by Marc Rochkind at Bell Labs, was centralized and relied on simple single-file locking-based concurrency. The Revision Control System (RCS), developed a decade later in 1982, followed the same lines but became widely popular due to its open-source nature. The second generation included the Concurrent Versions System (CVS), which allowed for more networked, collaborative work by introducing a merging-based approach instead of a file-locking approach. However, CVS had limitations that were addressed by its successor, Subversion, released in 2000, which allowed for more robust file set operations with atomic commits. Git represents the third generation of version control, combining lessons from these previous systems with new features suited for modern, large-scale software development. For more information, see Shane Lin’s Git for Humanists.↩︎",
    "crumbs": [
      "Resources",
      "Introductions & Installations",
      "Introduction to Versioning & Git"
    ]
  },
  {
    "objectID": "materials/introducing-humanities-computing/04-command-line-cheatsheet.html",
    "href": "materials/introducing-humanities-computing/04-command-line-cheatsheet.html",
    "title": "Command Line Cheatsheet",
    "section": "",
    "text": "Though Command Line might sounds a bit intimidating, it is very useful and like we read about in the Verge article can be enormously useful for when you work with many files and folders. It is also a great way to get a sense of how your computer works and how to navigate it. Finally, it’s incredibly useful for coding, as we’ll see throughout this semester.\nIf you are using the WSL (Windows Subsystem for Linux), a Mac, or a Linux, you can should use the MacOS cheatsheet. If you are using a Windows PowerShell, you should use the Windows cheatsheet. Remember, you can use all of these via VS Code’s terminal, but you should always check to see which operating system you are using.",
    "crumbs": [
      "Resources",
      "Introductions & Installations",
      "Command Line Cheatsheet"
    ]
  },
  {
    "objectID": "materials/introducing-humanities-computing/04-command-line-cheatsheet.html#why-use-the-command-line",
    "href": "materials/introducing-humanities-computing/04-command-line-cheatsheet.html#why-use-the-command-line",
    "title": "Command Line Cheatsheet",
    "section": "",
    "text": "Though Command Line might sounds a bit intimidating, it is very useful and like we read about in the Verge article can be enormously useful for when you work with many files and folders. It is also a great way to get a sense of how your computer works and how to navigate it. Finally, it’s incredibly useful for coding, as we’ll see throughout this semester.\nIf you are using the WSL (Windows Subsystem for Linux), a Mac, or a Linux, you can should use the MacOS cheatsheet. If you are using a Windows PowerShell, you should use the Windows cheatsheet. Remember, you can use all of these via VS Code’s terminal, but you should always check to see which operating system you are using.",
    "crumbs": [
      "Resources",
      "Introductions & Installations",
      "Command Line Cheatsheet"
    ]
  },
  {
    "objectID": "materials/introducing-humanities-computing/04-command-line-cheatsheet.html#command-line-cheatsheet-for-windows",
    "href": "materials/introducing-humanities-computing/04-command-line-cheatsheet.html#command-line-cheatsheet-for-windows",
    "title": "Command Line Cheatsheet",
    "section": "Command Line Cheatsheet for Windows",
    "text": "Command Line Cheatsheet for Windows\n\n\n\n\n\n\n\nCommand\nDescription\n\n\n\n\ncd\nPrint full working path\n\n\ndir or gci\nList contents of a directory\n\n\nchdir [folder] or sl\nChange into a directory\n\n\nsl ..\nGo one directory up\n\n\ncls\nClear the view\n\n\nopen [file]\nOpens a file\n\n\nopen .\nOpens the directory\n\n\nni [file name]\nCreates a new file\n\n\ndel [file name]\nRemove a single file\n\n\nni -ItemType dir [directory name]\nMake a new directory\n\n\ncopy [file] [new file/new directory]\nCopy file to file or new directory\n\n\nmove [file] [new file/new directory]\nMove file into new file or directory\n\n\nrmdir [directory]\nRemove directory ( only operates on empty directories )\n\n\nrmdir /s [directory name]\nForce remove a directory and all its contents\n\n\nhelp\nPrints all possible commands\n\n\nGet-ChildItem -Path [directory] -Recurse -Force -File\nList all files (including hidden ones) in a directory and subdirectories. All the flags are optional. You would use -Path if you wanted to specify a different directory than the one you’re in. You would use -Recurse if you wanted to list all files in subdirectories. You would use -Force if you wanted to list hidden files. You would use -File if you wanted to list only files and not directories.\n\n\n\n\nAdvanced Commands\n\n\n\n\n\n\n\nCommand\nDescription\n\n\n\n\nsudo [command]\nRun command with the security privileges of the superuser (Super User DO)\n\n\ncp *.js\nUse wildcards to get all files of a certain type when moving or copying\n\n\nedit [file]\nOpens file in Terminal editor\n\n\nexit\nExit",
    "crumbs": [
      "Resources",
      "Introductions & Installations",
      "Command Line Cheatsheet"
    ]
  },
  {
    "objectID": "materials/introducing-humanities-computing/04-command-line-cheatsheet.html#command-line-cheatsheet-for-macos",
    "href": "materials/introducing-humanities-computing/04-command-line-cheatsheet.html#command-line-cheatsheet-for-macos",
    "title": "Command Line Cheatsheet",
    "section": "Command Line Cheatsheet for MacOS",
    "text": "Command Line Cheatsheet for MacOS\n\n\n\n\n\n\n\nCommand\nDescription\n\n\n\n\npwd\nPrint full working path\n\n\n.\nCurrent folder\n\n\ncd [folder]\nChange into a directory\n\n\ncd ..\nChange directory upwards\n\n\nls\nList contents of a directory\n\n\nls -la\nList all contents including hidden files\n\n\nclear\nClear the view\n\n\nopen [file]\nOpens a file\n\n\nopen .\nOpens the directory\n\n\ntouch [file name]\nCreates a new file\n\n\nrm [file name]\nRemove a single file\n\n\nmkdir [directory name]\nMake a new directory\n\n\ncp [file] [new file/new directory]\nCopy file to file or new directory\n\n\nmv [file] [new file/new directory]\nMove file into new file or directory\n\n\nrmdir [directory]\nRemove directory ( only operates on empty directories )\n\n\nrm -rf [directory name]\nForce remove a directory and all its contents\n\n\n\n\nAdvanced Commands\n\n\n\n\n\n\n\nCommand\nDescription\n\n\n\n\nsudo [command]\nRun command with the security privileges of the superuser (Super User DO)\n\n\ncp *.js\nUse wildcards to get all files of a certain type when moving or copying\n\n\n!!\nUse double bang to repeat last command\n\n\nnano [file]\nOpens file in Terminal editor\n\n\nq\nExit",
    "crumbs": [
      "Resources",
      "Introductions & Installations",
      "Command Line Cheatsheet"
    ]
  },
  {
    "objectID": "materials/introducing-humanities-computing/04-command-line-cheatsheet.html#tips-for-the-command-line",
    "href": "materials/introducing-humanities-computing/04-command-line-cheatsheet.html#tips-for-the-command-line",
    "title": "Command Line Cheatsheet",
    "section": "Tips for the Command Line",
    "text": "Tips for the Command Line\n\nBasic Navigation\n\nTab Auto-Complete: Typing the first few letters of a directory or file name and then pressing Tab will auto-complete the name. This is incredibly useful in saving time and avoiding typos, especially with long or complex names.\nUp Arrow for History: Press the Up Arrow key to scroll through your previously entered commands. This is handy for repeating or modifying past commands without retyping them.\nCtrl + A and Ctrl + E:\n\nCtrl + A moves your cursor to the beginning of the line. Use this when you need to quickly go back to the start of your command.\nCtrl + E takes you to the end of the line. This is useful if you’re editing a command at the beginning and need to jump to the end.\n\n\n\n\nSearching Commands\n\nCtrl + R for Reverse Search: Press Ctrl + R and start typing to search through your command history. This reverse search allows you to find and reuse complex commands without having to remember them in full.\n\n\n\nManaging Processes\n\nCtrl + C to Stop Processes: If you run a command that takes too long or starts behaving unexpectedly, Ctrl + C will interrupt and stop it. This can be crucial for stopping scripts or commands that are leading you in circles.\n\n\n\nExiting\n\nExit Command: Type exit to leave the current shell session. This is like finding the exit in a maze; use it when you’ve reached your goal, or if you need to start over.",
    "crumbs": [
      "Resources",
      "Introductions & Installations",
      "Command Line Cheatsheet"
    ]
  },
  {
    "objectID": "materials/introducing-humanities-computing/04-command-line-cheatsheet.html#additional-tips",
    "href": "materials/introducing-humanities-computing/04-command-line-cheatsheet.html#additional-tips",
    "title": "Command Line Cheatsheet",
    "section": "Additional Tips",
    "text": "Additional Tips\n\nStay Organized: Keep track of your location. Use pwd to print your current directory and ls to list the contents of the directory you’re in.\nExplore Carefully: Before running scripts or opening files, use commands like cat, less, or head to preview their contents. This way, you avoid unexpected outcomes.\nDocument Your Journey: Consider taking notes in a separate document or on paper about the paths you’ve taken. This will help you avoid going in circles and make it easier to retrace your steps.",
    "crumbs": [
      "Resources",
      "Introductions & Installations",
      "Command Line Cheatsheet"
    ]
  },
  {
    "objectID": "materials/introducing-humanities-computing/02-intro-cli-prompt-engineering.html",
    "href": "materials/introducing-humanities-computing/02-intro-cli-prompt-engineering.html",
    "title": "Introduction to the Command Line & Prompt Engineering",
    "section": "",
    "text": "Note\n\n\n\n⚡️ This lesson has been adapted from and inspired by Melanie Walsh’s Textbook Introduction to Cultural Analytics & Python https://melaniewalsh.github.io/Intro-Cultural-Analytics/01-Command-Line/01-The-Command-Line.html and the Digital Humanities Research Institute’s Workshop on the Command Line https://github.com/DHRI-Curriculum/command-line. Many thanks to all authors for sharing their materials!",
    "crumbs": [
      "Resources",
      "Introductions & Installations",
      "Command Line & Prompt Engineering"
    ]
  },
  {
    "objectID": "materials/introducing-humanities-computing/02-intro-cli-prompt-engineering.html#introducing-the-command-line",
    "href": "materials/introducing-humanities-computing/02-intro-cli-prompt-engineering.html#introducing-the-command-line",
    "title": "Introduction to the Command Line & Prompt Engineering",
    "section": "Introducing the Command Line",
    "text": "Introducing the Command Line\nSo far, you have been installing software via something called the terminal, but we have yet to explain what the terminal is or how it works.\nTerminals are a way to interact with your computer via text commands. Today, we give usually commands to a computer via a Graphical User Interface (GUI), pronounced “gooey”. Any time you download an app, whether on your phone or computer, you are using a GUI. GUIs let us click on icons, drag items, and interact with our computers in a visual way. Terminals let us do many of the same functions, but we have to be more explicit and write these commands.\nYou’ve already experienced this when you checked your version of Python, for example, with this command python3 --version.\n\nWhat are Terminals?\nTo really understand what terminals are, we need to learn a bit about the history of computing.\n\n\n\nEarly computer\n\n\nEarly computer https://www.computerhope.com/cdn/eniac.jpg\nBack in the 1950s and 1960s, computers were the size of entire rooms and functioned through the use of something called punch cards.\n\n\n\nExample punch card from the Index Thomisticus, one of the first Computing in the Humanities projects started in the 1940s to digitize the writings of Thomas Aquinas\n\n\nExample punch card from the Index Thomisticus, one of the first Computing in the Humanities projects started in the 1940s to digitize the writings of Thomas Aquinas. More information available here https://theoreti.ca/?p=6096\nAs you can see in this example image, punch cards contained a series of holes that encoded information like numbers, letters, and even instructions for programs. These cards were fed into machines called card readers, translating the hole patterns into electrical signals the computer could understand. This enabled data entry for tasks like payroll calculations, statistical analysis, and even early gaming. The punch cards and computers were run by teams of operator, often comprised of women, who inputted these punch cards, prompting the computers to process and output the results on paper.\nThe term terminal then refers to the device that allowed operators to interact with the computer, and the command line refers to the text-based interface that allowed operators to input commands to the computer. The command line was the only way to interact with computers until the 1970s, when the first Graphical User Interface (GUI) was developed at Xerox PARC. This GUI was later adopted by Apple and Microsoft, and is the basis for the GUIs we use today.\nToday when we use terminals, we are not just using the terminal but also using often using something called a shell. A “shell” is a program within a terminal that interprets user commands and processes computer output. Originating from the Unix operating system, the term “shell” signifies a user-friendly interface that encapsulates the complexities of various computer systems. Popular shell programs include bash (Bourne Again SHell), which is the default shell for most Linux distributions and MacOS; PowerShell, which is Windows default option, and zsh (Z Shell) – the optional configuration from our course tools. Each shell has its own set of commands and syntax, but they all share the same basic functionality.\nFor more on this history, I would highly recommend watching “Computer History: Punch Cards Historical Overview -IBM Remington Rand UNIVAC - History 1900’s-1960’s”, 2016. https://www.youtube.com/watch?v=kKJxzay85Vk.\nThe most important thing to understand is that the terminal is how you interface with the computer, the shell is the program that interprets your commands, and the command line is the text-based interface that allows you to input commands to the shell.\nTo help make this more concrete, compare these two images from Melanie Walsh’s textbook:\n\n\n\nGUI example\n\n\n\n\n\nCLI example\n\n\nIn the first image, we see that Melanie is using the Mac Finder App to interact graphically with her file folders. This app is a GUI, so she is clicking and moving files around with her mouse.\nIn the second image, we see that Melanie is using the terminal and typing commands to move files around. We can see that the top of her program says bash, indicating which shell she is using, and then she is typing commands like mkdir and rmdir to create and remove directories.\nThese images give you a sense of how the command line works, and how we use it. So let’s start learning some commands!",
    "crumbs": [
      "Resources",
      "Introductions & Installations",
      "Command Line & Prompt Engineering"
    ]
  },
  {
    "objectID": "materials/introducing-humanities-computing/02-intro-cli-prompt-engineering.html#working-with-the-command-line",
    "href": "materials/introducing-humanities-computing/02-intro-cli-prompt-engineering.html#working-with-the-command-line",
    "title": "Introduction to the Command Line & Prompt Engineering",
    "section": "Working With The Command Line",
    "text": "Working With The Command Line\nFrom Melanie’s example, we can see that the command to create a directory is mkdir. Let’s try it out!\nFirst, we need to open our terminal, which we can do through VS Code. To do this, click on the Terminal tab in the top menu bar and select New Terminal. This will open a new terminal window at the bottom of your screen.\nThen we need to figure out the correct syntax or wording to use the mkdir command. We can see from Melanie’s example that she types mkdir and then Intro-CA-Notes.\nmkdir Intro-CA-Notes\n\n\n\nDemo example\n\n\nIf we press enter, it should seem like this works but how do we know for sure?\nWe have two options. First, we should take a look at the Command Line cheatsheet that contains a number of these commands and see what would be the best option to help us check if this worked.\n\nOur other option is to try and start testing out these AI Chatbots, and specifically GitHub Co-Pilot (though again you are welcome to use any AI tool that is free and helps you understand these materials).\nTo use these tools, though we first have to discuss their interface and how they work.\n\nPrompt Engineering\nSo far we have been doing a form of prompting, called Command Prompt, which refers to where we write the text commands in the terminal. Often times the command prompt is a symbol that indicates the start of a new command. For example, in Melanie’s example, the command prompt is the $ symbol. This symbol is called a prompt because it prompts you to enter a command. The prompt is followed by a cursor, which is the blinking vertical line that indicates where your next character will appear. Other symbols you might see include #, %, and &gt;.\nNow, the term prompt has become incredibly popular in the last few years, beginning with the release of GPT-3 by OpenAI in Summer 2020 and their use of prompt engineering to describe giving commands or prompts to chatbots for certain outputs or results.\n\n\n\nGPT-3 example\n\n\nWhile there idea of prompt engineering has a longer history in natural language processing, the term has taken over, with the proliferation of new AI chatbots and models.\nIn some ways our idea of giving text prompts to the terminal via the command line echoes how we do prompt engineering for AI chatbots, since both require writing text (something we’ll discuss more later in this lesson).\nBut while depending on your operating system, terminal, and shell, you have to give the command line certain set of commands that have been previously programmed, with AI chatbots you can write without those set structures.\nHowever, that doesn’t mean there aren’t some helpful guidelines for prompt engineering, even if the process often requires lots of trial and error, and is rarely reproducible or transparent.\nIf you search for tips on prompt engineering, you’ll find lots of infographics like this one:\n\n\n\nPrompt Engineering\n\n\nThese outline some of the core ways to think about prompt engineering (concepts like defining a role, chained prompting, and so on). But Microsoft actually provides more detailed guidelines for Co-Pilot, since it has been trained primarily for coding tasks, https://learn.microsoft.com/en-us/training/modules/introduction-prompt-engineering-with-github-copilot/2-prompt-engineering-foundations-best-practices.\n\n\n\n4S of Prompt Engineering\n\n\nIn this overview, they propose the 4S of Prompt Engineering: Single (keeping your prompt to one task or question), Specific (having detailed requests and specifications), Short (concise), and Surround (keep relevant files open in Co-Pilot).\nWhile these are general guidelines, the final one is particularly important for Co-Pilot, since it works best when it has code examples to learn from (something called few-shot learning, for those that are interested).\n\n\n\nPrompt Engineering\n\n\nThis very complex diagram details what GitHub Co-Pilot does with your prompt, and how it processes it. The key thing to understand is that it is looking for patterns in the prompt and then trying to match those patterns to code examples it has seen before. So the more specific and detailed your prompt is, the more likely it is to find a match.\nThe other key thing to know is that even though it claims to be secure, there’s a lot of potential for personal data to be leaked through this process. So it’s important to be careful about what you prompt Co-Pilot with, and to be aware of the potential risks (i.e. don’t share sensitive information in a prompt!!).\nAs we’ve hopefully discussed a bit by now, the legality of Co-Pilot remains hazy (not to mention the ethics or politics of it). You’ll likely see why as we continue to work with it, as it autocompletes file names and other information from GitHub that it has scraped. Again, I want to stress the use of this tool is optional, but also encourage you to see your use of it in a critical lens: what helps us critique these tools is often using them and understanding not only how they work, but whether the hype and their claims live up to the reality of using them.1\n\n\nDirectories and File Paths\nSo returning to our original question, hopefully by now we have discovered some of the commands we can use to check if our directory was created, using the Command Line cheatsheet. But we could also try out generating a prompt for GitHub Co-Pilot and see what it suggests.\nClick the button to see some potential answers.\n\n\n\n\n\n\nTry it: Terminal commands and Co-Pilot prompts\n\n\n\n\n\nSo looking at the cheatsheet, two promising commands include ls and pwd. We can try those out, and also try out a prompt for GitHub Co-Pilot.\nHow can I check if my directory was created in my terminal?\nShould I use the command ls or pwd to check if my directory was created? And what is the difference between these two commands?\nHow can I create a directory named is310-computing-humanities in my terminal?\nWith the help of the cheatsheet and GitHub Co-Pilot, we should see that the command ls lists the contents of a directory, and pwd prints the full working path. So we can use these commands to check if our directory was created.\nAnd then we can use the command mkdir to create a directory named is310-computing-humanities.\nmkdir is310-computing-humanities\n\n\n\nUsing these commands, we can see that our directory was created. But what does this mean exactly?\nAs we read this week, how we interact with computers has increasingly made it difficult to understand where our files are stored. While search functionality often takes care of this for us in GUI applications, in the command line we have to be more explicit about where we want to store our files and we have to know where we are located.\n\n\n\nCurrent Directory\n\n\nIn this figure, we see a representation of how folders and files are organized on a Windows computer. The top level is the root directory, which on Windows is the C: drive. On Macs and Linux machines, we have a similar root directory, but it is represented by a / symbol. This root directory contains all of the files and folders on your computer. The root directory is also called the parent directory because it is the parent of all the other directories on your computer.\nImportant to know: directory is the same as folder, and the two are often used interchangeably. Directory is the original term, but with GUIs we often use the term folder.\nIn this diagram, we can see that there is a Users folder or directory. This is where most of your files that you download or create are stored: under your user account and in the Documents, Downloads, and Desktop folders.\nIf I use the pwd command, I can start to see this structure.\n\n\n\nCurrent Directory\n\n\nYou’ll notice the command prints out a long string that has names like Users, followed by a series of names separated by a forward slash / (in PowerShell, you’ll see backslashes \\). Each of these are my folders or directories, and this represents the absolute file path from my root directory to my current directory – that is the current location of my terminal. I realize this might be a bit confusing initially, but eventually as you use the command line more, you’ll start to get a sense of how this works.\nEach file path is unique to your computer, and so if you are following along with this lesson, you’ll see a different file path, though there should be a Users folder as your root or home directory.\nYou’ll also notice I did the command cd ~. What do you think this command does? What does cd stand for do you think?\nA core distinction with how I used this final command is the fact that I used a tilde ~ symbol. This symbol is a relative file path, meaning it is relative to my current directory. So if I am in the Documents folder, cd ~ will take me to my home directory. You can also use cd .. to go up one directory, and cd ../.. to go up two directories, and so on.\n\nBeyond creating directories (which again is just a fancy term for file folders), we often use the command line to create files. To do this, we use the command touch. Let’s try it out!\ntouch is310-computing-humanities.txt\nNow we should see both our folder and our file in the terminal. But ideally, we would have the file in the folder. So how do we do that?\nWe can use the command mv to move the file into the folder. Let’s try it out!\nmv is310-computing-humanities.txt is310-computing-humanities\nNow we should just see our folder if we do ls in the terminal. But how do we check if our file is in the folder?\nWe can again use the command cd to change into the folder, and then use ls to list the contents of the folder. Let’s try it out!\ncd is310-computing-humanities\nls\nFinally, let me try leaving my directory and deleting it. To do this, I can use the command cd .. to go up one directory, and then rmdir to remove the directory. Let’s try it out!\ncd ..\nrmdir is310-computing-humanities\nNow if I do ls I should see that my folder is gone.Success!!\n\n\nIn Class Exercise\nNow it’s time for you to try out some of these commands on your own. Using a combination of our cheatsheet and your preferred AI chatbot, try to complete the following tasks:\n\nCreate a new directory called is310-computing-humanities in your Desktop folder\nIn your is310-computing-humanities folder, create a new file called is310-computing-humanities.txt\nTry adding some text to your file using the command line (this is a new command, so you’ll have to look it up!)\nTry displaying the contents of your file using the command line (again a new command, so you’ll have to look it up!)\nFinally, try leaving your folder and returning to your home directory.\n\nOptional Advanced Option:\n\nTry copying your folder and then deleting the copy. Remember to ask for help from the Instructors, if you need it!",
    "crumbs": [
      "Resources",
      "Introductions & Installations",
      "Command Line & Prompt Engineering"
    ]
  },
  {
    "objectID": "materials/introducing-humanities-computing/02-intro-cli-prompt-engineering.html#additional-resources",
    "href": "materials/introducing-humanities-computing/02-intro-cli-prompt-engineering.html#additional-resources",
    "title": "Introduction to the Command Line & Prompt Engineering",
    "section": "Additional Resources",
    "text": "Additional Resources\nIn addition to the resources, I linked at the beginning of this lesson, I would recommend the following:\n\nIan Milligan and James Baker, “Introduction to the Bash Command Line,” The Programming Historian 3 (2014), https://programminghistorian.org/en/lessons/intro-to-bash. This is an introduction to the Bash shell, which will serve well enough as an introduction to other shells like Zsh as well.\nBash Basics Part 1 of 8 Access and Navigation\nBeginner’s Guide to the Bash Terminal\nThe Most Important Thing You’ll Learn in the Command Line\nGo through the CodeAcademy command line course.\nShell Scripting Tutorial",
    "crumbs": [
      "Resources",
      "Introductions & Installations",
      "Command Line & Prompt Engineering"
    ]
  },
  {
    "objectID": "materials/introducing-humanities-computing/02-intro-cli-prompt-engineering.html#footnotes",
    "href": "materials/introducing-humanities-computing/02-intro-cli-prompt-engineering.html#footnotes",
    "title": "Introduction to the Command Line & Prompt Engineering",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIf you have serious concerns over GitHub Co-Pilot, I’ve just learned of Privy, a private and locally run version of a coding assistant https://github.com/srikanth235/privy. I haven’t had a chance to try it yet, but seems like a promising alternative. Another option is gpt4all which let’s you run a number of models locally (though anecdotally some of my previous students struggled to install it on their computers, so ymmv).↩︎",
    "crumbs": [
      "Resources",
      "Introductions & Installations",
      "Command Line & Prompt Engineering"
    ]
  },
  {
    "objectID": "schedule/02-programming-and-processing.html",
    "href": "schedule/02-programming-and-processing.html",
    "title": "Programming and Processing",
    "section": "",
    "text": "Discuss the assigned readings.\nAnswer any questions about the Introduction to Course Tools.\nStart Introduction to the Command Line & Prompt Engineering and Introduction to Versioning & Git(if not completed in-class, these will become homework).",
    "crumbs": [
      "Schedule",
      "January",
      "Jan 22: How Will We Work in this Course?"
    ]
  },
  {
    "objectID": "schedule/02-programming-and-processing.html#in-class-agenda",
    "href": "schedule/02-programming-and-processing.html#in-class-agenda",
    "title": "Programming and Processing",
    "section": "",
    "text": "Discuss the assigned readings.\nAnswer any questions about the Introduction to Course Tools.\nStart Introduction to the Command Line & Prompt Engineering and Introduction to Versioning & Git(if not completed in-class, these will become homework).",
    "crumbs": [
      "Schedule",
      "January",
      "Jan 22: How Will We Work in this Course?"
    ]
  },
  {
    "objectID": "schedule/02-programming-and-processing.html#assigned-materials-complete-prior-to-class",
    "href": "schedule/02-programming-and-processing.html#assigned-materials-complete-prior-to-class",
    "title": "Programming and Processing",
    "section": "Assigned Materials (Complete Prior to Class)",
    "text": "Assigned Materials (Complete Prior to Class)\nNo Hypothesis Annotations Required for This Week\n\nBuschek, Christo, and Jer Thorp. “Models All The Way Down.” Knowing Machines. https://knowingmachines.org/models-all-the-way\nVincent, Nick, and Hanlin Li. “GitHub Copilot and the Exploitation of ‘Data Labor’: A Wake-Up Call for the Tech Industry.” PSA Group (blog), July 8, 2021. https://www.psagroup.org/blogposts/101.\nComplete Introduction to Course Tools.",
    "crumbs": [
      "Schedule",
      "January",
      "Jan 22: How Will We Work in this Course?"
    ]
  },
  {
    "objectID": "schedule/02-programming-and-processing.html#additional-materials-not-required-reading",
    "href": "schedule/02-programming-and-processing.html#additional-materials-not-required-reading",
    "title": "Programming and Processing",
    "section": "Additional Materials (Not Required Reading)",
    "text": "Additional Materials (Not Required Reading)\n\nChiang, Ted. “ChatGPT Is a Blurry JPEG of the Web.” The New Yorker, February 9, 2023. https://www.newyorker.com/tech/annals-of-technology/chatgpt-is-a-blurry-jpeg-of-the-web.\nSchaul, Kevin, Szu Yu Chen, and Nitasha Tiku. “Inside the Secret List of Websites That Make AI like ChatGPT Sound Smart.” Washington Post, April 19, 2023. https://www.washingtonpost.com/technology/interactive/2023/ai-chatbot-learning/",
    "crumbs": [
      "Schedule",
      "January",
      "Jan 22: How Will We Work in this Course?"
    ]
  },
  {
    "objectID": "schedule/01-introducing-computing-in-the-humanities-culture-as-data.html",
    "href": "schedule/01-introducing-computing-in-the-humanities-culture-as-data.html",
    "title": "Introducing Computing in the Humanities & Culture As Data",
    "section": "",
    "text": "Introductions and goals for the course\nOverview and review the syllabus, slides available here\nDiscuss and define “What is computing in the humanities?” versus “What is culture as data?”\n\nVisit What is Digital Humanities? and refresh a few times\nAlso visit Open Syllabus Project\n\nStart survey for group assignments (time permitting) available on Canvas\nStart lesson on Course Tools",
    "crumbs": [
      "Schedule",
      "January",
      "Jan 20: Why Represent Culture As Data?"
    ]
  },
  {
    "objectID": "schedule/01-introducing-computing-in-the-humanities-culture-as-data.html#in-class-agenda",
    "href": "schedule/01-introducing-computing-in-the-humanities-culture-as-data.html#in-class-agenda",
    "title": "Introducing Computing in the Humanities & Culture As Data",
    "section": "",
    "text": "Introductions and goals for the course\nOverview and review the syllabus, slides available here\nDiscuss and define “What is computing in the humanities?” versus “What is culture as data?”\n\nVisit What is Digital Humanities? and refresh a few times\nAlso visit Open Syllabus Project\n\nStart survey for group assignments (time permitting) available on Canvas\nStart lesson on Course Tools",
    "crumbs": [
      "Schedule",
      "January",
      "Jan 20: Why Represent Culture As Data?"
    ]
  },
  {
    "objectID": "schedule/index.html",
    "href": "schedule/index.html",
    "title": "Schedule & Assignments Overview",
    "section": "",
    "text": "Note\n\n\n\nThis schedule is subject to change depending on our collective progress this semester, so please treat this as a general guideline that is not set in stone. I will add assignments as we progress below, but you should generally expect to have both assigned materials and assignments each week.",
    "crumbs": [
      "Schedule",
      "Schedule Overview"
    ]
  },
  {
    "objectID": "schedule/index.html#weekly-schedule",
    "href": "schedule/index.html#weekly-schedule",
    "title": "Schedule & Assignments Overview",
    "section": "Weekly Schedule",
    "text": "Weekly Schedule\nThis overview gives a top level view of the topics for each of our meetings. You can either use the side navigation or click on a topic to jump to that section.\n\n\n\nTuesday Topics (Explorations)\nThursday Topics (Experimentations)\n\n\n\n\nJanuary 20 - Why Represent Culture As Data? Introducing Computing in the Humanities & Culture As Data\nJanuary 22 - How Will We Work in This Course? Programming & Processing\n\n\nJanuary 27 - What does it mean to work with culture as data responsibly, and how does this change across disciplines and industries? Responsible Computing of Culture & Contextualizing Social Knowledge\nJanuary 29 - How can we work collaboratively and computationally? Version & File Histories",
    "crumbs": [
      "Schedule",
      "Schedule Overview"
    ]
  },
  {
    "objectID": "schedule/index.html#assignment-schedule",
    "href": "schedule/index.html#assignment-schedule",
    "title": "Schedule & Assignments Overview",
    "section": "Assignment Schedule",
    "text": "Assignment Schedule\nAssignments will be added throughout the semester depending on how much material we get through.\n\nIndividual Assignments\n\n\n\nAssignment\nDue Date (Subject to Change)\n\n\n\n\nInitial Interest Survey (Link Available on Canvas)\nDue by Midnight CT on January 21\n\n\nInit IS310 Homework\nDue by Midnight CT on January 28\n\n\n\n\n\nGroup Assignments\n\n\n\nAssignment\nDue Date (Subject to Change)",
    "crumbs": [
      "Schedule",
      "Schedule Overview"
    ]
  },
  {
    "objectID": "schedule/04-version-file-histories.html#assigned-materials",
    "href": "schedule/04-version-file-histories.html#assigned-materials",
    "title": "Version & File Histories",
    "section": "Assigned Materials",
    "text": "Assigned Materials\n\nChin, Monica. “Kids Who Grew up with Search Engines Could Change STEM Education Forever.” The Verge, September 22, 2021. https://web.archive.org/web/20211102083237/https://www.theverge.com/22684730/students-file-folder-directory-structure-education-gen-z. This is a web archive link because The Verge requires a subscription.",
    "crumbs": [
      "Schedule",
      "January",
      "Jan 29: How can we work collaboratively and computationally?"
    ]
  },
  {
    "objectID": "schedule/04-version-file-histories.html#assignments",
    "href": "schedule/04-version-file-histories.html#assignments",
    "title": "Version & File Histories",
    "section": "Assignments",
    "text": "Assignments\n\nComplete Init IS310 Homework",
    "crumbs": [
      "Schedule",
      "January",
      "Jan 29: How can we work collaboratively and computationally?"
    ]
  },
  {
    "objectID": "schedule/04-version-file-histories.html#additional-materials",
    "href": "schedule/04-version-file-histories.html#additional-materials",
    "title": "Version & File Histories",
    "section": "Additional Materials",
    "text": "Additional Materials\n\nWeingart, Scott B. “The Route of a Text Message.” Vice, February 2019. https://www.vice.com/en_us/article/kzdn8n/the-route-of-a-text-message-a-love-story.\n\n\nAdditional Links\n\nVannevar Bush, “As We May Think” and pdf version.\n\nExample of a potential memex machine.\n\nThe History of the Old Bailey Online Project https://www.oldbaileyonline.org/about/a-narrative-history-of-the-project#digitisation.\nHathiTrust Digital Library https://www.hathitrust.org/ and an issue of The Crisis https://babel.hathitrust.org/cgi/pt?id=coo.31924055965465&seq=1.\nThe Art of Google Books by Krissy Wilson and The Darker Side of Digitization by Leah Henrickson.\nScanning Labor Project by Eliie Colbert, Lucian Li, David Satten-Lopez, and Elizabeth Schwartz https://scanninglabor.github.io/IAScanningLabor/scanners.html.\nBreeding, Marshall. “History of Mergers and Acquisitions in the Library Technology Industry.” Library Technology Guides.http://librarytechnology.org/mergers/.",
    "crumbs": [
      "Schedule",
      "January",
      "Jan 29: How can we work collaboratively and computationally?"
    ]
  },
  {
    "objectID": "schedule/03-responsible-computing-of-culture-and-contextualizing-social-knowledge.html#assigned-materials",
    "href": "schedule/03-responsible-computing-of-culture-and-contextualizing-social-knowledge.html#assigned-materials",
    "title": "Responsible Computing of Culture & Contextualizing Social Knowledge",
    "section": "Assigned Materials",
    "text": "Assigned Materials\n\n“Mission.” Responsible Datasets in Context (blog), n.d. https://www.responsible-datasets-in-context.com/mission.html and discuss final projects\nLiao, Zhehui, Maria Antoniak, Inyoung Cheong, et al. “LLMs as Research Tools: A Large Scale Survey of Researchers’ Usage and Perceptions.” arXiv:2411.05025. Preprint, arXiv, October 30, 2024. https://arxiv.org/html/2411.05025v1.\n\nSalvaggio, Eryk. 2024. “Challenging The Myths of Generative AI” Tech Policy Press, August 29. https://techpolicy.press/challenging-the-myths-of-generative-ai.",
    "crumbs": [
      "Schedule",
      "January",
      "Jan 27: What does it mean to work with culture as data responsibly?"
    ]
  },
  {
    "objectID": "schedule/03-responsible-computing-of-culture-and-contextualizing-social-knowledge.html#assignments",
    "href": "schedule/03-responsible-computing-of-culture-and-contextualizing-social-knowledge.html#assignments",
    "title": "Responsible Computing of Culture & Contextualizing Social Knowledge",
    "section": "Assignments",
    "text": "Assignments\n\nComplete Initial Interest Survey (link available on Canvas) due by January 21 at midnight.",
    "crumbs": [
      "Schedule",
      "January",
      "Jan 27: What does it mean to work with culture as data responsibly?"
    ]
  },
  {
    "objectID": "assessments/index.html",
    "href": "assessments/index.html",
    "title": "Overview",
    "section": "",
    "text": "In this course, we have two main categories of assessments: weekly ones and ones associated with the semester long project. Both types are intended to introduce you to new materials, help you synthesize this content, and engage in research that is meaningful to you. Your final grade will be evenly divided between these two categories.",
    "crumbs": [
      "Assessments",
      "Overview"
    ]
  },
  {
    "objectID": "assessments/02-weekly-assessments.html",
    "href": "assessments/02-weekly-assessments.html",
    "title": "Weekly Assessments 50%",
    "section": "",
    "text": "Your performance in weekly assessments constitutes half of your final grade. These assessments include engaging with assigned materials through in-class discussions, asynchronous annotations of assigned materials, and weekly coding assignments, all of which is detailed below. Weekly assessments are graded as pass/fail though part of that assessment is also your degree of engagement and creativity, as well as the growth you demonstrated throughout the semester.",
    "crumbs": [
      "Assessments",
      "Weekly Assignments"
    ]
  },
  {
    "objectID": "assessments/02-weekly-assessments.html#weekly-participation-25",
    "href": "assessments/02-weekly-assessments.html#weekly-participation-25",
    "title": "Weekly Assessments 50%",
    "section": "Weekly Participation (25%)",
    "text": "Weekly Participation (25%)\nWeekly participation includes both individual engagement with course materials and collaborative group presentations. This combination ensures you are both deepening your own understanding and learning from your peers’ when working with culture as data.\n\nWeekly Seminar Discussion and Asynchronous Annotations 15%\nBefore each class, you’re expected to engage with assigned materials, ranging from reading a book chapter or article, to exploring a data visualization to reading documentation. These materials lay the groundwork for our class discussions, as well as helping you with your semester long project. There may be some weeks that are of more interest to you, and I encourage you to explore the additional listed materials, though these are optional.\nQuestions to consider when engaging with the weekly materials include:\n\nWhat is the main argument of this piece? Is there an argument? What is interesting or evocative or infuriating?\nIf there is an argument, how does the author support the argument? What sorts of evidence or data did they utilize? How did they organize the structure of their piece?\nIf you didn’t feel there was an argument, what do you think was the author’s purpose or goal with the material (whether a project, blog post, tool, etc…)?\nWhat is the likely audience for the material?\nWhat connections or tensions did you identify across materials?\nWhat did you not understand in the materials? What was confusing?\nHow would you connect the week’s materials to previous ones?\n\nOverall, remember the goal is not to detail everything the author or creator says, but instead try to summarize in a few lines their central points and how they relate to the other materials from that week.\nOne way to demonstrate your engagement with the weekly materials is to participate in our weekly seminar discussions. Your contributions could be in the form of questions about complex or confusing concepts in the readings, connections that you want to share across the assigned materials, or feedback and responding to peer contributions. More than simply talking as much as possible though, our goal with seminar discussions is to create a thoughtful and respectful intellectual community.\nThe other way to demonstrate engagement is through asynchronous annotations. We will be using the Hypothesis annotation platform, with an introductory session in the first week of class. These annotations are a space for you to share your thoughts on the week’s materials and engage with your peers, and you will occasionally be asked to expand on your annotations during our discussions. You can sign up for Hypothesis here hypothes.is/signup and join our collective group (link in the syllabus or on Canvas). You will need to install the Hypothesis Browser Extension for your preferred browser as well.\n\nGrading Policy\nFor weekly seminar discussions, you must both be in person in class and be an active participant in the discussion or activities to receive full credit. Simply attending class is not enough; you are expected to contribute thoughtfully to discussions and engage with the material and your peers.\nFor the weekly annotations, you need to submit these via Hypothesis by midnight the day before class. This ensures that your contributions are available for others to review and discuss during our seminar. Your annotations should be tagged appropriately in our Hypothesis group to facilitate organization and discussion. To receive full credit, annotations must demonstrate thoughtful engagement with the material—this includes summarizing key points, raising questions, and making connections with other readings or topics we’ve covered.\n\n\n\nGroup Presentations and Collaboration (10%)\nGroups will be assigned within the first two weeks of class after an initial survey of student interests and backgrounds (e.g., music, literature, social media, gaming) . This group will work together to complete active in-class activities and out-of-class assignments, which will be documented and submitted via GitHub.\nStructure and Expectations:\n\nWeekly Prompts and Activities: Each week, your group will be assigned a prompt or task that builds on the week’s materials. This could involve finding digital objects, exploring datasets, or applying a concept from the readings.\nIn-Class Presentations: Some weeks, your group will present your findings to the class. Not every group will present every week, but even when your group is not presenting, you are still expected to submit your work to GitHub and be prepared to discuss it if called upon. Presentations should be clear, concise, and demonstrate your group’s understanding and application of the week’s materials\nDocumentation and Submission: All group work, whether presented in class or not, must be documented and submitted to your group’s GitHub repository. This documentation should include a summary of your group’s activities, details on how the labor was divided, and any relevant reflections on the process. Clear and detailed documentation is crucial for both grading and for your group members’ progress on the semester-long project.\n\n\nGrading Policy\nYour grade for weekly group work will be based on your active participation in presentations, the quality of your contributions, and the effectiveness of your collaboration. This includes both in-class activities and the quality of the work submitted on GitHub. This grade will not be impacted if a member of your group is absent, though the group will be required to pivot their schedule and plan in that event.\nWeekly group work must be submitted to GitHub by midnight prior to the class when it is due. This ensures that the work is available for review and that all group members are accountable for their contributions. Late submissions will receive half credit, as long as they are accompanied by an explanation for the delay. Repeated late submissions will result in the group meeting with the Instructor.\n\n\n\nWeekly Coding Assignments 25%\nYou will also have weekly programming assignments to complete and share via GitHub, unless indicated otherwise. Most coders work collaboratively, often engaging in pair programming, a method of talking through code problems together. This course encourages you to pair program, and work together to learn from each other’s solutions. However, be aware that copying and pasting code might get you through one week, but the concepts in this course move quickly and build upon each other. So, I would encourage you to do your best to not only try and complete the assignments each week, but to ensure that you are understanding the core concepts.. Questions are encouraged both in person and online, and we will be discussing more about best coding practices during our first week.\n\nGrading Policy\nWeekly coding assignments must be submitted via GitHub by midnight prior to the class meeting when they are due. These assignments are an opportunity to practice and solidify your understanding of the concepts covered each week. Late submissions will be accepted but will only receive half credit, as long as they are turned in before the final class meeting. Pair programming is encouraged, but remember that understanding the underlying concepts is crucial as the material builds progressively throughout the semester.",
    "crumbs": [
      "Assessments",
      "Weekly Assignments"
    ]
  },
  {
    "objectID": "assessments/02-weekly-assessments.html#required-texts-and-resources",
    "href": "assessments/02-weekly-assessments.html#required-texts-and-resources",
    "title": "Weekly Assessments 50%",
    "section": "Required Texts and Resources",
    "text": "Required Texts and Resources\nAlmost all the course materials will be available online through either the course website or Canvas, so you are not required to purchase any materials. Throughout the semester, I will highlight books that you may want to purchase but that is completely up to you. The same goes for our coding resources. You will not be required to purchase any software but you will need access to a computer, so please let me know early on if you think that will be an issue and we can try and find solutions.",
    "crumbs": [
      "Assessments",
      "Weekly Assignments"
    ]
  },
  {
    "objectID": "assessments/03-semester-project.html",
    "href": "assessments/03-semester-project.html",
    "title": "Semester Long Project 50%",
    "section": "",
    "text": "The remainder of your grade will be determined through a semester-long group project. The goal of this project is to expose you to how we create and work with culture as data. As stated above, this project will be completed in groups, and you will be assessed both on your individual contributions and the group’s final submission. The final project is modeled on the Responsible Datasets in Context Project (RDC) https://www.responsible-datasets-in-context.com/, which was created to help students “work with data responsibly.” While we will be using these datasets to practice and learn how to programmatically work with data, they also provide an example of how best to curate and share data about complex cultural phenomena and objects.\nAs the authors of the project write in the mission statement: &gt; “Data cannot be analyzed responsibly without deep knowledge of its social and historical context, provenance, and limitations. Anyone who works with data—from academic researchers to industry professionals—will know this claim to be true.\nWhile I do not expect you to create as polished or extensive of an output as the datasets available on the RDC Project, you will be working collaboratively to create a first draft of what could eventually be part of this project.",
    "crumbs": [
      "Assessments",
      "Semester Long Project"
    ]
  },
  {
    "objectID": "assessments/03-semester-project.html#project-deadlines-milestones",
    "href": "assessments/03-semester-project.html#project-deadlines-milestones",
    "title": "Semester Long Project 50%",
    "section": "Project Deadlines & Milestones",
    "text": "Project Deadlines & Milestones\n\nCollective & Individual Topic Selection\nDUE FEBRUARY 5, 2026 (Optional Extension to February 12, 2026) PASS/FAIL Grade\nIn the first two weeks of the course, you will be assigned to a group based on shared interests and complementary skill sets. Your first task is to collaboratively determine the overall thematic focus of your group, as well as brainstorm some potential topics for your individual dataset. You will have time to work on this planning document in class and will submit it as a Markdown file in your group’s GitHub repository.\nThis planning document should address:\n\nGroup Theme: What is your shared area of interest? What kinds of cultural objects, practices, or phenomena fall within this theme? What’s included and what’s beyond scope?\nIndividual Ideas: What specific dataset is each group member considering? How do these relate to the group theme? (These can be preliminary - you’ll develop them further as the semester progresses)\nCollaboration Plan: How will your group communicate and share progress? What’s your GitHub organization strategy?\n\nFormat & Submission\n\nMarkdown file (planning.md or similar) in the root of your GitHub repo or a planning/ subfolder\n500–750 words is sufficient, though you are welcome to be as creative and verbose as you like.\nUse headings, bullet points, links, images, or tables if helpful\n\n\n\n\nInitial Dataset Submission\nDUE MARCH 12, 2026 (Optional Extension to March 24, 2026) 15%\nThis is your first major deliverable, and it focuses on creating a small, bespoke dataset through close, interpretive work. The goal is to understand what it means to make data carefully—item by item, decision by decision—before you attempt to scale that work computationally. You will create approximately 50-100 data items that reflect deep engagement with a particular cultural object, practice, or phenomenon.\nWhy small? We start with small-scale, bespoke data creation for a crucial reason: this is where you learn that every dataset embeds interpretive choices. When you manually work through 75 items, deciding what counts, what to capture, how to categorize, you experience the intellectual and ethical labor that gets hidden when you download a dataset or automate collection at scale. It is also an opportunity to get creative and think about what data you wish would exist for your cultural topic of interest.\nWhat about computation? While your dataset is small and reflects close interpretive work, you are required to use computational tools to assist your process. This is not about automation—it is about understanding how computation can augment even bespoke data work. Essentially, we want to break the divide between manual and automated to explore how computation and data can assist across scales.\nTo address this, your dataset must implement at least one of the following approaches:\n\nCreate Data from Scratch: Today, we are often handed datasets without ever experiencing the messy, interpretive process of creating one ourselves. This approach invites you to explore how complex cultural materials—say, a sculpture or a medieval manuscript—get transformed into structured data. While this may sound straightforward, every step involves interpretive decisions that shape how others can engage with the data downstream.\n\nFor example, if you are interested in the history of children’s literature and working with a lesser-known author whose work has yet to be digitized, you could scan those materials and decide what to capture: Is it just the text? The illustrations? The paratextual material? You might use image models to extract drawings and text, enabling analysis of the author’s evolving style.\nAlternatively, if you might be curious about how students are using AI in their daily lives and could turn to social media platforms like TikTok. While we cannot conduct human-subject research in this course (e.g., no surveys or interviews requiring IRB approval), you could analyze publicly posted videos. For instance, you might annotate TikToks that mention AI to track recurring themes or rhetorical patterns.\nThese are just a few possibilities, but whatever your focus, this approach requires you to define a clear methodology, justify your sampling, and carefully document your decisions. You are not just collecting data. You are making data, and that comes with responsibility.\n\nAudit and Augment Existing Data: Not all responsible data work begins from scratch though. In fact, some of the most important contributions come from uncovering how existing datasets were made, what they omit, and how they might be improved. This approach asks you to critically engage with an existing dataset—especially one that lacks documentation or transparency—and to make its origins, structure, and limitations legible to others.\n\nFor example, you might find a movie dataset on Kaggle that includes scraped reviews and ratings, but provides little information about where the data came from or how it was cleaned. You could trace the dataset back to its source—say, IMDb or Rotten Tomatoes or even historic newspapers—and compare a sample of the original reviews to what appears in the dataset. Were the reviews shortened or misattributed? What patterns of exclusion or distortion emerge? How much has been lost in this process?\nOr you might find a dataset that is widely reused but lacks key metadata or flattens the complexity of the phenomenon it represents, like popular economic benchmark datasets like GDP. Your goal then would be to try and reintroduce that complexity either through supplementing it with missing information, reconciling conflicting sources, or annotating entries to flag inconsistencies or bias. These contributions not only enhance the dataset, but also model what responsible reuse looks like.\nWhatever dataset you choose, this approach requires you to investigate its provenance, reflect on its transformations, and augment it in some meaningful way—whether by correcting errors, adding annotations, or merging it with new material. Just like with data creation, you’ll need a clear methodology, a justification for your interventions, and a well-documented data biography that makes your process transparent.\n\n\nFormat & Submission\nYour submission includes three components:\n\nInitial Dataset (~50-100 items). Your bespoke dataset in a structured format of your choice. You should have a rationale for how you have organized your data.\nInitial Documentation. Your first attempt at documentation that you think best reflects your process, interpretative choices, and explains the dataset. Some questions you might want to address include:\n\nWhat cultural materials are you working with and why? What approach did you take (from scratch or auditing)?\nWhat computational tools did you use to assist your work? How did they help? What were their limitations?\nWhat decisions did you make about what to include, exclude, or how to categorize? Why? What challenges did you encounter? How did you address them? What patterns, questions, or tensions emerged from working closely with this data?\n\nNext Steps. After creating this bespoke dataset, after Spring Break, you will work on scaling or expanding your dataset with computation. This plan should outline how you will address the following questions:\n\nHow will you generate more data computationally (Option 1: scale up to 500-1,000+ items using automation)? Or will you combine your bespoke data with an existing large dataset (Option 2: audit, merge, and analyze)? What computational methods will you use? (APIs, web scraping, LLMs, pattern matching, etc.)\nWhat will change when you scale? What interpretive decisions will you try to automate? What technical challenges do you anticipate?\nThis plan is your roadmap for understanding how scale transforms data work. You don’t need to implement it yet—but you should be thinking critically about what happens when you move from 75 carefully crafted items to 1,000 algorithmically generated ones. Your initial dataset should be submitted in your group’s GitHub repository and you should update any collective documentation to help users navigate the files and folders.\n\n\n\n\nData Demo Day\nEITHER DUE APRIL 30, 2026 OR MAY 5, 2026 5%\nWhile we are primarily using computation to create or curate data, you will also get the opportunity to speculate about what researchers might do with your data and, if you are ambitious, even present on your experiments with computation.\nRather than sharing all the details of your process or focus, this presentation is primarily focused on what might come next: What users do you envision? What computational methods might you want to try out on this data? What patterns or insights might emerge at scale that you couldn’t see in your bespoke manual dataset? What future data would you collect?\nWe will go over logistics closer to the date, but the focus here is on speculative experimentation and peer learning, so I encourage you to be creative and ambitious in your presentations. This is work-in-progress, not a final polished product.\n\n\n\nFinal Project Submission\nDUE MAY 15, 2026 NO EXTENSIONS 30%\nThis is the culmination of your semester-long engagement with culture as data. By this point, you will have created data manually with computational assistance, augmented it at scale, experimented with methods, and presented your speculative visions. Now you will submit your complete dataset and write scholarly essays that demonstrate what you have learned about the interpretive, technical, and ethical dimensions of representing culture as data, as well as how you situate your work in broader scholarly data practices.\nYour final submission includes both individual and collaborative components, all submitted via your group’s GitHub repository.\n\nIndividual Data Essay & Dataset 25%\nCulture As Dataset: You will submit your dataset that contains both your bespoke manual work and your computational augmentation at scale. It should be in a structured format, thoughtfully organized, and accompanied by relevant documentation, which is detailed in the next section.\nCulture As Documentation: Along with your dataset you will submit both documentation of your dataset but also a final data essay that tells the story of your dataset—how you made it, what it represents, what it reveals, and what it conceals. This essay should demonstrate your intellectual growth over the semester and your ability to critically reflect on the process of working with culture as data. You should pay special attention to explaining how computation played a role in your dataset; how scale shaped the data over the semester; the limitations or qualifications of your dataset; any ethical or privacy considerations; any lessons learned; and finally, how you situate your work in published peer reviewed scholarship.\nMore details about both parts of this assignment will be discussed over the course of the semester.\n\n\nCollective Principles & Documentation 5%\nYour group will collaboratively write a document with you group that synthesizes what you collectively learned about working with your particular type of cultural data. This is NOT a repeat of individual data essays—it’s methodological guidance and collective wisdom for future researchers.\nThink of this as: Writing the documentation you wish had existed when you started this project. What should someone know before they attempt to represent music as data? Or social media? Or gaming culture? What principles emerged from your group’s diverse approaches to similar materials? At its core, this document should include a set of principles for working with your cultural topic as data. The document should also attempt to note how each member contributed to its format, and like all work it will be submitted in your group’s collective GitHub repository.",
    "crumbs": [
      "Assessments",
      "Semester Long Project"
    ]
  },
  {
    "objectID": "materials/introducing-humanities-computing/05-advanced-git-github.html",
    "href": "materials/introducing-humanities-computing/05-advanced-git-github.html",
    "title": "Advanced Git and GitHub",
    "section": "",
    "text": "Note\n\n\n\n⚡️ This lesson has been adapted from materials drawn from The Turing Way book https://the-turing-way.netlify.app/collaboration/github-novice/github-novice-firststeps. Many thanks to the authors for sharing their materials!",
    "crumbs": [
      "Resources",
      "Introductions & Installations",
      "Advanced Git and GitHub"
    ]
  },
  {
    "objectID": "materials/introducing-humanities-computing/05-advanced-git-github.html#github-features",
    "href": "materials/introducing-humanities-computing/05-advanced-git-github.html#github-features",
    "title": "Advanced Git and GitHub",
    "section": "GitHub Features",
    "text": "GitHub Features\nGitHub is a massive platform that adds features all the time. While we won’t cover all of them, there are a few that are worth highlighting.\n\nRepositories\nWhile you have already created a GitHub repository, it’s worth taking a moment to talk about what a repository is and how it works.\nJust to reiterate, repositories are the core of GitHub and git, and are essentially like folders in Google Drive. They are where all the files, code, and data for a project is stored, along with version histories of them. On GitHub, as you’ve seen now, repositories can be either public or private. Public repositories are visible to anyone, while private repositories are only visible to the owner and collaborators.\n\nRepository Structure\n\nIn the diagram above, you see the basic structure of a repository, which consists of:\n\nUsername: This is the owner of the repository. In the case of our course website, the owner is me (ZoeLeBlanc) whereas in this example, the username is EKaroune.\nRepository: This is the name of the repository (or you can think of it as a project directory). In this example, the repository name is trial-repo.\nCode: The top of the repository has a series of tabs. This first tab brings you back to your landing page for the repository and shows you the folders and files that are in the repository.\nMain: This indicates the branch you are viewing in the repository. We’ll discuss branches in more detail below.\nBranch: If you click on this button, you’ll see a page that lists all the branches in repository. You can select which one to view from this page or the drop-down menu.\nREADME.md file: This is a special file that GitHub displays. It serves as an introduction to your GitHub repository, and often includes the repository name, details on how to use it, and specifics about the license.\nGreen Code Button: This button allows you to download the code for the repository. You can download it as a zip file or clone it using the command line.\n+ symbol: This button allows you to create a new repository.\nForks: This button allows you to fork a repository. Forking a repository creates a copy of it in your own GitHub account. We’ll discuss more below.\nAdd File: This button allows you to add a new file to the repository.\nCommits/Clock symbol: This shows you the commit history for the repository. You can click on the commit to see what changes were made. Remember commits are the saved snapshots, so this shows you how many have been made and when.\nEdit/Pencil symbol: This button allows you to edit the README.md file.\n\n\n\nRepository Features\n\n\nProjects: This tab allows you to create a project board for the repository. Project boards are a way to track tasks and issues related to a project. They are a way to organize work and can be used to coordinate work between collaborators. You can read more about GitHub Projects here https://docs.github.com/en/issues/planning-and-tracking-with-projects/learning-about-projects/about-projects.\nIssues: This tab takes you to the issues page for the repository. You can read more about GitHub Issues below.\nInsights: This tab takes you to the insights page for the repository. You can read more about GitHub Insights below.\nEdit Repo Details: This button allows you to edit the repository details, including the name, description, and license.\nDescription of the repository: This is the description of the repository. It is a good idea to include a description of the repository so that others can understand what it is about. However, GitHub will only render a few sentences so it’s best to keep it short.\nLink for GitHub Pages or Website: This is the link to the GitHub Pages site for the repository. We’ll be discussing GitHub Pages more soon, but generally it’s a good idea to link to your website here.\nTopics: This is where you can add topics to your repository. We’ll discuss topics more below GitHub Topics below.\nLicense: This is where you can add a license to your repository. We’ll discuss licenses more in-depth later in the semester.\n\n\n\nInsights\nYou can view the activity on a repository by clicking on the Insights tab. This will show you a graph of the activity on the repository, including activity by contributors, under the Contributors tab.\n\n\n\nContributors to the Programming Historian Jekyll repository https://github.com/programminghistorian/jekyll\n\n\nContributors to the Programming Historian Jekyll repository https://github.com/programminghistorian/jekyll\nFor example, this image is of the insights page on GitHub one of the most popular Computing in the Humanities projects, The Programming Historian. Highly worth checking out if you’re interested in learning about more advanced programming topics and how they relate to the humanities!\n\n\n\nIssues\nIssues are a way to track bugs, feature requests, or other tasks related to a project. They are a way to track the development of a project and can be used to coordinate work between collaborators. Issues can be assigned to specific users, labeled, and commented on. They can also be closed when the issue is resolved. You can read more about GitHub Issues here https://docs.github.com/en/issues/tracking-your-work-with-issues/about-issues.\n\n\n\nGitHub Issue\n\n\n\n\nDiscussions\nWe will be using discussions extensively this semester, but generally you can think of them as a forum. They are similar to issues, but are more open-ended and can be used for general questions or conversations. They are most commonly used to build community around a project, but I find they are also very useful for sharing homework and assignments. You can read more about GitHub Discussions here https://docs.github.com/en/discussions\n\n\n\nGitHub Discussions\n\n\n\n\nTopics\nWhile GitHub’s search engine is powerful, one way to make your projects for findable is to add topics, which allows you to tag a repository with keywords. For example, if you search for digital-humanities and then click on the Topics tab, you’ll see a list of topics that are associated with repositories that have been tagged with digital-humanities. You can also see how many repositories have been tagged with each topic. You can read more about GitHub Topics here https://docs.github.com/en/github/administering-a-repository/classifying-your-repository-with-topics.\n\n\n\nGitHub Topics\n\n\n\n\nStars\nStars are a way to bookmark repositories. They are a way to keep track of repositories that you are interested in or want to follow. You can read more about starring repositories here https://docs.github.com/en/get-started/exploring-projects-on-github/saving-repositories-with-stars.\n\n\n\nGitHub Stars\n\n\n\n\nUsers\nYou are all also familiar now with users, since you all have accounts on GitHub. While user profiles are somewhat self-explanatory, there’s a number of features for users that are worth highlighting.\n\nProfile\nUser profiles are where you can see all of a user’s repositories, issues, and discussions. It’s also where you can see their followers and who they are following. You can also see their activity on GitHub, including their contributions to repositories, issues, and discussions. You can read how to create a profile here https://docs.github.com/en/account-and-profile/setting-up-and-managing-your-github-profile/customizing-your-profile.\nWe can explore some of these features through my profile as an example.\n\n\n\nleblanc profile\n\n\nMy GitHub profile https://github.com/ZoeLeBlanc\n\n\nFollowers and Following\nYou can follow other users on GitHub, which allows you to see their activity on the platform. You can also see who is following you. This is a way to build community and find other users who are working on similar projects. You can read more about following users here https://docs.github.com/en/get-started/exploring-projects-on-github/following-people.\n\n\n\nGitHub Followers\n\n\nThere’s still a lot of features we haven’t covered, but these are the ones that are most relevant to our course. If you’re interested in learning more about GitHub, I recommend checking out GitHub’s documentation, which is very robust and detailed https://docs.github.com/en.",
    "crumbs": [
      "Resources",
      "Introductions & Installations",
      "Advanced Git and GitHub"
    ]
  },
  {
    "objectID": "materials/introducing-humanities-computing/05-advanced-git-github.html#advanced-git-github",
    "href": "materials/introducing-humanities-computing/05-advanced-git-github.html#advanced-git-github",
    "title": "Advanced Git and GitHub",
    "section": "Advanced Git & GitHub",
    "text": "Advanced Git & GitHub\nWhile we won’t be using many advanced Git features in this course, it’s worth highlighting a few of them. I want to stress that if you ever have any problems with git, please reach out to the Instructors sooner than later. We are here to help you and want to make sure you are able to use git effectively.\n\n.gitignore\n\n\n\ngitignore\n\n\nWhen we first created a repository on GitHub, it asked us if we wanted to add a .gitignore file. We said no, but now we’re going to add one.\nRather than having GitHub create it, we can do it locally:\ntouch .gitignore\nEven though we didn’t add a file extension, .gitignore is a plain text file. But this is a special type of file that lets you tell git to ignore certain files and folders in your local repository. While .gitignore is technically a hidden file (notice it starts with a period), it should show up in your VS Code file explorer or other IDEs.\nWe usually add any file that we do not want to push up to your remote GitHub repository to the .gitignore file. So if you had a draft file, like test.txt and didn’t want to push it up to GitHub, you would add it to the .gitignore file. Other common files to add to the .gitignore file include .DS_Store files, which are created by macOS, and .env files, which are used to store sensitive information like passwords and API keys.\nTo add the file to the .gitignore file, you can open it in your IDE and add the file name to it. For example, if we wanted to add test.txt to the .gitignore file, we would type in the file:\ntest.txt\nThen we would save the file. Now if we do git status, we should see something like this:\ngit status\nWhich would show us something like this:\nOn branch main\nYour branch is up to date with 'origin/main'.\n\nUntracked files:\n  (use \"git add &lt;file&gt;...\" to include in what will be committed)\n        .gitignore\n\nnothing added to commit but untracked files present (use \"git add\" to track)\nYou’ll notice git is telling us we have a new file .gitignore that is untracked, but does not show us the test.txt file. This is because we told git to ignore it.\nBeyond files you don’t want to share (whether that’s work in progress or private information), GitHub also restricts the size of files we can put in our repositories. The limit is 100MB, so if you have a large file, you can add it to the .gitignore file so that it doesn’t get pushed up to GitHub. You can read more about GitHub’s file size restrictions here https://docs.github.com/en/github/managing-large-files/what-is-my-disk-quota and you can read more about .gitignore files here https://docs.github.com/en/get-started/getting-started-with-git/ignoring-files.\n\nWhat to do if you forget to add a file to .gitignore\nDon’t panic since this happens to everyone at some point! If you forget to add a file to the .gitignore file and you’ve already pushed it up to GitHub, you can still add it to the .gitignore file and then remove it from the repository. To do this, you can use the git rm command. For example, if you wanted to remove the test.txt file from the repository, you would type:\nrm -rf test.txt\nThen you would remove it from the git log by typing:\ngit rm --cached test.txt\nThen you want to add it to the .gitignore file and save it. If you haven’t created one yet, you can do so by typing:\ntouch .gitignore\nThen you would add the file to the .gitignore file by typing:\ntest.txt\nOnce you save the file, you can commit the changes by typing:\ngit add .\ngit commit -m \"adding test.txt to .gitignore\"\nFinally you would push the changes up to GitHub by typing:\ngit push origin main\nYou can also remove files remotely via GitHub, though remember you will need to then fix any merge conflicts locally. Read more about removing files from a GitHub repository here https://docs.github.com/en/repositories/working-with-files/managing-files/deleting-files-in-a-repository.\n\n\n\nBranches\nSo far we have been doing a very standard git workflow of creating a repository, adding files to it, then committing those files, pushing those to our remote repository or pulling them back down. However, there are a number of other features that git offers that are worth highlighting. One of these is branches.\n\n\n\nGit Branching\n\n\nIn this figure, we start to see the concept of branches. But first let’s cover why and when you might want to use branches.\n\nWhy Branches?\nBranches are a way to create a copy of your project that you can make changes to without affecting the main project. You can create as many branches as you want and then merge them back into the main project when you’re ready. This is useful if you want to try something new, but don’t want to affect the main project. For example, if you wanted to try a new feature on your website, you could create a branch to test it out. If it works, you can merge it back into the main project. If it doesn’t, you can delete the branch and try something else.\nBranches are also very useful for collaboration. For example, if you are working on a project with a group of people, you can create a branch for each person to work on. Then you can merge them back into the main project when you’re ready. This is a way to coordinate work, allow people to work on separate features or ideas, and then make sure that these paths can be incorporated into the main version of the project or left as a rabbit hole.\n\n\n\nLoki Branches\n\n\nFor those that are a fan of time travel fiction, you can think of branches as a way to create alternate timelines. This is a bit of a spoiler alert, but in the Marvel series Loki, the main character creates a branch in the timeline when he steals the Tesseract. This branch is then used to create a new timeline, which is different from the main timeline. This is a good way to think about branches, as they are a way to create a new timeline for your project.\n\n\nHow to Create a Branch\nBefore creating a branch, you should first take a look at what branches you have in your repository. To do this, you can use the git branch command. For example, if we type git branch in our terminal, we see that we have one branch called main.\ngit branch\n* main\nThis is the default branch that is created when you create a repository. However, we can create a new branch if we want to.\nTo create a branch, you can use the git branch command. For example, if we wanted to create a branch called new-feature, we would type:\ngit branch new-feature\nThis would create a new branch called new-feature. However, we would still be on the main branch. To switch to the new-feature branch, we would use the git checkout command. For example, we would type:\ngit checkout new-feature\nNow if we look at our branches, we see that we are on the new-feature branch.\ngit branch\n  main\n* new-feature\nHowever, if we look at our files we won’t see any changes. This is because we haven’t made any changes to the files yet. Let’s make a change to our README.md file, adding some more information.\n# IS 310 Test Repository\n\nThis is my first repository.\n\n## About\n\nThis is a test repository for IS 310.\nNow if we save the file, we can start our standard git workflow:\n\ngit add .\ngit commit -m \"adding more information to README.md\"\n\nBut rather than pushing the changes to the main branch, we want to push them to the new-feature branch. To do this, we need to use the -u flag with the git push command. This tells git to push the changes to the new-feature branch. For example, we would type:\ngit push -u origin new-feature\nThis should show us something like the following:\nEnumerating objects: 5, done.\nCounting objects: 100% (5/5), done.\nDelta compression using up to 8 threads\nCompressing objects: 100% (2/2), done.\nWriting objects: 100% (3/3), 315 bytes | 315.00 KiB/s, done.\nTotal 3 (delta 0), reused 0 (delta 0), pack-reused 0\nremote:\nremote: Create a pull request for 'new-feature' on GitHub by visiting:\nremote:      https://github.com/ZoeLeBlanc/is310-test-repo/pull/new/new-feature\nremote:\nTo github.com:ZoeLeBlanc/is310-test-repo.git\n * [new branch]      new-feature -&gt; new-feature\nLet’s click on that link and see what happens.\n\n\n\nPull Requests & Merging\nWhen we click on the link, we see a page that looks like this:\n\n\n\nPull Request\n\n\nThis is something called a Pull Request, which is an interface that GitHub provides as a way to merge changes from one branch into another. In this case, we are merging the changes from the new-feature branch into the main branch. This is a way to review the changes before merging them into the main branch. It’s also a way to discuss the changes and make sure they are ready to be merged.\nI don’t expect anyone do any pull requests (often called PRs by developers) in this course, but I wanted to share some core things in case you need to. First, the key thing to pay attention is at the top of the interface where it says base: main and compare: new-feature. This tells us that we are merging the changes from the new-feature branch into the main branch. If we wanted to merge the changes from the main branch into the new-feature branch, we would need to switch the branches.\nIf everything is good then you’ll see a green checkmark and the message Able to merge. If there are any conflicts, you’ll see a red X and the message This branch has conflicts that must be resolved. If you see this message, you’ll need to resolve the conflicts before you can merge the changes. You can read more about resolving conflicts here https://docs.github.com/en/github/collaborating-with-pull-requests/addressing-merge-conflicts/resolving-a-merge-conflict-using-the-command-line.\nYou’ll also notice that our commit message is now the title of the pull request. We could also add a description in the Write box, which also allows us to Preview our description. Often this is a good way to add more details to whatever change you’re making to the repository, since commit messages are usually fairly short.\nTo see a more fleshed out example, take a look at this screenshot from a recent PR on The Programming Historian:\n\n\n\nPull Request Example\n\n\nPull Request Example https://github.com/programminghistorian/jekyll/pull/3150\nYou can see that the journal uses GitHub for its editorial workflow, so Pull Request is where they detail all the changes to an article. This is a way to track the changes and make sure that they are ready to be merged into the main branch. It’s also a way to discuss the changes and make sure they are ready to be merged.\nTo finish our Pull Request, we need to press the green Create Pull Request button. This will take us to this interface:\n\n\n\nPull Request Example 2\n\n\nThis is the final check to make sure that everything is ready to be merged. If everything looks good, we can press the green Merge pull request button. This will merge the changes from the new-feature branch into the main branch. If we got to the Code tab, we should see the changes we made to the README.md file. Then our final step is to pull those changes back into our local repository. To do this, we can use the git pull command. For example, we would type:\ngit pull origin main\nNow both our local and remote repositories are up to date, and we’ve tried using branches, merging, and pull requests to update our repository. I realize this is fairly advanced topic, so if you want to keep learning more, I would highly recommend taking a look at the Learn Git Branching interactive tutorial https://learngitbranching.js.org/.\n\n\nCloning a Repository\nSo far we have been creating new repositories on GitHub and then pushing content from our local computer. But what if we want to pull a repository that already exists on GitHub? This is where cloning becomes useful.\nCloning is GitHub’s term for downloading a repository from GitHub to your local computer. To clone a repository, we need to use the git clone command and then the URL of the remote repository.\ngit clone https://github.com/USERNAME/REPOSITORY\nYou can find the correct URL of a repository by clicking on the green Code button and then copying the URL. Do not copy the above example, since it’s just a placeholder. You need to copy the URL of the repository you want to clone.\n\nIf you setup SSH, you should select the Use SSH button, but otherwise you can use HTTPS and then copy the URL.\nNow we can clone the repository by using the git clone command and then the URL of the remote repository. Let’s clone our is310-test-repo.\ngit clone git@github.com:ZoeLeBlanc/is310-test-repo.git\nYou should see something that looks like the following:\n$ git clone https://github.com/ZoeLeBlanc/is310-test-repo.git\n&gt; Cloning into `is310-test-repo`...\n&gt; remote: Counting objects: 10, done.\n&gt; remote: Compressing objects: 100% (8/8), done.\n&gt; remove: Total 10 (delta 1), reused 10 (delta 1)\n&gt; Unpacking objects: 100% (10/10), done.\nNow if we type ls we will see that we have a new directory called is310-test-repo. If we navigate into that directory, we will see that it has all the files from our remote repository.\n\n\nForking\nThe final concept you might want to try out is forking. While forking sounds like it should involve cutlery, it is actually a way to create a copy of a repository in your own GitHub account.\nWhat’s the difference between forking and cloning?\nForking and cloning are very similar: both make a copy of a repository. The difference is that forking creates a copy of the repository in your own GitHub account, while cloning creates a copy of the repository on your local computer. You can read more about forking and cloning here https://docs.github.com/en/get-started/quickstart/fork-a-repo.\nForking is most useful when you want to make changes to a repository that you don’t own, but you also want to keep getting updates from the original project. So example might include a website theme or a template for a project. You can fork the repository, make changes to it, and then pull updates from the original repository. This is a way to keep your fork up to date with the original repository.\nThis diagram helps to illustrate our full git & GitHub workflow:\n\n\n\nForking\n\n\nBut remember these are advanced concepts so if you have any questions, please reach out to the Instructors and also check out the resources below.",
    "crumbs": [
      "Resources",
      "Introductions & Installations",
      "Advanced Git and GitHub"
    ]
  },
  {
    "objectID": "materials/introducing-humanities-computing/05-advanced-git-github.html#git-commands-cheat-sheet",
    "href": "materials/introducing-humanities-computing/05-advanced-git-github.html#git-commands-cheat-sheet",
    "title": "Advanced Git and GitHub",
    "section": "Git Commands Cheat Sheet",
    "text": "Git Commands Cheat Sheet\n\n\n\n\n\n\n\nCommand\nUse\n\n\n\n\ngit init\nInitializes a Git repository in whatever directory we are located in. Remember to do pwd to check if you’re in the correct directory. Also you only want to run this command once. You should be able to tell if it active in your terminal, but in case you’re unsure you can do whatever command we do for showing hidden files because git init creates the .git folder.\n\n\ngit add .\nThis command adds our file to the staging area. You’ll notice here I’m not writing the file name but using .. While you can write each file name individually, it is very tedious. Instead, you can use the . and that will include all files that have been changed in your directory (unless they are in your .gitignore file).\n\n\ngit commit -m \"MESSAGE\"\nThis command commits our staged files and lets us write a commit message about our changes\n\n\ngit remote add origin URL\nConnects our local repository with a remote repository (that is one on GitHub) at the specified URL\n\n\ngit push origin branch_name\nPushes our local changes to the specified branch of the remote repository\n\n\ngit pull origin branch_name\nPulls any new changes from the GitHub repository into our local repository\n\n\ngit branch branch_name\nCreates a new branch with our specified name\n\n\ngit checkout branch_name\nSwitches to the specified branch\n\n\ngit log\nOutputs a log of past commits (snapshots) with their commit messages\n\n\ngit status\nShows our current status, including what branch you are on and what changes are staged or un-staged\n\n\ngit clone URL\nMakes a clone of the repository at the specified URL",
    "crumbs": [
      "Resources",
      "Introductions & Installations",
      "Advanced Git and GitHub"
    ]
  },
  {
    "objectID": "materials/introducing-humanities-computing/05-advanced-git-github.html#general-best-practices-for-git-and-github",
    "href": "materials/introducing-humanities-computing/05-advanced-git-github.html#general-best-practices-for-git-and-github",
    "title": "Advanced Git and GitHub",
    "section": "General Best Practices for git and GitHub",
    "text": "General Best Practices for git and GitHub\n\nDo your best to keep your local git enabled project and remote GitHub project in sync. This means that you should always do a git pull before you start working on a project and a git push when you finish working on a project. This will help to avoid conflicts and make sure that your changes are up to date.\nAlways include a .gitignore file. This will help to avoid pushing up files that you don’t want to share or that are too large. It will also help to keep your repository clean and organized.\nAlways include a README.md file. This is a good way to introduce your project and explain what it is about. It’s also a good way to explain how to use your project and what the license is.\nFor projects with multiple collaborators, try to use branches. This will help to avoid conflicts and make sure that everyone’s changes are incorporated into the main project.",
    "crumbs": [
      "Resources",
      "Introductions & Installations",
      "Advanced Git and GitHub"
    ]
  },
  {
    "objectID": "materials/introducing-humanities-computing/05-advanced-git-github.html#resources",
    "href": "materials/introducing-humanities-computing/05-advanced-git-github.html#resources",
    "title": "Advanced Git and GitHub",
    "section": "Resources",
    "text": "Resources\n\nYour best resource on GitHub will always be the documentation https://docs.github.com/en\nJulia Evans’ documentation on git branches https://jvns.ca/blog/2023/11/23/branches-intuition-reality/.\nJulia Evans’ zine on what is inside of .git folder https://wizardzines.com/comics/inside-git/.\nShane Lin’s textbook Git for Humanists https://shane-et-al.github.io/git_slab/",
    "crumbs": [
      "Resources",
      "Introductions & Installations",
      "Advanced Git and GitHub"
    ]
  },
  {
    "objectID": "materials/introducing-humanities-computing/01-course-tools.html",
    "href": "materials/introducing-humanities-computing/01-course-tools.html",
    "title": "Course Software & Tools",
    "section": "",
    "text": "In this course, we will be utilizing a number of different software platforms and tools. Some of the tools are optional, but are highly recommended. Setting up some of these tools will be easy, but others will expose you to some of the more frustrating aspects of working with computers. The best representation of what I’m talking about is this xkcd comic, entitled Dependency.\nDependency by Randall Munroe\nEssentially, modern software is a bit of a house of cards, and it’s not uncommon to spend hours trying to get something to work, only to find out that you were missing a single character in a configuration file or have some conflicting dependency. The key thing to know is that this is not a reflection of your ability to work with computers, but rather a reflection of the state of modern software. In fact, an HCI professor wrote an excellent post about how this type of configuration errors can deter students (you can read a bit about that here https://www.kevinbrowne.ca/command-line-bullshittery-and-other-realities-of-computing/). There’s also this great free MIT resources called The Missing Semester of Your CS Education https://missing.csail.mit.edu/2020/course-shell/ and https://missing.csail.mit.edu/2020/command-line/ that covers some of what we will be doing here if you would like more background information.\nThe remainder of this page contains installation instructions for the tools/software/programming languages we will be using in this course, listed in order of complexity of installation. Most of the links are operating system agnostic, but if indicated, please follow the instructions for your computer type (Windows = any computer running windows, Mac = any apple computer). Most of the instructions involve clicking on links to download programs, though a few also require you to paste text into your computer.\nYou should do your best to get through the majority of the installation instructions before our next class, but if you get stuck, please reach out to the instructor via Slack (especially if you’ve been trying for more than ~20 minutes) and also know that we will have time in our Thursday class session to help troubleshoot any installation issues. Again, any issue you face is a feature of the difficulty of working with computers, not a reflection or indication of your ability to code.",
    "crumbs": [
      "Resources",
      "Introductions & Installations",
      "Course Software & Tools"
    ]
  },
  {
    "objectID": "materials/introducing-humanities-computing/01-course-tools.html#assigned-list-of-course-software-tools",
    "href": "materials/introducing-humanities-computing/01-course-tools.html#assigned-list-of-course-software-tools",
    "title": "Course Software & Tools",
    "section": "Assigned List of Course Software & Tools",
    "text": "Assigned List of Course Software & Tools\n\nRequired\n\nSlack Account with DH@UIUC Server.\nHypothesis Account with IS310-Spring-2026 Group.\nGitHub Account.\nLocal Python 3 Installation.\nGit.\nLocal Text Editor (VS Code Recommended).\n\n\n\nOptional\n\nAI Tooling of Your Choice:\n\nRecommend GitHub Co-Pilot and using GitHub Education Benefits to get access to Co-Pilot.\n\nIDE of Your Choice:\n\nRecommend Visual Studio Code and Visual Studio Code Extensions.\n\nOh-My-Zsh.",
    "crumbs": [
      "Resources",
      "Introductions & Installations",
      "Course Software & Tools"
    ]
  },
  {
    "objectID": "materials/introducing-humanities-computing/01-course-tools.html#slack",
    "href": "materials/introducing-humanities-computing/01-course-tools.html#slack",
    "title": "Course Software & Tools",
    "section": "Slack",
    "text": "Slack\n\n\n\nOur Slack Channel on DH@UIUC Slack Team\n\n\nSlack is a communication tool that we will use for this course. You can download the app for your computer or use the web version. You can also download the app for your phone. You will need to create an account and join the course channel is310-spring-2026 on DH@UIUC team. The link to join Slack is available on Canvas and in the syllabus, and you can add yourself to our channel or ask the Instructors for assistance.\nI realize that Slack is less popular than a platform like Discord, but it is a very popular platform for Digital Humanities. In fact, the primary DH Slack Team is a great place to ask questions and get help from other DHers. You can read more about it and how this field uses Slack in this writeup by Amanda Visconti https://literaturegeek.com/2016/07/06/digital-humanities-slack-community-design.\nYou are welcome to make any username you want, but please be sure to share who you are in the course channel so we can all get to know each other.",
    "crumbs": [
      "Resources",
      "Introductions & Installations",
      "Course Software & Tools"
    ]
  },
  {
    "objectID": "materials/introducing-humanities-computing/01-course-tools.html#hypothesis",
    "href": "materials/introducing-humanities-computing/01-course-tools.html#hypothesis",
    "title": "Course Software & Tools",
    "section": "Hypothesis",
    "text": "Hypothesis\nFor our collective annotations, we will be using the Hypothesis platform. You can sign up for Hypothesis here hypothes.is/signup and join our collective group at the link in our syllabus or via Canvas. You will need to install the Hypothesis Browser Extension for your preferred browser as well. You should follow the instructions available in this Quick Start Guide https://web.hypothes.is/help/quick-start-guide/.\nYou are welcome to make your username whatever you choose. And please be sure to make your annotations in our hypothes.is group is310-Spring-2026 and that these annotations are public. You can find the link to join the group at the top of our Canvas home page.\n\nHow to Annotate with Hypothesis\nIn short, once you’ve created your account and joined our group, you can start making annotations. To do that, you’ll need to install the Hypothesis browser extension for your browser of choice.\nFor Chrome (or other Chromium browsers such as Edge or Brave) can install the Hypothesis extension from the Chrome Web Store.\n\n\n\nHypothesis Chrome Extension\n\n\nFor Firefox or Safari can install the Hypothesis bookmarklet by visiting the start page and following the instructions in step 2.\n\n\n\nHypothesis Bookmarklet\n\n\nOnce you’ve done that, you can start annotating by selecting text on a webpage and clicking the “Annotate” button that appears. You can find detailed instructions on this guide https://web.hypothes.is/help/annotation-basics/. But I will share a few key points here.\n\nViewing & Making Annotations with Hypothesis\nTo view annotations and activate your hypothesis extension, you should click the hypothesis extension icon if working in Chrome or similar browsers, which looks like the following:\n\n\n\nHypothesis Chrome Extension\n\n\nOr if you are in Firefox or Safari, you should activate the bookmarklet by clicking the bookmarklet in your bookmarks bar.\n\n\n\nHypothesis Firefox Extension\n\n\nOnce activated you can view existing annotations by clicking the sidebar icon that appears on the right side of your browser window.\n\n\n\nHypothesis Sidebar\n\n\nYou’ll want to make sure that you are logged into your account, which you can do in the sidebar:\n\n\n\nHypothesis Signing In\n\n\nFinally, you can start annotating by selecting text on a webpage and clicking the “Annotate” button that appears.\n\n\n\nHypothesis Adding Annotations\n\n\nThere’s lots of additional features that you can read about in the guide, but the key thing is to make sure you are logged in and that you are making your annotations in the is310-Spring-2026 group.\nYou can switch your annotations from being public to being part of the group by selecting the group from the dropdown menu in the annotation window.\n\n\n\nHypothesis Annotation\n\n\nYou can read more about using Hypothesis groups in this guide https://web.hypothes.is/help/annotating-with-groups/. Feel free to reach out to the instructor if you have any questions or need help with Hypothesis.\n\n\nUsing Hypothesis on Local PDFs\nWhile most of the materials for this course are public on the web, we will occasionally have PDFs that are not. You can still annotate these PDFs using Hypothesis by following the instructions in this guide https://web.hypothes.is/help/annotating-locally-saved-pdfs/. The main steps are as follows:\n\nSave the PDF to your computer. These will be available on Canvas or through the UIUC library.\nIf you are using Chrome, you will need to adjust your Hypothesis extension settings to allow for local PDFs. You can do this by clicking on the Hypothesis extension icon, selecting the gear icon, and then toggling the setting to allow for local PDFs.\n\n\nSo first, right-click on the Hypothesis extension icon in Chrome and select “Options” from the context menu\n\n\n\n\nHypothesis Extension Icon\n\n\n\nThen, dismiss the pop-up window that appears.\n\n\n\n\nHypothesis Extension Options\n\n\n\nFinally, make sure site access is set to “On all sites” and switch the control on for “Allow access to file URLs”\n\n\n\n\nHypothesis Extension Options\n\n\n\nOpen the PDF in your browser and click the Hypothesis extension icon to start annotating. Specifically, in the menu bar at the top of your screen, click File and select Open File. (alternatively, you can use ⌘O on Macs or CTRL O on PCs). Then make sure you have the Hypothesis extension enabled, that you are logged in and posting to is310-Spring-2026 and you should be able to annotate the PDF.\n\nPlease let the instructor know if you have any issues with this process.",
    "crumbs": [
      "Resources",
      "Introductions & Installations",
      "Course Software & Tools"
    ]
  },
  {
    "objectID": "materials/introducing-humanities-computing/01-course-tools.html#github",
    "href": "materials/introducing-humanities-computing/01-course-tools.html#github",
    "title": "Course Software & Tools",
    "section": "GitHub",
    "text": "GitHub\n\n\n\nGitHub Logo\n\n\nIf you do not already have a GitHub account, you will need to sign up for an account for this course. For those unfamiliar, GitHub https://github.com/ is a web-based platform that allows you to host and version code-related projects, while also working collaboratively. The best analogy is to imagine a blend of a data management and collaborative work platform like Google Drive with a social media platform like Twitter. It uses Git, a version control system, which is somewhat confusingly different from GitHub, to manage the history of a project.\nGitHub is free to use (part of its popularity) and once you have an account you can store code and related materials (like datasets, for example) remotely, while also working collaboratively on these materials. The platform primarily consists of users, organizations (which users can join), and repositories (often called “repos” that are akin to folders or directories) for storing materials. In many ways, GitHub is similar to Google Drive, allowing users to both upload, organize, and keep versioned histories of their files. However, unlike Google Drive or other research management platforms, GitHub is also a platform for social coding, where users can publicly work on projects together, follow one another, comment on each other’s work, and view each other’s activity (somewhat like Twitter, Mastodon, Bluesky, etc…).\nWe will be delving into GitHub’s functionality in the next few weeks, but first you will need to sign up for an account. You can sign up for an account here https://github.com/join. GitHub provides additional documentation on signing up for an account. In terms of selecting a username, I would recommend choosing something that you plan to use professionally.\n\n\n\n\n\n\nNote\n\n\n\n⚡️ If you have issues signing up for the GitHub Education Benefits, please contact the Instructor for assistance. You do not need to apply for GitHub Educational Benefits, if you do not plan\n\n\nOnce you have an account, you will also need to sign up for the GitHub Education Global Campus account https://education.github.com/benefits. You can find step-by-step instructions on how to sign up here https://docs.github.com/en/education/explore-the-benefits-of-teaching-and-learning-with-github-education/github-global-campus-for-students/apply-to-github-global-campus-as-a-student, but you will need to use your @illinois.edu email and show proof of your student status (either ID or academic record).\nGetting approved might take a few days, but once you are approved, you will be able to not only create private repositories for free (we will get into what repositories are soon), but also use GitHub’s AI coding tool called Co-Pilot.\nBoth GitHub and Co-Pilot are owned by Microsoft, which acquired the platform in 2018 for 7.5 Billion dollars1, and the platform has become the de facto platform for hosting code, with the platform recently announcing that they had over 100 million of users.2\n\nConsiderations and Criticisms of GitHub\n\n\n\n100 million users\n\n\nThese numbers sound impressive, but should also be concerning. The consolidation of code and data on a single platform raises questions about the centralization of knowledge and the power of a single company to control the future of software development, if not all technology writ large.\nWhile alternatives to GitHub exist (primarily GitLab or Bitbucket), it is hard to overstate how much in the last decade GitHub has become the platform for this type of work. Such a centralization is truly a double-edged sword. For example, GitHub has created what amounts to the largest code archive in the world through their GitHub Arctic Code Vault, an initiative to take a snapshot of all code hosted on their platform on February 2, 2020 and store it in the Svalbard Global Seed Vault. Such an initiative is impressive and was done in partnership with some academic institutions (the exact partners were the Long Now Foundation, the Internet Archive, the Software Heritage Foundation, Arctic World Archive, Microsoft Research, the Bodleian Library, and Stanford Libraries). But it also raises a number of ethical, archival, and political questions.\n\n\n\nGitHub Arctic Code Vault\n\n\nFor example, this Arctic Code Vault has been criticized for its erasure of the indigenous Sami people who live on Svalbard and its performative approach to archives.3 GitHub also has contracts with U.S. Immigrations and Customs Enforcement (ICE) led to protests and boycotts from some sectors of the developer community.4 While we plan to use this platform, it’s important to do so with these issues in mind. For more on these criticisms, see:\n\nFussell, Sidney. “The Schism at the Heart of the Open-Source Movement.” The Atlantic, January 3, 2020. https://www.theatlantic.com/technology/archive/2020/01/ice-contract-github-sparks-developer-protests/604339/.\nMackenzie, Adrian. “48 Million Configurations and Counting: Platform Numbers and Their Capitalization.” Journal of Cultural Economy 11, no. 1 (January 2, 2018): 36–53. https://doi.org/10.1080/17530350.2017.1393443.\n\n\n\nWhy Use GitHub for Digital Humanities/Computing in the Humanities?\nWhile primarily marketed towards software developers, GitHub is also incredibly popular for digital humanities. It is widely used for hosting data sets, text corpora, and other scholarly research assets. It provides a central place to publish, discover, and collaborate on such resources.\nFor instance, academic journals like the Programming Historian use GitHub for the entire publishing life cycle (you can explore for yourself https://github.com/programminghistorian/jekyll). You can get a sense of how popular GitHub is for digital humanities through searching for digital-humanities on the platform https://github.com/search?q=digital%20humanities&type=repositories or taking a look at the number of repositories on Github tagged as digital-humanities https://github.com/topics/digital-humanities.\n\n\n\nDigital Humanities Repositories on GitHub\n\n\nThough this platform is very popular and useful for this course, we will do our best to use it critically and thoughtfully (a mindset we will hopefully bring to our various infrastructures and tools!).",
    "crumbs": [
      "Resources",
      "Introductions & Installations",
      "Course Software & Tools"
    ]
  },
  {
    "objectID": "materials/introducing-humanities-computing/01-course-tools.html#ai-coding-with-github-co-pilot",
    "href": "materials/introducing-humanities-computing/01-course-tools.html#ai-coding-with-github-co-pilot",
    "title": "Course Software & Tools",
    "section": "AI Coding with GitHub Co-Pilot",
    "text": "AI Coding with GitHub Co-Pilot\nAs part of our course, we will be using GitHub’s new AI coding tool called Co-Pilot. Co-Pilot is a tool that uses machine learning to suggest code as you write. It is a bit like the auto-complete feature in Google Docs, but instead of just suggesting words, it suggests entire lines of code. You can read more about it here https://copilot.github.com/.\nAs a student on GitHub, you should be able to get free access to Co-Pilot. You can read more about how to get access here https://docs.github.com/en/github/copilot/getting-started-with-github-copilot.\nIf for whatever reason you cannot get access, please let the instructor know and we can try and find a solution. Other tools that may be of use include ChatGPT (the free 3.5 version) or gpt4all (which is a bit more complicated to set up).",
    "crumbs": [
      "Resources",
      "Introductions & Installations",
      "Course Software & Tools"
    ]
  },
  {
    "objectID": "materials/introducing-humanities-computing/01-course-tools.html#vs-code",
    "href": "materials/introducing-humanities-computing/01-course-tools.html#vs-code",
    "title": "Course Software & Tools",
    "section": "VS Code",
    "text": "VS Code\nWe will be using Visual Studio Code (VS Code) as our primary code editor for this course. You can download VS Code here https://code.visualstudio.com/. You should be able to use the link above to download VS Code for Windows, Mac, and Linux.\nLike GitHub and Co-Pilot, VS Code is also owned by Microsoft, which is why all three work relatively well together. VS Code is something called a Integrated Development Environment (IDE), which is a fancy way of saying it is a program for writing code. You can read more about IDEs here https://en.wikipedia.org/wiki/Integrated_development_environment. Other popular IDEs include Atom, Sublime Text, and PyCharm. While you are welcome to use any IDE you want, I will be using VS Code in this course and I’m not sure that Co-Pilot works with other IDEs, so would highly encourage you to at least test it out.\n\n\n\nVisual Studio Code\n\n\nAn example of Visual Studio Code\n\n\n\nVisual Studio Code\n\n\nComparison of Most Popular IDEs from the annual Stack Overflow Survey\nAnother benefit of getting used to VS Code is that you can use it in the browser for any Github repository by changing .com to .dev. So for example if you took the link to our reposition, https://github.com/ZoeLeBlanc/is310-computing-humanities-2024 and switched it to https://github.dev/ZoeLeBlanc/is310-computing-humanities-2024, you would see the VS Code version of our repository in the browser.\n\nInstallation for Mac Users\nFor Macs, you will need to check if you have an Intel Chip or an Apple Silicon. You can check this by going to the Apple menu  &gt; About This Mac.\n\n\n\nMac chip\n\n\nYou can see in this example the Chip says Apple M2 Pro, which means this is an Apple Silicon. If you see Intel then you have an Intel Chip. Download the appropriate version for your computer through selecting the zip option and then opening and installing the program.\n\n\nInstallation for Windows Users\nFor Windows, you will need to check if you have a 64-bit or Arm64 operating system. You can check this by going to Settings &gt; System &gt; About and looking at the System type field.\n\n\n\nWindows chip\n\n\nOnce identified, you should select the User Installer option and then open and install the program.\n\n\nVS Code Extensions\nOnce you have VS Code installed, you will need to install a few extensions. Extensions are a bit like apps for VS Code, and you can read more about them here https://code.visualstudio.com/docs/editor/extension-gallery. You can install extensions by clicking on the Extensions icon in the left sidebar of VS Code and then searching for your relevant extension. We will be installing various extensions over the course of the semester, but the first one you should install is the GitHub Copilot extension. You can find it by searching for GitHub Copilot in the Extensions Marketplace.\n\n\n\nVS Code GitHub Copilot\n\n\nOnce you click install, you will need to reload VS Code and then you should be able to use Co-Pilot. We will be learning how to use Co-Pilot with VS Code in the next few weeks, but you can read more about it here https://code.visualstudio.com/docs/editor/github-copilot.\n\nDisabling GitHub Co-Pilot\nWhile this class allows you to use AI tools, some of your other coding classes will not. So, if you need to disable Co-Pilot, all you need to do is click the Extensions icon in the left sidebar of VS Code, search for GitHub Copilot, and then click Disable.\n\n\n\nDisable GitHub Copilot\n\n\nOnce it is disabled, you will need to reload VS Code. And then you can re-enable it by clicking Enable in the same location.\n\n\n\nEnable GitHub Copilot",
    "crumbs": [
      "Resources",
      "Introductions & Installations",
      "Course Software & Tools"
    ]
  },
  {
    "objectID": "materials/introducing-humanities-computing/01-course-tools.html#python",
    "href": "materials/introducing-humanities-computing/01-course-tools.html#python",
    "title": "Course Software & Tools",
    "section": "Python",
    "text": "Python\n\nWhat is Python?\n\n\n\nPython Logo\n\n\n\n\n\nPython Definition\n\n\nYou can read more about Python here https://en.wikipedia.org/wiki/Python_(programming_language) and here https://www.python.org/about/ (also in case you did not know, yes Python is named after the comedic Monty Python series). This course presumes you already have about a semester of experience with Python, if not more, but if you are new to Python, please let the Instructor know and also would highly recommend taking a look at the resources on The Programming Historian https://programminghistorian.org/en/lessons/ and looking at Melanie Walsh, Introduction to Cultural Analytics & Python, https://melaniewalsh.github.io/Intro-Cultural-Analytics/02-Python/00-Python.html.\n\n\nWhy Python?\n\n\n\n\n\n\npython twitter\n\n\nJake VanderPlas Tweet\n\n\n\n\n\npython stack overflow\n\n\nStack Overflow’s Annual Survey\n\n\n\nIn this course, you’ll be writing code in a few different programming languages, but we’ll primarily be using Python. The choice of Python is for a few different reasons. First, Python is well suited to both web development and data analysis – two of the core activities of this course. Second, Python is increasingly becoming a standard in a number of industries and academic disciplines, though other languages like R are also popular. Third is that many of the programming concepts you will learn in Python can transfer relatively well to other programming languages. And finally, Python is a popular language for Digital Humanities research and is used by a number of different DH projects and platforms.\n\n\nInstalling Python\nInstalling Python and getting it correctly configured is increasingly a very challenging and time consuming task, but I believe it is worthwhile since you will ideally keep coding after this course ends. But I want to forewarn you that this process can be frustrating and time consuming, so please reach out to the instructor if you get stuck or if you are feeling unsure or nervous about this task. To give you a sense of what I mean, here is a comic from xkcd about the Python Environment.\n\n\n\nPython Environment\n\n\nPython Environment by Russell Monroe\n\nMac Installation\nYou may have previously used Anaconda for installing Python on your Mac. Anaconda has a number of benefits, but also a number of drawbacks once you start getting deeper into programming. For this course, we will be trying our best to install Python without Anaconda, but if you are having trouble, please let the instructor know and we can try and find a solution.\nWe’ll largely be following the instructions from https://realpython.com/installing-python/#how-to-install-python-on-macos.\nThe first step is to check if you have Python installed on your computer. You can do this by opening up VS Code and then opening up the terminal. You can do this by clicking on either the Terminal menu option and then selecting New Terminal, or selecting View and then selecing the Terminal optional. You should see a terminal window open up at the bottom of your VS Code window, and can find more detailed information on this here https://code.visualstudio.com/docs/terminal/basics.\n\n\n\nVS Code Terminal\n\n\nOnce you have your terminal open, you can check if you have Python installed by typing the following command:\npython --version\nThis should show you the version of Python you have installed, and it is likely going to be 2.7 since that ships with Mac. Unfortunately, that version is now officially retired and so will you might come across a library or two using it, you’ll end with a lot of errors if we don’t upgrade (also you can check the old countdown clock for the migration from Python 2 to 3 https://pythonclock.org/).\nYou should also double check that you have not already installed Python 3. You can do this by typing the following command:\npython3 --version\nIf you have Python 3 installed, you should see something like this:\nPython 3.9.7\nWhile your version number might be different, you should see Python 3 in the output. If you do not see Python 3 in the output, then you will need to install Python 3.\nThere’s currently two ways to install Python on a Mac, using either the official installer or the Homebrew package manager. There’s pros and cons to each, and so if you think you might be using Python quite a bit going forward I would recommend the Homebrew approach, but if you are not sure, then I would recommend the official installer.\n\nHomebrew Installation\nOpen a browser and navigate to http://brew.sh/. You should see a command for installing Homebrew near the top of the page under the tile “Install Homebrew.” It should look something like this:\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\nCopy this command and paste it into the VS Code Terminal, and then press enter. You will be prompted to enter your computer password (i.e. the one you use to login into your Macbook). You will need to enter this password but you will not be able to see it as your typing. Once you have typed it, hit enter and then Homebrew will install.\nYou should also upgrade Homebrew by typing the following command into the terminal:\nbrew update && brew upgrade\nThis will take quite some time, so do not be concerned if it is taking an hour or more (more than that though there’s probably an issue and you should contact the instructor).\nOnce that’s complete, you may also need to install Xcode Command Line Tools, by typing into the terminal:\nxcode-select --install\nIf you already have Xcode installed, you’ll see a message like this:\nxcode-select: error: command line tools are already installed, use \"Software Update\" to install updates\nOtherwise, you’ll see a message about installing the tools, and you’ll need to click Install to continue.\nTo ensure that Homebrew works with your computer, you will need to add Homebrew to your path. You can do this by typing the following command into your terminal:\necho 'export PATH=\"/opt/homebrew/bin:$PATH\"' &gt;&gt; ~/.zshrc\nAnd then:\nsource ~/.zshrc\nWe will discuss what this is doing a bit more below, but for now, just know that it is making it so your computer uses Homebrew with Python.\nThen you will need to install Python 3 by typing the following command into your terminal:\nbrew install python3\nYou can check that you have Python 3 installed by typing the following command:\npython3 --version\nYou should see something like this:\nPython 3.9.7\nWhile your version number might be different, you should see Python 3 in the output.\n\n\nOfficial Installer\nYou can download the official installer here https://www.python.org/downloads/macos/. You should download the latest version of Python 3, which should be 3.12 and use the macOS 64-bit universal installer. Once downloaded, you should open the installer and follow the instructions. You can find more detailed instructions here https://realpython.com/installing-python/#how-to-install-from-the-official-installer.\nOnce installed, you can check that you have Python 3 installed by typing the following command in the VS Code terminal:\npython3 --version\nYou should see something like this:\nPython 3.9.7\nWhile your version number might be different, you should see Python 3 in the output.\n\n\n\nWindows Installation (Somewhat Optional)\nIf you have a previous version of Python installed on your Windows machine, you are welcome to use that rather than following the instructions below. I would highly recommend the setup described below if you have long term plans to use Python, but if you are not sure, then you can use your previous version of Python. However, even those this installation choice is optional, you must be able to run Python. So please reach out to the instructor if you are having trouble.\nWe’ll be following the instructions for using Python for web development on Windows https://docs.microsoft.com/en-us/windows/python/web-frameworks and https://learn.microsoft.com/en-us/windows/python/beginners.\n\nInstalling WSL\nThe first step is to install the Windows Subsystem for Linux following these steps https://docs.microsoft.com/en-us/windows/python/web-frameworks#install-windows-subsystem-for-linux.\nIn either PowerShell or Command Prompt, run the following command as an administrator:\nwsl --install\nYou will then be prompted to restart your computer. Once restarted, you will need to open PowerShell or Command Prompt again as an administrator and run the following command:\nwsl --set-default-version 2\nAccording to the instructions: &gt; This command will enable the required optional components, download the latest Linux kernel, set WSL 2 as your default, and install a Linux distribution for you (Ubuntu by default, see below to change this).\n\nThe first time you launch a newly installed Linux distribution, a console window will open and you’ll be asked to wait for files to de-compress and be stored on your machine. All future launches should take less than a second.\n\nAfter this you’ll need to setup your WSL environment following these instructions https://docs.microsoft.com/en-us/windows/wsl/setup/environment.\nSpecifically you’ll need to set up your Linux username and password https://docs.microsoft.com/en-us/windows/wsl/setup/environment#set-up-your-linux-username-and-password.\n\n\n\nWSL Setup\n\n\nOnce setup, I would recommend first checking which version of WSL you are running and upgrading to WSL 2 if you are not already using it https://docs.microsoft.com/en-us/windows/wsl/install#check-which-version-of-wsl-you-are-running. And then I would recommend updating your packages for Ubuntu https://docs.microsoft.com/en-us/windows/wsl/setup/environment#update-and-upgrade-packages.\n\nCustomizing Windows Terminal and VS Code\nNow that you have WSL installed, you can customize your Windows Terminal. I recommend using the https://docs.microsoft.com/en-us/windows/wsl/setup/environment#set-up-windows-terminal guide to customize your terminal.\nYou should also familiarize yourself with working with files in WSL https://docs.microsoft.com/en-us/windows/wsl/setup/environment#file-storage.\nNow you can get VS Code working with WSL by following these installation instructions for working with the Remote WSL Extension https://docs.microsoft.com/en-us/windows/wsl/tutorials/wsl-vscode#install-vs-code-and-the-remote-wsl-extension.\nAccording to the docs: &gt; In order to install the Remote-WSL extension, you will need the 1.35 May release version or later of VS Code. We do not recommend using WSL in VS Code without the Remote-WSL extension as you will lose support for auto-complete, debugging, linting, etc. Fun fact: this WSL extension is installed in $HOME/.vscode/extensions (enter the command ls $HOME.vscode in PowerShell).\nNow you can open Windows Terminal or PowerShell and type:\ncode .\nAnd that should start your VS Code Server\n\n\n\nVS Code Server\n\n\nOR you can open VS Code and follow these instructions: &gt; You can also access more VS Code Remote options by using the shortcut: CTRL+SHIFT+P in VS Code to bring up the command palette. If you then type Remote-WSL you will see a list of the VS Code Remote options available, allowing you to reopen the folder in a remote session, specify which distribution you want to open in, and more.\n\n\n\nVS Code Remote Options\n\n\n\n\n\n\nBreak In Case of Emergency (Google Colab or Anaconda)\nIf, after consulting with the Instructor, you still cannot get Python installed, you do have installing Python via Anaconda or using Google Colab for this course.\nAnaconda is a free and open-source distribution of the Python and R programming languages for scientific computing, that aims to simplify package management and deployment. You can read more about it here https://en.wikipedia.org/wiki/Anaconda_(Python_distribution). You can download Anaconda here https://www.anaconda.com/products/individual. You should be able to use the link above to download Anaconda for Windows, Mac, and Linux.\nGoogle Colab is a free cloud service that allows you to write and execute Python in your browser. You can read more about it here https://colab.research.google.com/notebooks/intro.ipynb. You will need a Google account to use Colab, but you can use your @illinois.edu email.\nI would prefer that we only use these options in the most extreme circumstances, but wanted to share that it is available and an option.",
    "crumbs": [
      "Resources",
      "Introductions & Installations",
      "Course Software & Tools"
    ]
  },
  {
    "objectID": "materials/introducing-humanities-computing/01-course-tools.html#terminals-and-oh-my-zsh-optional",
    "href": "materials/introducing-humanities-computing/01-course-tools.html#terminals-and-oh-my-zsh-optional",
    "title": "Course Software & Tools",
    "section": "Terminals and Oh-My-Zsh (Optional)",
    "text": "Terminals and Oh-My-Zsh (Optional)\nCurrently in VS Code, when you open a Terminal, you’ll see somewhat gnarly looking text like this:\nzoe@ZoeLeBlanc MINGW64 ~/Documents/GitHub/is310-computing-humanities-2026 (main)\nThis is because VS Code is using the default terminal, whether for Mac or Windows.\nWe will discuss more in-depth what terminals are in our Command Line session, but for now, you should know that a terminal is a bit like an app for working with files, but instead of clicking on icons, you write commands. You can read more about terminals here https://en.wikipedia.org/wiki/Command-line_interface.\nInstalling a separate terminal is completely optional since VS Code has built-in functionality, but in case you would like to try it out, I recommend iTerm2 https://iterm2.com/ for Mac and the Windows Terminal https://www.microsoft.com/en-us/p/windows-terminal/9n0dx20hk701 for Windows (for more info about Windows Terminal see this blog post). You should be able to use both these links to download the appropriate version for your operating system, but please let the instructor know if you are having problems.\nOnce you have installed your terminal, you can customize it to be a little more human readable through using Oh-My-Zsh https://ohmyz.sh/. This step is again optional, but I find having a nicer and more readable terminal is helpful for working with code.\n\nWindows Installation\nZsh is a unix shell and should be already installed on your WSL system. If not though you can install with the following:\nsudo apt install zsh\nThen you install oh-my-zsh with the following:\nsh -c \"$(curl -fsSL https://raw.github.com/ohmyzsh/ohmyzsh/master/tools/install.sh)\"\n\n\n\ninstall zsh\n\n\nYou can find more in depth instructions here https://dev.to/contactsunny/installing-zsh-and-oh-my-zsh-on-windows-11-with-wsl2-1p5i\n\n\nMac Installation\nZsh is a unix shell and should be already installed on your Mac.\nInstall oh-my-zsh with the following:\nsh -c \"$(curl -fsSL https://raw.github.com/ohmyzsh/ohmyzsh/master/tools/install.sh)\"\n\n\n\ninstall zsh\n\n\nYou can find more in depth instructions here https://dev.to/hannahgooding/how-i-customise-my-terminal-with-oh-my-zsh-macos-427i\n\n\nPowerShell Alternative\nIf you decided not to install WSL and our instead using PowerShell, rather than using oh-my-zsh which only works on unix/linux operating systems, you can instead install oh-my-posh, following these instructions https://ohmyposh.dev/docs/installation/windows. Again this step is optional but as you can see in the image below, these types of configurations do help make your terminal more human readable.\n\n\n\noh-my-posh",
    "crumbs": [
      "Resources",
      "Introductions & Installations",
      "Course Software & Tools"
    ]
  },
  {
    "objectID": "materials/introducing-humanities-computing/01-course-tools.html#git",
    "href": "materials/introducing-humanities-computing/01-course-tools.html#git",
    "title": "Course Software & Tools",
    "section": "Git",
    "text": "Git\n\nWhat is Git?\n\n\n\nGit Logo\n\n\n\n\n\nGit Definition\n\n\nYou can read more about Git here https://en.wikipedia.org/wiki/Git and here https://git-scm.com/about. Git is a version control system that allows you to track changes to files over time that was first created by Linus Torvals in 2005. It is a bit like Google Drive, but instead of just tracking changes to files, it also allows you to track changes to folders and directories. Somewhat confusingly, git is not the same as GitHub, though the two often are used together. We will be delving into these differences and how to use them later this week.\n\nMac Installation\nGit should already be installed on your Mac, but you can check by opening up the VS Code terminal and typing:\ngit --version\nIf you do not have git installed, this will prompt you to install it. You can also install it by following these instructions https://git-scm.com/book/en/v2/Getting-Started-Installing-Git.\n\n\nWindows Installation\nOpen the VS Code terminal and make sure that the WSL is selected as your terminal. You can do this by clicking on the Terminal menu option and then selecting Select Default Profile. You should see a list of options, and you should select Ubuntu or Ubuntu-20.04 (or whatever version of Ubuntu you have installed).\nNow you first need to update the package list with the following command:\nsudo apt update\nThen you can install git with the following command:\nsudo apt install git\nYou can check that git is installed by typing:\ngit --version\n\n\n\nGit Configuration\nOnce you have git installed, you will need to configure it. You can do this by opening up the VS Code terminal and typing:\ngit config --global user.name \"Your Name\"\ngit config --global user.email \"your.email@example.com\"\nReplace “Your Name” with your exact GitHub Username handle. So for example, in my case it would be git config --global user.name \"Zoe LeBlanc\".\nReplace “your.email@example.com” with the email address you used to sign up for GitHub. So in my case it would be git config --global user.email \"zleblanc@illinois.edu\".\nBe sure to get both of these exactly right, as they are used to connect your GitHub account and git activity.\nYou can also check your git configuration by typing:\ngit config --list\nCONGRATS YOU’VE SETUP YOUR COMPUTER 🎉",
    "crumbs": [
      "Resources",
      "Introductions & Installations",
      "Course Software & Tools"
    ]
  },
  {
    "objectID": "materials/introducing-humanities-computing/01-course-tools.html#footnotes",
    "href": "materials/introducing-humanities-computing/01-course-tools.html#footnotes",
    "title": "Course Software & Tools",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFor more about this acquistion see Warren, Tom. “Microsoft Confirms It Will Acquire GitHub for $7.5 Billion.” The Verge, June 4, 2018. https://www.theverge.com/2018/6/4/17422788/microsoft-github-acquisition-official-deal.↩︎\nDohmke, Thomas. “100 Million Developers and Counting.” The GitHub Blog (blog), January 25, 2023. https://github.blog/2023-01-25-100-million-developers-and-counting/.↩︎\nDavid., “Seeds Or Code?,” accessed April 5, 2023, https://blog.dshr.org/2019/11/seeds-or-code.html.↩︎\nFor more information on this topic, you can read “The Schism at the Heart of the Open-Source Movement” and Dear GitHub.↩︎",
    "crumbs": [
      "Resources",
      "Introductions & Installations",
      "Course Software & Tools"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "IS310 - Culture As Data Spring 2026",
    "section": "",
    "text": "Link to iSchool Course Listing"
  },
  {
    "objectID": "index.html#course-description",
    "href": "index.html#course-description",
    "title": "IS310 - Culture As Data Spring 2026",
    "section": "Course Description",
    "text": "Course Description\nCurrently the iSchool catalogue lists this following course description:\n\nExplores use and application of technology to scholarly activity in the humanities, including projects that put classic texts on the web or create multimedia applications on humanities topics.\n\nThis isn’t necessarily wrong, but not quite descriptive enough for this version of the course.\nOur goal in this course is to start to understand how culture broadly defined can be represented as data and studied with computation. By culture, we don’t mean bacteria cultures, but rather the type of culture that is usually associated with academic disciplines in the Humanities, such as English, History, Fine Arts, etc.. Culture in this context encompasses everything from popular fiction to newspapers, government documents, and even sociological studies of online communities like Reddit or TikTok subcultures. While culture is an intrinsic part of being human, today that culture is increasingly both digital and datafied. Representing culture whether digitally or as data might seem obvious initially; after all, most of how we all work and socialize is now often experienced through digital platforms, generating bytes and bytes of data. However, in this course we will investigate how representing our cultural heritage and past is rarely straightforward or without tradeoffs.\nThis course will also explore how humanities can change how we think about computing. We will explore how histories of data collection and computation can help us understand that these activities are fundamentally political, even if we often see them as ‘technical’ and therefore somehow neutral or objective. We will consider the ways that interpretation can become ‘baked’ into these technologies, and how in turn this makes it difficult to discern how these forces are shaping both scholarship and society.\nWhile these are all big topics, we will investigate them through weekly readings and assignments, as well as a semester-long project. We will experience the process of working with culture as data - from developing an initial humanistic question to collecting and curating data to analyzing and communicating our findings. Through weekly assignments and projects, we will debate larger issues in computing in the humanities, including data ethics and privacy, sustainability and curation of digital projects, the possibilities and limitations of computational methods, how this relates to other disciplines, and how computing in the humanities is part of global conversations about data and society."
  },
  {
    "objectID": "index.html#overall-learning-objectives",
    "href": "index.html#overall-learning-objectives",
    "title": "IS310 - Culture As Data Spring 2026",
    "section": "Overall Learning Objectives",
    "text": "Overall Learning Objectives\n\nExplore computing in the humanities as a research field. Through class discussions, readings, selected projects, and assignments, this course will provide an overview of the history, debates, and current trends in the field.\nExperiment with computing in the humanities as a research praxis. Through learning coding, data analysis, and project management, this course will provide a foundation for how to make projects that blend culture and coding, as well as engage with some of the debates over how to maintain and evaluate this type of scholarship.\n\nWhat comes after this course?\nMuch of what you do with this course depends on your interests, but at the very least, you will be well equipped to continue undertaking substantive and innovative research on culture using computation and data. These skills are incredibly useful whether you aim to be a data scientist, journalist, HCI or UX researcher, or simply someone who understands how technology and information shape our world. Ideally, I hope each of you continues to work on your final project and share your research long after the course ends."
  },
  {
    "objectID": "index.html#pre--and-co-requisites",
    "href": "index.html#pre--and-co-requisites",
    "title": "IS310 - Culture As Data Spring 2026",
    "section": "Pre- and Co-Requisites",
    "text": "Pre- and Co-Requisites\nThere is no required prerequisite but students should have some previous experience equivalent to a semester of programming, ideally in Python. Relevant courses include IS205 and IS107.\nInterested students should contact the instructor if they have any questions."
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "IS310 - Culture As Data Spring 2026",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThanks to John R Ladd, Melanie Walsh, Anna Preus, Brandon Walsh, Meredith Martin, Sierra Eckert, Anelise Shrout, Cameron Blevins, Lincoln Mullen, Benjamin Schmidt, Lauren Klein, Miriam Posner, Alan Liu, Ted Underwood, and Ryan Cordell for sharing their syllabi - all of which have been immensely helpful and influential.\nI also want to especially thank Rebecca Munson, who remains an inspiration for how I think and teach about data and who is sorely missed but will never be forgotten."
  },
  {
    "objectID": "slides/index.html",
    "href": "slides/index.html",
    "title": "Course Slides",
    "section": "",
    "text": "Slides from class lectures and presentations."
  },
  {
    "objectID": "slides/index.html#available-slides",
    "href": "slides/index.html#available-slides",
    "title": "Course Slides",
    "section": "Available Slides",
    "text": "Available Slides\n\nFirst Class Slides\nSecond Class Slides"
  },
  {
    "objectID": "slides/02-programming-and-processing.html#reminder-these-are-required",
    "href": "slides/02-programming-and-processing.html#reminder-these-are-required",
    "title": "Programming & Processing",
    "section": "Reminder: These Are Required",
    "text": "Reminder: These Are Required\n\nSlack Account with DH@UIUC Server.\nHypothesis Account with IS310-Spring-2026 Group.\nGitHub Account.\nLocal Python 3 Installation.\nGit.\nLocal Text Editor (VS Code Recommended)."
  },
  {
    "objectID": "slides/02-programming-and-processing.html#reminder-these-are-optional",
    "href": "slides/02-programming-and-processing.html#reminder-these-are-optional",
    "title": "Programming & Processing",
    "section": "Reminder: These Are Optional",
    "text": "Reminder: These Are Optional\n\nAI Tooling of Your Choice:\n\nRecommend GitHub Co-Pilot and using GitHub Education Benefits to get access to Co-Pilot.\n\nIDE of Your Choice:\n\nRecommend Visual Studio Code and Visual Studio Code Extensions.\n\nOh-My-Zsh."
  },
  {
    "objectID": "slides/02-programming-and-processing.html#any-python-issues",
    "href": "slides/02-programming-and-processing.html#any-python-issues",
    "title": "Programming & Processing",
    "section": "Any Python Issues?",
    "text": "Any Python Issues?\n\nPython Environment by Russell Monroe"
  },
  {
    "objectID": "slides/02-programming-and-processing.html#why-python",
    "href": "slides/02-programming-and-processing.html#why-python",
    "title": "Programming & Processing",
    "section": "Why Python?",
    "text": "Why Python?\n\n\n\n\n\n\nJake VanderPlas Tweet\n\n\n\n\n\n\n\nStack Overflow’s Annual Survey"
  },
  {
    "objectID": "slides/02-programming-and-processing.html#alternatives-to-github",
    "href": "slides/02-programming-and-processing.html#alternatives-to-github",
    "title": "Programming & Processing",
    "section": "Alternatives to GitHub",
    "text": "Alternatives to GitHub\n\n\nWhile alternatives to GitHub exist (primarily GitLab or Bitbucket), it is hard to overstate how much in the last decade GitHub has become the platform for this type of work. Such a centralization is truly a double-edged sword"
  },
  {
    "objectID": "slides/02-programming-and-processing.html#arctic-code-vault",
    "href": "slides/02-programming-and-processing.html#arctic-code-vault",
    "title": "Programming & Processing",
    "section": "Arctic Code Vault",
    "text": "Arctic Code Vault\n\nGitHub Arctic Code Vault\nFor example, GitHub has created what amounts to the largest code archive in the world through their GitHub Arctic Code Vault, an initiative to take a snapshot of all code hosted on their platform on February 2, 2020 and store it in the Svalbard Global Seed Vault. Such an initiative is impressive and was done in partnership with some academic institutions (the exact partners were the Long Now Foundation, the Internet Archive, the Software Heritage Foundation, Arctic World Archive, Microsoft Research, the Bodleian Library, and Stanford Libraries). But it also raises a number of ethical, archival, and political questions. For example, this Arctic Code Vault has been criticized for its erasure of the indigenous Sami people who live on Svalbard and its performative approach to archives. GitHub also has contracts with U.S. Immigrations and Customs Enforcement (ICE) led to protests and boycotts from some sectors of the developer community.[^4] While we plan to use this platform, it’s important to do so with these issues in mind."
  },
  {
    "objectID": "slides/02-programming-and-processing.html#what-is-data-labor-why-does-data-labor-matter",
    "href": "slides/02-programming-and-processing.html#what-is-data-labor-why-does-data-labor-matter",
    "title": "Programming & Processing",
    "section": "What is Data Labor? Why Does Data Labor Matter?",
    "text": "What is Data Labor? Why Does Data Labor Matter?"
  },
  {
    "objectID": "slides/02-programming-and-processing.html#what-is-laion-5b",
    "href": "slides/02-programming-and-processing.html#what-is-laion-5b",
    "title": "Programming & Processing",
    "section": "What is LAION-5B?",
    "text": "What is LAION-5B?\n\n\nLAION-5B is a “foundation dataset” for generative AI - it’s what models like Stable Diffusion and Midjourney are trained on. The scale makes human review essentially impossible, which is precisely why problems emerge.\n\nReleased in 2022 by LAION, a German non-profit\nImages and text captions scraped from the internet\nBuilt to give AI models “a comprehensive representation of the world”"
  },
  {
    "objectID": "slides/02-programming-and-processing.html#how-was-laion-5b-built",
    "href": "slides/02-programming-and-processing.html#how-was-laion-5b-built",
    "title": "Programming & Processing",
    "section": "How was LAION-5B Built?",
    "text": "How was LAION-5B Built?\n\n\nLAION-5B was built from Common Crawl using automated methods:\n\nSearch for HTML &lt;img&gt; tags with alt attributes\n\nALT tags describe what site owners want algorithms to see, not what humans see\nOnly about 40% of images on the web have ALT tags. Sites like Pinterest, Shopify, and SlidePlayer have high ALT tag coverage because they auto-generate them for SEO purposes. This means LAION-5B is “powerfully shaped by commercial logics” - it reflects how search engines see the world, not how humans do."
  },
  {
    "objectID": "slides/02-programming-and-processing.html#what-is-in-common-crawl",
    "href": "slides/02-programming-and-processing.html#what-is-in-common-crawl",
    "title": "Programming & Processing",
    "section": "What is in Common Crawl?",
    "text": "What is in Common Crawl?\nSchaul, Kevin, Szu Yu Chen, and Nitasha Tiku. “Inside the Secret List of Websites That Make AI like ChatGPT Sound Smart.” Washington Post, April 19, 2023."
  },
  {
    "objectID": "slides/02-programming-and-processing.html#about-me",
    "href": "slides/02-programming-and-processing.html#about-me",
    "title": "Programming & Processing",
    "section": "About Me",
    "text": "About Me\nLucy, Li, Suchin Gururangan, Luca Soldaini, et al. “AboutMe: Using Self-Descriptions in Webpages to Document the Effects of English Pretraining Data Filters.” arXiv:2401.06408. Preprint, arXiv, January 12, 2024. https://doi.org/10.48550/arXiv.2401.06408."
  },
  {
    "objectID": "slides/02-programming-and-processing.html#about-me-1",
    "href": "slides/02-programming-and-processing.html#about-me-1",
    "title": "Programming & Processing",
    "section": "About Me",
    "text": "About Me"
  },
  {
    "objectID": "slides/02-programming-and-processing.html#what-is-seeing-like-an-algorithm",
    "href": "slides/02-programming-and-processing.html#what-is-seeing-like-an-algorithm",
    "title": "Programming & Processing",
    "section": "What is seeing like an algorithm?",
    "text": "What is seeing like an algorithm?\n\n\n\nUse OpenAI’s CLIP model to score image-text similarity\nInclude pairs above a threshold (0.26-0.28)\nALT tags describe what site owners want algorithms to see, not what humans see\n\nOnly about 40% of images on the web have ALT tags. Sites like Pinterest, Shopify, and SlidePlayer have high ALT tag coverage because they auto-generate them for SEO purposes. This means LAION-5B is “powerfully shaped by commercial logics” - it reflects how search engines see the world, not how humans do."
  },
  {
    "objectID": "slides/02-programming-and-processing.html#excavating-ai",
    "href": "slides/02-programming-and-processing.html#excavating-ai",
    "title": "Programming & Processing",
    "section": "Excavating AI",
    "text": "Excavating AI\nThe Politics of Images in Machine Learning Training Sets \nBy Kate Crawford and Trevor Paglen"
  },
  {
    "objectID": "slides/02-programming-and-processing.html#why-models-all-the-way-down-what-is-the-circularity-problem",
    "href": "slides/02-programming-and-processing.html#why-models-all-the-way-down-what-is-the-circularity-problem",
    "title": "Programming & Processing",
    "section": "Why Models All the Way Down? What is the Circularity Problem?",
    "text": "Why Models All the Way Down? What is the Circularity Problem?\n\n“There are models on top of models, and training sets on top of training sets.”\n\n\n\n“Omissions and biases and blind spots from these stacked-up models and training sets shape all of the resulting new models and new training sets.”\n\n\nThere’s a deep circularity here. LAION-5B’s similarity score comes from CLIP (trained on undisclosed data). To set thresholds, they trained another model and compared to ImageNet benchmarks. ImageNet’s gold standard was set by ResNet50. Each layer inherits biases from the previous ones.\n“Omissions and biases and blind spots from these stacked-up models and training sets shape all of the resulting new models and new training sets.” This is why investigating datasets is so important - it’s perhaps the only way to understand what these models actually contain."
  },
  {
    "objectID": "slides/02-programming-and-processing.html#what-are-the-problems-with-this-data",
    "href": "slides/02-programming-and-processing.html#what-are-the-problems-with-this-data",
    "title": "Programming & Processing",
    "section": "What are the problems with this data?",
    "text": "What are the problems with this data?\n\nWhy is scale a problem?\n\n\nThe Problem: In December 2023, researchers found over 1,000 images of Child Sexual Abuse Material (CSAM) in the dataset\n\n\n“If your full-time, eight-hours-a-day, five-days-a-week job were to look at each image in the dataset for just one second, it would take you 781 years.”\n\nLAION-5B contains 5.85 billion image-text pairs\nLAION-5B is a “foundation dataset” for generative AI - it’s what models like Stable Diffusion and Midjourney are trained on. The scale makes human review essentially impossible, which is precisely why problems emerge. The stated goal was research into automated dataset curation - building a dataset with “no humans in the mix.” The creators explicitly warned against using it for “ready-to-go industrial products.” This warning was largely ignored by Midjourney and Stable Diffusion."
  },
  {
    "objectID": "slides/02-programming-and-processing.html#how-are-these-datsets-curated",
    "href": "slides/02-programming-and-processing.html#how-are-these-datsets-curated",
    "title": "Programming & Processing",
    "section": "How are these datsets “curated”?",
    "text": "How are these datsets “curated”?"
  },
  {
    "objectID": "slides/02-programming-and-processing.html#curation-by-statistics",
    "href": "slides/02-programming-and-processing.html#curation-by-statistics",
    "title": "Programming & Processing",
    "section": "Curation by Statistics",
    "text": "Curation by Statistics\n\n“The tiniest of shifts in LAION’s thresholds could have excluded or included hundreds of millions of images”\n\n\n\n16% of images are within 0.01 of the lower threshold\nRaising the threshold by just 0.01 would remove 937 million image pairs\n\n\nThis is what “curation by statistics” looks like: tiny tweaks to code can have profound effects on the content of training sets. The thresholds are “very often poorly understood” - even by the developers who set them."
  },
  {
    "objectID": "slides/02-programming-and-processing.html#whose-images",
    "href": "slides/02-programming-and-processing.html#whose-images",
    "title": "Programming & Processing",
    "section": "Whose Images?",
    "text": "Whose Images?\nFor every English speaker: 1.6 image captions\nFor every French speaker: 0.5 image captions\nFor every Swahili speaker: 0.03 image captions\n\n\n“For models trained on LAION-5B, English (and English-speaking culture) is valued more than the other 107 languages combined.”\n\n\nThe language distribution in LAION differs significantly from Common Crawl. CLD3 (the language detector) miscategorized millions of captions - for example, labeling 34 million captions as Luxembourgian (a language only 300,000 people speak). These are mostly actually English."
  },
  {
    "objectID": "slides/02-programming-and-processing.html#whose-aesthetics",
    "href": "slides/02-programming-and-processing.html#whose-aesthetics",
    "title": "Programming & Processing",
    "section": "Whose Aesthetics?",
    "text": "Whose Aesthetics?\n\n“The concepts of what is and isn’t visually appealing can be influenced in outsized ways by the tastes of a very small group of individuals”\n\n\nMidjourney’s aesthetic fine-tuning: shaped by a handful of Discord users and a 65-year-old mechanical engineer from Wisconsin\n\nLAION-Aesthetics: a subset for “high visual quality” images\nThe aesthetic model was trained on:\n\n15,000 logos\n238,000 synthetic AI images rated by Discord users\n250,000 photos rated on dpchallenge.com\n\nThe Discord raters were described as “WEIRD” (Western, Educated, Industrialized, Rich, Democratic) and “nerdy” and “esoteric.” The dpchallenge.com top 50 reviewers are 95% from the US, Canada, or Europe - “mostly middle-aged photography enthusiasts from small American cities.”\nThe SAC dataset creators acknowledge that “most of the ratings in the dataset were submitted by a handful of users” whose “aesthetic preferences dominate the dataset.” Yet this shapes what millions of users see when they generate images."
  },
  {
    "objectID": "slides/02-programming-and-processing.html#what-does-this-mean-for-how-we-use-ai",
    "href": "slides/02-programming-and-processing.html#what-does-this-mean-for-how-we-use-ai",
    "title": "Programming & Processing",
    "section": "What does this mean for how we use AI?",
    "text": "What does this mean for how we use AI?\n\nWhat sorts of use cases are appropriate for AI?\nWhat dangers exist because of the reliance on training data?\nDo we understand the difference between and LLM and a chatbot? hint: models all the way down!"
  },
  {
    "objectID": "slides/02-programming-and-processing.html#what-is-a-terminal",
    "href": "slides/02-programming-and-processing.html#what-is-a-terminal",
    "title": "Programming & Processing",
    "section": "What is a Terminal?",
    "text": "What is a Terminal?\nA terminal is a way to interact with your computer via text commands.\n\nInstead of clicking on icons (GUI), we type explicit commands.\n\n\nYou’ve already used it!\npython3 --version\n\nSo far, you have been installing software via something called the terminal, but we have yet to explain what the terminal is or how it works.\nTerminals are a way to interact with your computer via text commands. Today, we usually give commands to a computer via a Graphical User Interface (GUI), pronounced “gooey”. Any time you download an app, whether on your phone or computer, you are using a GUI. GUIs let us click on icons, drag items, and interact with our computers in a visual way. Terminals let us do many of the same functions, but we have to be more explicit and write these commands."
  },
  {
    "objectID": "slides/02-programming-and-processing.html#a-brief-history-of-computing",
    "href": "slides/02-programming-and-processing.html#a-brief-history-of-computing",
    "title": "Programming & Processing",
    "section": "A Brief History of Computing",
    "text": "A Brief History of Computing\n\n\n\nEarly computer (ENIAC)\n\n\n\nBack in the 1950s and 1960s, computers were the size of entire rooms and functioned through the use of something called punch cards. These early computers required teams of operators, often comprised of women, who inputted punch cards to prompt the computers to process and output results on paper."
  },
  {
    "objectID": "slides/02-programming-and-processing.html#punch-cards",
    "href": "slides/02-programming-and-processing.html#punch-cards",
    "title": "Programming & Processing",
    "section": "Punch Cards",
    "text": "Punch Cards\n\n\n\nExample punch card from the Index Thomisticus\n\n\n\nPunch cards contained a series of holes that encoded information like numbers, letters, and even instructions for programs. These cards were fed into machines called card readers, translating the hole patterns into electrical signals the computer could understand. This enabled data entry for tasks like payroll calculations, statistical analysis, and even early gaming. The punch cards and computers were run by teams of operator, often comprised of women, who inputted these punch cards, prompting the computers to process and output the results on paper."
  },
  {
    "objectID": "slides/02-programming-and-processing.html#punch-cards-1",
    "href": "slides/02-programming-and-processing.html#punch-cards-1",
    "title": "Programming & Processing",
    "section": "Punch Cards",
    "text": "Punch Cards\n\nOne of the first Computing in the Humanities projects started in the 1940s to digitize the writings of Thomas Aquinas. More information available here https://theoreti.ca/?p=6096\nFor more on this history, I would highly recommend watching “Computer History: Punch Cards Historical Overview -IBM Remington Rand UNIVAC - History 1900’s-1960’s”, 2016. https://www.youtube.com/watch?v=kKJxzay85Vk.\n\n\nThe term terminal then refers to the device that allowed operators to interact with the computer, and the command line refers to the text-based interface that allowed operators to input commands to the computer."
  },
  {
    "objectID": "slides/02-programming-and-processing.html#the-first-gui",
    "href": "slides/02-programming-and-processing.html#the-first-gui",
    "title": "Programming & Processing",
    "section": "The First GUI",
    "text": "The First GUI\n\nSmallTalk: The First GUIReimer, Jeremy. “A History of the GUI.” Ars Technica, May 5, 2005. https://arstechnica.com/features/2005/05/gui/.\n\nThe command line was the only way to interact with computers until the 1970s, when the first Graphical User Interface (GUI) was developed at Xerox PARC. This GUI was later adopted by Apple and Microsoft, and is the basis for the GUIs we use today. Smalltalk was conceived as a programming language and development environment so easy to use that a child could understand it, and in many respects was successful in this goal. Smalltalk was the world’s first object-oriented programming language, where program code and data could be encapsulated into single units called objects that could then be reused by other programs without having to know the details of the object’s implementation. It also had modern, Java-like features like automatic memory management, to take some of the hard work away from the programmer. The development environment of Smalltalk was also the user interface that Smalltalk programs ran in, and introduced many modern GUI concepts. It first began to take shape around 1974, and was continuously updated and enhanced."
  },
  {
    "objectID": "slides/02-programming-and-processing.html#shells",
    "href": "slides/02-programming-and-processing.html#shells",
    "title": "Programming & Processing",
    "section": "Shells?",
    "text": "Shells?\n\n\nToday when we use terminals, we are not just using the terminal but also using often using something called a shell. A “shell” is a program within a terminal that interprets user commands and processes computer output. Originating from the Unix operating system, the term “shell” signifies a user-friendly interface that encapsulates the complexities of various computer systems. Popular shell programs include bash (Bourne Again SHell), which is the default shell for most Linux distributions and MacOS; PowerShell, which is Windows default option, and zsh (Z Shell) – the optional configuration from our course tools. Each shell has its own set of commands and syntax, but they all share the same basic functionality."
  },
  {
    "objectID": "slides/02-programming-and-processing.html#key-terminology",
    "href": "slides/02-programming-and-processing.html#key-terminology",
    "title": "Programming & Processing",
    "section": "Key Terminology",
    "text": "Key Terminology\n\n\n\n\n\n\n\nTerm\nDefinition\n\n\n\n\nTerminal\nThe device/program that allows you to interact with the computer\n\n\nShell\nA program that interprets your commands (bash, zsh, PowerShell)\n\n\nCommand Line\nThe text-based interface where you type commands\n\n\n\n\nThe term “terminal” refers to the device that allowed operators to interact with the computer. The “command line” refers to the text-based interface that allowed operators to input commands. The command line was the only way to interact with computers until the 1970s, when the first GUI was developed at Xerox PARC.\nA “shell” is a program within a terminal that interprets user commands and processes computer output. Popular shells include bash (default for most Linux/MacOS), PowerShell (Windows), and zsh."
  },
  {
    "objectID": "slides/02-programming-and-processing.html#gui-vs-cli",
    "href": "slides/02-programming-and-processing.html#gui-vs-cli",
    "title": "Programming & Processing",
    "section": "GUI vs CLI",
    "text": "GUI vs CLI\n\n\nGUI (Graphical) \n\nCLI (Command Line) \n\n\nIn the GUI example, we see using the Mac Finder App to interact graphically with file folders - clicking and moving files around with a mouse.\nIn the CLI example, we see typing commands in the terminal like mkdir and rmdir to create and remove directories. The shell being used is bash."
  },
  {
    "objectID": "slides/02-programming-and-processing.html#creating-a-directory",
    "href": "slides/02-programming-and-processing.html#creating-a-directory",
    "title": "Programming & Processing",
    "section": "Creating a Directory",
    "text": "Creating a Directory\nOpen your terminal in VS Code: Terminal &gt; New Terminal\n\nmkdir Intro-CA-Notes\n\n\nBut how do we know it worked?\n\nTo create a directory (folder), we use the mkdir command followed by the name we want. But after pressing enter, there’s no visual confirmation - we need to use other commands to verify it worked."
  },
  {
    "objectID": "slides/02-programming-and-processing.html#writing-vs.-prompting",
    "href": "slides/02-programming-and-processing.html#writing-vs.-prompting",
    "title": "Programming & Processing",
    "section": "Writing Vs. Prompting",
    "text": "Writing Vs. Prompting\nWe have two options: - look at the Command Line cheatsheet that contains a number of these commands and see what would be the best option to help us check if this worked."
  },
  {
    "objectID": "slides/02-programming-and-processing.html#writing-vs.-prompting-1",
    "href": "slides/02-programming-and-processing.html#writing-vs.-prompting-1",
    "title": "Programming & Processing",
    "section": "Writing Vs. Prompting",
    "text": "Writing Vs. Prompting\nOur other option is to try and start testing out these AI Chatbots, and specifically GitHub Co-Pilot (though again you are welcome to use any AI tool that is free and helps you understand these materials)."
  },
  {
    "objectID": "slides/02-programming-and-processing.html#what-is-a-prompt",
    "href": "slides/02-programming-and-processing.html#what-is-a-prompt",
    "title": "Programming & Processing",
    "section": "What is a Prompt?",
    "text": "What is a Prompt?\nIn the terminal: the symbol that indicates where you type\n\n$ (bash)\n% (zsh)\n&gt; (PowerShell)\n\n\nIn AI: the text input you give to a chatbot\n\nSo far we have been doing a form of prompting, called Command Prompt, which refers to where we write the text commands in the terminal. Often times the command prompt is a symbol that indicates the start of a new command. For example, in Melanie’s example, the command prompt is the $ symbol. This symbol is called a prompt because it prompts you to enter a command. The prompt is followed by a cursor, which is the blinking vertical line that indicates where your next character will appear. Other symbols you might see include #, %, and &gt;.\nThe term “prompt” has become incredibly popular since the release of GPT-3 by OpenAI in Summer 2020. In the terminal, the prompt is a symbol like $ or % that indicates where your next command will appear. In AI contexts, it refers to the text commands or questions you give to chatbots."
  },
  {
    "objectID": "slides/02-programming-and-processing.html#the-ai-explosion",
    "href": "slides/02-programming-and-processing.html#the-ai-explosion",
    "title": "Programming & Processing",
    "section": "The AI Explosion",
    "text": "The AI Explosion\n\n\n\nEvolution of LLMs\n\n\n\nWhile there idea of prompt engineering has a longer history in natural language processing, the term has taken over, with the proliferation of new AI chatbots and models.\nIn some ways our idea of giving text prompts to the terminal via the command line echoes how we do prompt engineering for AI chatbots, since both require writing text (something we’ll discuss more later in this lesson).\nThe term “prompt engineering” describes giving commands or prompts to chatbots for certain outputs or results. While prompt engineering has a longer history in natural language processing, the term has taken over with the proliferation of new AI chatbots and models since 2020."
  },
  {
    "objectID": "slides/02-programming-and-processing.html#terminal-vs-ai-prompting",
    "href": "slides/02-programming-and-processing.html#terminal-vs-ai-prompting",
    "title": "Programming & Processing",
    "section": "Terminal vs AI Prompting",
    "text": "Terminal vs AI Prompting\n\n\n\nTerminal Commands\nAI Prompts\n\n\n\n\nFixed, specific syntax\nNatural language\n\n\nPredictable outputs\nVariable outputs\n\n\nMust use exact commands\nFlexible phrasing\n\n\nReproducible\nRarely reproducible\n\n\n\n\nBoth require writing text, but depending on your operating system, terminal, and shell, you have to give the command line certain set of commands that have been previously programmed. With AI chatbots, you can write without those set structures - but that doesn’t mean there aren’t helpful guidelines."
  },
  {
    "objectID": "slides/02-programming-and-processing.html#prompting-guidelines",
    "href": "slides/02-programming-and-processing.html#prompting-guidelines",
    "title": "Programming & Processing",
    "section": "Prompting Guidelines?",
    "text": "Prompting Guidelines?\n\nInfographics Abound\nBut while depending on your operating system, terminal, and shell, you have to give the command line certain set of commands that have been previously programmed, with AI chatbots you can write without those set structures.\nHowever, that doesn’t mean there aren’t some helpful guidelines for prompt engineering, even if the process often requires lots of trial and error, and is rarely reproducible or transparent.\nIf you search for tips on prompt engineering, you’ll find lots of infographics like this one:"
  },
  {
    "objectID": "slides/02-programming-and-processing.html#the-4s-of-prompt-engineering",
    "href": "slides/02-programming-and-processing.html#the-4s-of-prompt-engineering",
    "title": "Programming & Processing",
    "section": "The 4S of Prompt Engineering",
    "text": "The 4S of Prompt Engineering\nMicrosoft’s guidelines for GitHub Copilot:\n\n\nSingle - One task or question at a time\n\n\n\n\nSpecific - Detailed requests and specifications\n\n\n\n\nShort - Be concise\n\n\n\n\nSurround - Keep relevant files open for context\n\n\nMicrosoft provides these guidelines specifically for Copilot since it has been trained primarily for coding tasks. The final one is particularly important for Copilot since it works best when it has code examples to learn from - this is called “few-shot learning.”\nWhile these are general guidelines, the final one is particularly important for Co-Pilot, since it works best when it has code examples to learn from (something called few-shot learning, for those that are interested)."
  },
  {
    "objectID": "slides/02-programming-and-processing.html#how-copilot-processes-prompts",
    "href": "slides/02-programming-and-processing.html#how-copilot-processes-prompts",
    "title": "Programming & Processing",
    "section": "How Copilot Processes Prompts",
    "text": "How Copilot Processes Prompts\n\n\n\nCopilot processing flow\n\n\n\nThis diagram details what GitHub Copilot does with your prompt. The key thing to understand is that it looks for patterns in the prompt and tries to match those patterns to code examples it has seen before. So the more specific and detailed your prompt is, the more likely it is to find a match."
  },
  {
    "objectID": "slides/02-programming-and-processing.html#important-warning",
    "href": "slides/02-programming-and-processing.html#important-warning",
    "title": "Programming & Processing",
    "section": "Important Warning",
    "text": "Important Warning\nNever share sensitive information in prompts!\n\n\nPersonal data can be leaked\nCopilot scrapes GitHub repositories\nThe legality and ethics remain hazy\n\n\n\nUse AI tools with a critical lens\n\nEven though Copilot claims to be secure, there’s a lot of potential for personal data to be leaked through this process. The legality of Copilot remains hazy, not to mention the ethics or politics of it. You’ll see why as we continue to work with it, as it autocompletes file names and other information from GitHub that it has scraped."
  },
  {
    "objectID": "slides/02-programming-and-processing.html#example-prompts-for-the-cli",
    "href": "slides/02-programming-and-processing.html#example-prompts-for-the-cli",
    "title": "Programming & Processing",
    "section": "Example Prompts for the CLI",
    "text": "Example Prompts for the CLI\n\n\n\n\n\n\nTry these prompts!\n\n\nHow can I check if my directory was created in my terminal?\nShould I use the command ls or pwd to check if my directory\nwas created? And what is the difference between these two?\nHow can I create a directory named is310-computing-humanities\nin my terminal?\n\n\n\n\nThese are examples of good prompts for getting help with command line tasks. Notice they are specific, ask one thing at a time, and are concise. When you get answers, always verify them using the command line cheatsheet or by testing carefully."
  },
  {
    "objectID": "slides/02-programming-and-processing.html#essential-commands",
    "href": "slides/02-programming-and-processing.html#essential-commands",
    "title": "Programming & Processing",
    "section": "Essential Commands",
    "text": "Essential Commands\n\n\n\nCommand\nWhat it does\n\n\n\n\npwd\nPrint working directory (where am I?)\n\n\nls\nList directory contents\n\n\ncd\nChange directory\n\n\nmkdir\nMake a new directory\n\n\ntouch\nCreate a new file\n\n\nmv\nMove or rename files\n\n\nrmdir\nRemove an empty directory\n\n\n\n\nThese are the fundamental commands you’ll use constantly. pwd shows your current location, ls shows what’s in your current folder, cd moves you around, mkdir creates folders, touch creates files, mv moves things, and rmdir removes empty folders.\nSee the Command Line cheatsheet for more commands and details!"
  },
  {
    "objectID": "slides/02-programming-and-processing.html#making-directories",
    "href": "slides/02-programming-and-processing.html#making-directories",
    "title": "Programming & Processing",
    "section": "Making Directories?",
    "text": "Making Directories?\nmkdir is310-computing-humanities\n\nWith the help of the cheatsheet and GitHub Co-Pilot, we should see that the command ls lists the contents of a directory, and pwd prints the full working path. So we can use these commands to check if our directory was created.\nAnd then we can use the command mkdir to create a directory named is310-computing-humanities.\nUsing these commands, we can see that our directory was created. But what does this mean exactly?\nAs we read this week, how we interact with computers has increasingly made it difficult to understand where our files are stored. While search functionality often takes care of this for us in GUI applications, in the command line we have to be more explicit about where we want to store our files and we have to know where we are located."
  },
  {
    "objectID": "slides/02-programming-and-processing.html#understanding-file-paths",
    "href": "slides/02-programming-and-processing.html#understanding-file-paths",
    "title": "Programming & Processing",
    "section": "Understanding File Paths",
    "text": "Understanding File Paths\n\n\n\nFile tree structure\n\n\n\nIn this figure, we see a representation of how folders and files are organized on a Windows computer. The top level is the root directory, which on Windows is the C: drive. On Macs and Linux machines, we have a similar root directory, but it is represented by a / symbol. This root directory contains all of the files and folders on your computer. The root directory is also called the parent directory because it is the parent of all the other directories on your computer.\nImportant to know: directory is the same as folder, and the two are often used interchangeably. Directory is the original term, but with GUIs we often use the term folder.\nIn this diagram, we can see that there is a Users folder or directory. This is where most of your files that you download or create are stored: under your user account and in the Documents, Downloads, and Desktop folders.\nIf I use the pwd command, I can start to see this structure."
  },
  {
    "objectID": "slides/02-programming-and-processing.html#working-directories",
    "href": "slides/02-programming-and-processing.html#working-directories",
    "title": "Programming & Processing",
    "section": "Working Directories",
    "text": "Working Directories\n\nCurrent Directory\nYou’ll notice the command prints out a long string that has names like Users, followed by a series of names separated by a forward slash / (in PowerShell, you’ll see backslashes \\). Each of these are my folders or directories, and this represents the absolute file path from my root directory to my current directory – that is the current location of my terminal. I realize this might be a bit confusing initially, but eventually as you use the command line more, you’ll start to get a sense of how this works.\nEach file path is unique to your computer, and so if you are following along with this lesson, you’ll see a different file path, though there should be a Users folder as your root or home directory."
  },
  {
    "objectID": "slides/02-programming-and-processing.html#absolute-vs-relative-paths",
    "href": "slides/02-programming-and-processing.html#absolute-vs-relative-paths",
    "title": "Programming & Processing",
    "section": "Absolute vs Relative Paths",
    "text": "Absolute vs Relative Paths\nAbsolute path: Full path from root\n/Users/zleblanc/Documents/is310\n\nRelative path: Path from current location\ncd ~        # Go to home directory\ncd ..       # Go up one directory\ncd ../..    # Go up two directories\n\nYou’ll also notice I did the command cd ~. What do you think this command does? What does cd stand for do you think?\nA core distinction with how I used this final command is the fact that I used a tilde ~ symbol. This symbol is a relative file path, meaning it is relative to my current directory. So if I am in the Documents folder, cd ~ will take me to my home directory. You can also use cd .. to go up one directory, and cd ../.. to go up two directories, and so on.\nAn absolute file path is the complete path from the root directory to a specific location. It’s unique to your computer. A relative path is relative to your current directory. The tilde ~ represents your home directory, and .. means “parent directory” (one level up)."
  },
  {
    "objectID": "slides/02-programming-and-processing.html#putting-it-together",
    "href": "slides/02-programming-and-processing.html#putting-it-together",
    "title": "Programming & Processing",
    "section": "Putting It Together",
    "text": "Putting It Together\n# Create a file\ntouch is310-computing-humanities.txt\n\n# Move it into a folder\nmv is310-computing-humanities.txt is310-computing-humanities\n\n# Change into the folder\ncd is310-computing-humanities\n\n# List contents to verify\nls\n\n# Go back up and remove the folder\ncd ..\nrmdir is310-computing-humanities\n\nThis sequence demonstrates a typical workflow: creating a file, moving it into a directory, navigating into that directory to verify, then cleaning up by going back and removing the directory. Note that rmdir only works on empty directories."
  },
  {
    "objectID": "slides/02-programming-and-processing.html#your-tasks",
    "href": "slides/02-programming-and-processing.html#your-tasks",
    "title": "Programming & Processing",
    "section": "Your Tasks",
    "text": "Your Tasks\nUsing the cheatsheet and/or your preferred AI chatbot:\n\nCreate is310-computing-humanities directory in your Desktop folder\nInside it, create is310-computing-humanities.txt\nAdd text to your file using the command line (new command!)\nDisplay the file contents (new command!)\nReturn to your home directory\n\n\nBonus: Copy your folder, then delete the copy\n\nThis exercise combines what we’ve learned about the command line with using AI tools for help. The new commands you’ll need to look up are for adding text to a file (like echo or cat with redirection) and displaying file contents (like cat). Remember to ask the instructors if you need help!"
  },
  {
    "objectID": "slides/02-programming-and-processing.html#additional-resources",
    "href": "slides/02-programming-and-processing.html#additional-resources",
    "title": "Programming & Processing",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nIntroduction to the Bash Command Line - Programming Historian\nBash Basics Part 1 - Video tutorial\nBeginner’s Guide to the Bash Terminal\nCodeAcademy Command Line Course\n\n\nThese resources provide additional depth on the command line. The Programming Historian tutorial is particularly good for humanities contexts. The videos are helpful if you prefer visual learning. And the CodeAcademy course provides interactive practice."
  },
  {
    "objectID": "slides/02-programming-and-processing.html#questions",
    "href": "slides/02-programming-and-processing.html#questions",
    "title": "Programming & Processing",
    "section": "Questions?",
    "text": "Questions?\nRemember:\n\nThe terminal is the interface\nThe shell interprets commands\nThe command line is where you type\nPrompts work for both CLI and AI!\n\n\nSee also: Command Line Cheatsheet"
  },
  {
    "objectID": "slides/02-programming-and-processing.html#the-problem",
    "href": "slides/02-programming-and-processing.html#the-problem",
    "title": "Programming & Processing",
    "section": "The Problem",
    "text": "The Problem\nHave you ever had files like this?\n\n\nessay_final.docx\nessay_final_v2.docx\nessay_FINAL_FINAL.docx\nessay_FINAL_FINAL_actually_final.docx\n\n\n\nVersion control solves this problem!\n\nSo far, we have been using the command line in the terminal, installed GitHub Co-Pilot and git, but we have yet to use either for their main purpose: version control and collaborative coding. In this lesson, we will explore what version control is and how it works, as well as how to use GitHub to collaborate with others and share our code and datasets.\nVersion control is the practice of tracking and managing changes to files over time. We’ve all experienced the chaos of multiple file versions. Git provides a systematic way to track changes, collaborate, and maintain a clean history of your work."
  },
  {
    "objectID": "slides/02-programming-and-processing.html#what-is-git",
    "href": "slides/02-programming-and-processing.html#what-is-git",
    "title": "Programming & Processing",
    "section": "What is Git?",
    "text": "What is Git?\nGit is software for version control - tracking changes in files over time.\n\n\nCreated in 2005 by Linus Torvalds (creator of Linux)\nMost popular version control system in the world\nUsed by millions of developers and researchers\n\n\nGit requires changes to be saved (committed) before they can be combined (merged) with the main project. You’re already familiar with this concept if you’ve used Google Docs or Word’s “Track Changes” - but with those you get version history automatically. With git, we have to tell it what to track and when to track it.\nCreated in 2005 by Linus Torvalds, the creator of Linux, the open-source operating system that powers much of the internet, Git is now the most popular version control system in the world."
  },
  {
    "objectID": "slides/02-programming-and-processing.html#git-vs-github",
    "href": "slides/02-programming-and-processing.html#git-vs-github",
    "title": "Programming & Processing",
    "section": "Git vs GitHub",
    "text": "Git vs GitHub\n\n\n\n\n\n\n\n\n\nGit\nGitHub\n\n\n\n\nWhat\nVersion control system\nPlatform for hosting git repositories\n\n\nWhere\nLocal (on your computer)\nRemote (website)\n\n\nInstall?\nYes\nNo (it’s a website)\n\n\n\n\nThey are two separate technologies often used together!\n\nA very important thing to understand is that while git and GitHub sound the same, they are two separate technologies. Git is a version control system, while GitHub is a platform for hosting git repositories. In git, a folder is called a repository. We’ll be using both in this course, but it’s important to realize they are not identical."
  },
  {
    "objectID": "slides/02-programming-and-processing.html#git-github-together",
    "href": "slides/02-programming-and-processing.html#git-github-together",
    "title": "Programming & Processing",
    "section": "Git + GitHub Together",
    "text": "Git + GitHub Together\n\n\n\nGit and GitHub workflow\n\n\n\nEven though these two technologies are separate, they are often used together. You can use git to track changes in your files locally on your computer, and then push those changes to a remote repository on GitHub. This is what we will be doing in this course."
  },
  {
    "objectID": "slides/02-programming-and-processing.html#why-git-for-culture-as-data",
    "href": "slides/02-programming-and-processing.html#why-git-for-culture-as-data",
    "title": "Programming & Processing",
    "section": "Why Git for Culture as Data?",
    "text": "Why Git for Culture as Data?\nGit allows you to:\n\nKeep track of changes in plain text files\nCollaborate in real-time with other researchers\nEasily revert to earlier versions\nCreate different branches to explore new ideas\nSee who added what to the project\n\n\nAlthough Git and version control systems are predominantly used in software development, they offer several advantages for scholars in Digital Humanities. Similar to how you might handle versions of a paper, essay, or dataset, Git provides systematic version tracking."
  },
  {
    "objectID": "slides/02-programming-and-processing.html#what-version-tracking-looks-like",
    "href": "slides/02-programming-and-processing.html#what-version-tracking-looks-like",
    "title": "Programming & Processing",
    "section": "What Version Tracking Looks Like",
    "text": "What Version Tracking Looks Like\n\n\n\nExample of a git diff\n\n\n\nThis screenshot is from GitHub but shows what git tracks. You can see on the left, the previous erroneous line was deleted, and on the right is the corrected version. This is a very simple example, but it gives you a sense of how git tracks changes - showing exactly what was removed and what was added."
  },
  {
    "objectID": "slides/02-programming-and-processing.html#check-your-installation",
    "href": "slides/02-programming-and-processing.html#check-your-installation",
    "title": "Programming & Processing",
    "section": "Check Your Installation",
    "text": "Check Your Installation\nFirst, verify git is installed:\ngit --version\n\nYou should see something like:\ngit version 2.39.0\n\nRemember that we run our commands in the terminal! If you don’t see a version number, you may need to install git. Let the instructors know if you’re having issues."
  },
  {
    "objectID": "slides/02-programming-and-processing.html#create-a-project-folder",
    "href": "slides/02-programming-and-processing.html#create-a-project-folder",
    "title": "Programming & Processing",
    "section": "Create a Project Folder",
    "text": "Create a Project Folder\n# Create a new folder\nmkdir is310-first-repo\n\n# Navigate into it\ncd is310-first-repo\n\n# Create a file\ntouch first_file.txt # For Unix/Linux\nNew-Item -ItemType File -Path . -Name first_file.txt # For PowerShell\n\nAdd some text to the file:\necho \"This is my first file\" &gt; first_file.txt # For Unix/Linux\n\n\"This is my first file\" | Out-File -FilePath first_file.txt #For PowerShell\n\nLet’s create a new folder for our project. We use mkdir to create the folder, cd to navigate into it, and touch to create an empty file. Then we use echo with the redirect operator &gt; to add text to the file."
  },
  {
    "objectID": "slides/02-programming-and-processing.html#initialize-a-git-repository",
    "href": "slides/02-programming-and-processing.html#initialize-a-git-repository",
    "title": "Programming & Processing",
    "section": "Initialize a Git Repository",
    "text": "Initialize a Git Repository\ngit init\n\nOutput:\nInitialized empty Git repository in /Users/YOU/is310-first-repo/.git/\n\n\nA hidden .git folder is created. We can use ls -a for Unix/Linux or Get-ChildItem -Force for PowerShell to see it.\n\nTo start tracking changes, we need to initialize a git repository. The git init command creates a hidden .git folder that contains all the information about our repository. The .git folder is hidden by default, so we need to use the -a or -Force flag to see it. This means that if you try looking at your folder either in a file explorer or in the terminal with just ls, or even in the VS Code Explorer, you won’t see the .git folder."
  },
  {
    "objectID": "slides/02-programming-and-processing.html#the-git-workflow",
    "href": "slides/02-programming-and-processing.html#the-git-workflow",
    "title": "Programming & Processing",
    "section": "The Git Workflow",
    "text": "The Git Workflow\n\n\n\nGit as a camera\n\n\nThink of git as a camera taking snapshots of your project\n\nThinking of git as a camera taking a photo of your project at a particular point in time is helpful. Every time you commit your changes, you are taking a snapshot. You can then go back to that snapshot at any time. Like save points in a video game!"
  },
  {
    "objectID": "slides/02-programming-and-processing.html#step-1-add-to-staging-area",
    "href": "slides/02-programming-and-processing.html#step-1-add-to-staging-area",
    "title": "Programming & Processing",
    "section": "Step 1: Add to Staging Area",
    "text": "Step 1: Add to Staging Area\nGit doesn’t track automatically - you must tell it what to track:\ngit add first_file.txt\n\nCheck the status:\ngit status\nOn branch main\n\nNo commits yet\n\nChanges to be committed:\n    new file:   first_file.txt\n\nAfter initializing a repository, tracking is NOT automatic. We have to tell git which files to pay attention to by adding them to the staging area. The staging area is like selecting which photos you want to include in an album before actually printing them."
  },
  {
    "objectID": "slides/02-programming-and-processing.html#step-2-commit-changes",
    "href": "slides/02-programming-and-processing.html#step-2-commit-changes",
    "title": "Programming & Processing",
    "section": "Step 2: Commit Changes",
    "text": "Step 2: Commit Changes\nFinalize your snapshot with a descriptive message:\ngit commit -m \"Added first file to our repository\"\n\nCheck status again:\ngit status\nOn branch main\nnothing to commit, working tree clean\n\nCommitting is like actually taking the photo. The -m flag lets you add a message describing what changed. Your commit messages should be short and descriptive - they help you (and collaborators) understand the history of your project."
  },
  {
    "objectID": "slides/02-programming-and-processing.html#if-you-forget-the--m-flag",
    "href": "slides/02-programming-and-processing.html#if-you-forget-the--m-flag",
    "title": "Programming & Processing",
    "section": "If You Forget the -m Flag…",
    "text": "If You Forget the -m Flag…\nYou’ll see Vim (a text editor):\n\n\n\nVim editor\n\n\n\nIdeally, your commit messages should be short and descriptive. They should describe the changes you made to the file.\nIf you don’t add a message, git will automatically open a text editor in your terminal and ask you to add a message. You’ll likely see something that looks like the following.\nThis looks very intimidating, but it’s actually not that bad. This is the default text editor (think like VS Code, but much simpler) for git, which is called Vim. Vim is a very powerful text editor, but it has a steep learning curve. If you want to learn more about Vim, you can read this https://www.freecodecamp.org/news/vim-beginners-guide/."
  },
  {
    "objectID": "slides/02-programming-and-processing.html#if-you-forget-the--m-flag-1",
    "href": "slides/02-programming-and-processing.html#if-you-forget-the--m-flag-1",
    "title": "Programming & Processing",
    "section": "If You Forget the -m Flag…",
    "text": "If You Forget the -m Flag…\n\nPress i to type, then esc, then :wq to save and exit\nOr press esc, then :q! to exit without saving\n\nAborting commit due to empty commit message.\n\nThe key thing to know is that if you get this screen you can either add a message or exit the text editor. To add a message, you need to press i on your keyboard. This will put you in insert mode, which means you can type. Then you can add your message. Once you have added your message, you need to press esc on your keyboard. This will take you out of insert mode. Then you need to type :wq and press enter. This will save your message and exit the text editor.\nIf you just want to exit the text editor, you can press esc on your keyboard. Then you need to type :q! and press enter. This will exit the text editor without saving your message. In which case, you’ll see the following message:"
  },
  {
    "objectID": "slides/02-programming-and-processing.html#quotes",
    "href": "slides/02-programming-and-processing.html#quotes",
    "title": "Programming & Processing",
    "section": "Quotes",
    "text": "Quotes\nquote&gt;\n\nWhile doing this once or twice is fine, it can get annoying so it’s better to add a message to your commit with the -m flag. Be sure that you use quotations marks around your message, and that you use the same type of quotations marks. So if you use double quotes, use double quotes around your message. If you use single quotes, use single quotes around your message. If for some reason you end up mixing them, you’ll get a quote prompt in your terminal. This is just git telling you that you didn’t close your quotation marks, so if you add the correct quotation marks and press enter, it will add your message."
  },
  {
    "objectID": "slides/02-programming-and-processing.html#step-3-view-history",
    "href": "slides/02-programming-and-processing.html#step-3-view-history",
    "title": "Programming & Processing",
    "section": "Step 3: View History",
    "text": "Step 3: View History\nSee your commit history:\ngit log\ncommit 8bb8306c1392eed52d4407eb16867a49b49a46ac (HEAD -&gt; main)\nAuthor: Your Name &lt;your-email@gmail.com&gt;\nDate:   Tue Jan 22 16:03:39 2024 -0400\n\n    Added first file to our repository\n\nNow that we have committed our changes, we can view the version history of our file. To do this, we use the git log command. This will show us the commit history of our file. We can see the commit message, the author, the date, and the commit hash. The commit hash is a unique identifier for each commit. It is a long string of letters and numbers that is used to identify each commit. We can use this commit hash to revert to a previous version of our file.\nThis is telling us that we have one commit, the commit is identified with that long string of letters and numbers (this is the commit hash), the commit is on the main branch. The Author and Date should correspond to your information and when you made the commit. Finally, we see our commit message.\nWe could now update the file and commit our changes again. If we did that, then our version history via git log would show the two commits.\nThese activities are the core features of git. The next step is to connect our local repository to a remote repository on GitHub."
  },
  {
    "objectID": "slides/02-programming-and-processing.html#git-commands-summary",
    "href": "slides/02-programming-and-processing.html#git-commands-summary",
    "title": "Programming & Processing",
    "section": "Git Commands Summary",
    "text": "Git Commands Summary\n\n\n\nCommand\nWhat it does\n\n\n\n\ngit init\nInitialize a repository\n\n\ngit add &lt;file&gt;\nAdd file to staging area\n\n\ngit commit -m \"msg\"\nCommit with message\n\n\ngit status\nCheck current status\n\n\ngit log\nView commit history\n\n\n\n\nThese are the core local git commands. You’ll use these constantly: init to start, add to stage, commit to save snapshots, status to check where you are, and log to see history."
  },
  {
    "objectID": "slides/02-programming-and-processing.html#creating-a-repository-on-github",
    "href": "slides/02-programming-and-processing.html#creating-a-repository-on-github",
    "title": "Programming & Processing",
    "section": "Creating a Repository on GitHub",
    "text": "Creating a Repository on GitHub\n\nGo to github.com\nClick the New button\n\n\n\n\nCreate repo button\n\n\n\nTo get our local git repository onto GitHub, we first need to create a new repository on the website. Click the New button in the top right corner of GitHub."
  },
  {
    "objectID": "slides/02-programming-and-processing.html#repository-settings",
    "href": "slides/02-programming-and-processing.html#repository-settings",
    "title": "Programming & Processing",
    "section": "Repository Settings",
    "text": "Repository Settings"
  },
  {
    "objectID": "slides/02-programming-and-processing.html#repository-settings-1",
    "href": "slides/02-programming-and-processing.html#repository-settings-1",
    "title": "Programming & Processing",
    "section": "Repository Settings",
    "text": "Repository Settings\n\nName it is310-first-repo\nChoose Public so we can see each other’s work\nLeave README unchecked for now\n\n\nGive your repository a name, add a description if you want, and choose whether it’s public or private. For class purposes, choose public so we can see each other’s repositories. Leave the README option unchecked since we’ll push our existing local repo.\nYou also have the option to initialize the repository with a README, which is a file that provides information about the project. We will be discussing this more later so for now leave this option unchecked. You can also add a .gitignore file, which is a file that tells Git to ignore certain files in the repository. Again, something we’ll discuss more in-depth in the coming weeks."
  },
  {
    "objectID": "slides/02-programming-and-processing.html#repository-settings-2",
    "href": "slides/02-programming-and-processing.html#repository-settings-2",
    "title": "Programming & Processing",
    "section": "Repository Settings",
    "text": "Repository Settings\n\nFirst Commit\nOnce you create your repository it should like the photo above. If you selected the Initialize this repository with a README option, you will see a blank repository that looks like the photo below."
  },
  {
    "objectID": "slides/02-programming-and-processing.html#repository-settings-3",
    "href": "slides/02-programming-and-processing.html#repository-settings-3",
    "title": "Programming & Processing",
    "section": "Repository Settings",
    "text": "Repository Settings\n\nFirst Commit With READMETo understand what each button does feel free to browse through our advanced git and GitHub resource, but first let’s try to get our local repository onto GitHub."
  },
  {
    "objectID": "slides/02-programming-and-processing.html#connect-local-to-remote",
    "href": "slides/02-programming-and-processing.html#connect-local-to-remote",
    "title": "Programming & Processing",
    "section": "Connect Local to Remote",
    "text": "Connect Local to Remote\nAdd your GitHub repo as a “remote”:\ngit remote add origin https://github.com/USERNAME/is310-first-repo.git\n\nNow that we have created a new repository on GitHub, we need to connect it to our local repository. To do this, we need to add a remote repository. We can do this by using the git remote add command and then the name of the remote repository and the URL of the remote repository. Let’s call our remote repository origin and use the URL of our GitHub repository."
  },
  {
    "objectID": "slides/02-programming-and-processing.html#connect-local-to-remote-1",
    "href": "slides/02-programming-and-processing.html#connect-local-to-remote-1",
    "title": "Programming & Processing",
    "section": "Connect Local to Remote",
    "text": "Connect Local to Remote\nVerify it worked:\ngit remote -v\norigin  https://github.com/username/repository.git (fetch)\norigin  https://github.com/username/repository.git (push)\n\nIn this case, we would replace OWNER with our GitHub username and REPOSITORY with the name of our repository. So for me, it would be ZoeLeBlanc/is310-first-repo.\nWe can check this by using the git remote -v command. This will show us the remote repositories we have added to our local repository."
  },
  {
    "objectID": "slides/02-programming-and-processing.html#push-to-github",
    "href": "slides/02-programming-and-processing.html#push-to-github",
    "title": "Programming & Processing",
    "section": "Push to GitHub",
    "text": "Push to GitHub\ngit push [remote repository] [flags] [local repository]\n\nSo all we need to do is specify the name of our remote repository and the name of our local repository. What’s a bit confusing is that we don’t need to write our GitHub URL here but instead can just write origin. If you look at the output from git remote -v again, you’ll notice it says origin next to our GitHub URL. This is because origin is the default name for our remote repository. We can change this, but for now we’ll just use origin.\nThen for our local repository, we don’t need to say our directory name but instead need to specify the name of the branch we want to push. We’ll discuss branches more later, but for now we can just use main. So our command will look like this:"
  },
  {
    "objectID": "slides/02-programming-and-processing.html#push-to-github-1",
    "href": "slides/02-programming-and-processing.html#push-to-github-1",
    "title": "Programming & Processing",
    "section": "Push to GitHub",
    "text": "Push to GitHub\nSend your local commits to GitHub:\ngit push origin main\nEnumerating objects: 3, done.\nCounting objects: 100% (3/3), done.\nWriting objects: 100% (3/3), 226 bytes | 226.00 KiB/s, done.\nTo github.com:username/is310-first-repo.git\n * [new branch]      main -&gt; main\n\nThe push command sends your local commits to the remote repository. “origin” is the name we gave our remote, and “main” is the branch we’re pushing. After this, you’ll see your files on GitHub!\n\nFatal Errors?\nIf you see a fatal error that looks like this:\nfatal: 'origin' does not appear to be a git repository\nfatal: Could not read from remote repository.\n\nPlease make sure you have the correct access rights\nand the repository exists.\n\nThat just means you forgot to add your remote repository. If you fix that and try again, it should work.\nNow if we go to our GitHub repository, we will see that our local repository has been pushed to our remote repository."
  },
  {
    "objectID": "slides/02-programming-and-processing.html#ssh-keys-optional-but-recommended",
    "href": "slides/02-programming-and-processing.html#ssh-keys-optional-but-recommended",
    "title": "Programming & Processing",
    "section": "SSH Keys (Optional but Recommended)",
    "text": "SSH Keys (Optional but Recommended)\nTired of entering your username/password?\n\nSSH keys let you connect securely without credentials.\n\n\nYou can find instructions on how to do it here"
  },
  {
    "objectID": "slides/02-programming-and-processing.html#create-files-in-the-browser",
    "href": "slides/02-programming-and-processing.html#create-files-in-the-browser",
    "title": "Programming & Processing",
    "section": "Create Files in the Browser",
    "text": "Create Files in the Browser\nClick Add file &gt; Create new file\n\n\n\nAdd file button\n\n\n\nSo far, we have been editing our files locally and pushing them up to GitHub. However, we can also edit files in GitHub. This is a great way to make quick changes to a file or add a new file to a repository. Let’s try adding using the GitHub interface in the browser and add a special type of file called a README.md file."
  },
  {
    "objectID": "slides/02-programming-and-processing.html#create-a-readme.md",
    "href": "slides/02-programming-and-processing.html#create-a-readme.md",
    "title": "Programming & Processing",
    "section": "Create a README.md",
    "text": "Create a README.md"
  },
  {
    "objectID": "slides/02-programming-and-processing.html#create-a-readme.md-1",
    "href": "slides/02-programming-and-processing.html#create-a-readme.md-1",
    "title": "Programming & Processing",
    "section": "Create a README.md",
    "text": "Create a README.md\nName your file README.md and add:\n# IS 310 Test Repository\n\nThis is my first repository.\n\nLet’s create a README file - the .md extension means Markdown, which is a way to format text. README files are special - GitHub displays them automatically on your repository’s main page. They typically describe what the project is about."
  },
  {
    "objectID": "slides/02-programming-and-processing.html#create-a-readme.md-2",
    "href": "slides/02-programming-and-processing.html#create-a-readme.md-2",
    "title": "Programming & Processing",
    "section": "Create a README.md",
    "text": "Create a README.md\n\n\nClick Preview to see formatted output\nClick Commit changes\nAdd a commit message\nSelect main branch\n\n\nNow we need to commit our changes and file by clicking the green Commit changes box. This will prompt us for a commit message. Let’s add the following message Finally, we need to tell GitHub whether to create a new branch or commit to the main branch. Click main for now and then click Commit changes. Now we have added a new file to our repository and you should see it when you navigate to your repository."
  },
  {
    "objectID": "slides/02-programming-and-processing.html#github-dev-bonus-feature",
    "href": "slides/02-programming-and-processing.html#github-dev-bonus-feature",
    "title": "Programming & Processing",
    "section": "GitHub Dev (Bonus Feature)",
    "text": "GitHub Dev (Bonus Feature)\nChange github.com to github.dev in your URL. So for us, we would change https://github.com/[USER NAME]/is310-first-repo to https://github.dev/[USER NAME]/is310-first-repo.\n\nGitHub Dev"
  },
  {
    "objectID": "slides/02-programming-and-processing.html#github-dev-bonus-feature-1",
    "href": "slides/02-programming-and-processing.html#github-dev-bonus-feature-1",
    "title": "Programming & Processing",
    "section": "GitHub Dev (Bonus Feature)",
    "text": "GitHub Dev (Bonus Feature)\nOr press . on any repository\n\n\n\n\nGitHub Dev interface\n\n\nIt’s VS Code in your browser! You can read more about it https://docs.github.com/en/codespaces/the-githubdev-web-based-editor.\n\nGitHub Dev is a new feature that gives you a VS Code-like editor right in the browser. Just change .com to .dev in the URL or press the period key. You can edit files and commit changes through this interface."
  },
  {
    "objectID": "slides/02-programming-and-processing.html#the-problem-1",
    "href": "slides/02-programming-and-processing.html#the-problem-1",
    "title": "Programming & Processing",
    "section": "The Problem",
    "text": "The Problem\nNow we have two versions:\n\nLocal repository (on your computer)\nRemote repository (on GitHub)\n\n\nThey have different files! How do we sync them?\n\nAfter creating a file on GitHub, our local and remote repositories are out of sync. The remote has a README that our local doesn’t have. We need to pull those changes down."
  },
  {
    "objectID": "slides/02-programming-and-processing.html#pull-from-github",
    "href": "slides/02-programming-and-processing.html#pull-from-github",
    "title": "Programming & Processing",
    "section": "Pull from GitHub",
    "text": "Pull from GitHub\ngit pull origin main\nremote: Enumerating objects: 4, done.\nremote: Counting objects: 100% (4/4), done.\nremote: Compressing objects: 100% (2/2), done.\nremote: Total 3 (delta 0), reused 0 (delta 0), pack-reused 0\nUnpacking objects: 100% (3/3), 935 bytes | 233.00 KiB/s, done.\nFrom github.com:ZoeLeBlanc/is310-first-repo\n * branch            main       -&gt; FETCH_HEAD\n   d8dad7b..832b673  main       -&gt; origin/main\nUpdating d8dad7b..832b673\nFast-forward\n README.md | 1 +\n 1 file changed, 1 insertion(+)\n create mode 100644 README.md\n\nNow ls shows the README.md file locally!\n\nThe git pull command downloads changes from GitHub to your local repository. After pulling, you’ll see the README.md file that was created on GitHub is now on your local computer. We’ve closed the loop!"
  },
  {
    "objectID": "slides/02-programming-and-processing.html#the-complete-workflow",
    "href": "slides/02-programming-and-processing.html#the-complete-workflow",
    "title": "Programming & Processing",
    "section": "The Complete Workflow",
    "text": "The Complete Workflow\n\n\n\nGit architecture\n\n\n\nThis diagram shows the complete workflow. Your working directory is where you edit files. You add changes to the staging area, commit them to your local repository, then push to the remote (GitHub). And you pull from GitHub to get others’ changes."
  },
  {
    "objectID": "slides/02-programming-and-processing.html#core-workflow-summary",
    "href": "slides/02-programming-and-processing.html#core-workflow-summary",
    "title": "Programming & Processing",
    "section": "Core Workflow Summary",
    "text": "Core Workflow Summary\n\nEdit files locally\nAdd to staging: git add &lt;file&gt;\nCommit snapshot: git commit -m \"message\"\nPush to GitHub: git push origin main\nPull from GitHub: git pull origin main\n\n\nThis is the core git/GitHub workflow you’ll use constantly. Edit locally, add, commit, push. When working with others or after editing on GitHub, pull first to get the latest changes."
  },
  {
    "objectID": "slides/02-programming-and-processing.html#your-assignment",
    "href": "slides/02-programming-and-processing.html#your-assignment",
    "title": "Programming & Processing",
    "section": "Your Assignment",
    "text": "Your Assignment\n\nCreate a new GitHub repository called is310-coding-assignments.\nCreate a new directory in your local computer called is310-coding-assignments and enable it as a git repository.\nCreate a Markdown file called README.md within is310-coding-assignments.\nCreate a folder called images within is310-coding-assignments.\n\n\nFor homework, you’ll practice this entire workflow. Create a repository for your coding assignments, add proof that you’ve installed all the required course tools, and push it to GitHub. Post the link in our discussion forum when done."
  },
  {
    "objectID": "slides/02-programming-and-processing.html#readme-template",
    "href": "slides/02-programming-and-processing.html#readme-template",
    "title": "Programming & Processing",
    "section": "README Template",
    "text": "README Template\n# Init IS310 Homework\n\n## Proof of Installation\n\n1. Python\n\n![Python Installation](images/python-installation.png)\n\n2. Git\n\n![Git Installation](images/git-installation.png)\n\n3. VS Code\n\n![VS Code Installation](images/vscode-installation.png)\nInclude your Hypothesis username!\n\nUse this template for your README. Take screenshots of each tool running on your computer, save them in the images folder, and reference them in your markdown file. Don’t forget to include your Hypothesis username for the annotation tool."
  },
  {
    "objectID": "slides/02-programming-and-processing.html#pushing-your-homework",
    "href": "slides/02-programming-and-processing.html#pushing-your-homework",
    "title": "Programming & Processing",
    "section": "Pushing Your Homework",
    "text": "Pushing Your Homework\ngit add .\ngit commit -m \"Init IS310 Homework\"\ngit push origin main\n\nRemember to connect local to remote first:\ngit remote add origin https://github.com/USERNAME/is310-coding-assignments.git\n\n\nPost your repo link in the discussion forum!\n\nUse git add . (with the period) to add all files at once. Commit with a descriptive message, then push. Make sure you’ve connected your local and remote repositories first! When done, share your repository link in the class discussion forum."
  },
  {
    "objectID": "slides/02-programming-and-processing.html#resources",
    "href": "slides/02-programming-and-processing.html#resources",
    "title": "Programming & Processing",
    "section": "Resources",
    "text": "Resources\n\nAdvanced Git & GitHub Resource\nGit Commands Cheat Sheet\nGitHub Docs: Managing Remote Repositories\nGitHub Docs: SSH Keys"
  },
  {
    "objectID": "slides/02-programming-and-processing.html#questions-1",
    "href": "slides/02-programming-and-processing.html#questions-1",
    "title": "Programming & Processing",
    "section": "Questions?",
    "text": "Questions?\nKey takeaways:\n\nGit = local version control\nGitHub = remote hosting platform\nWorkflow: edit → add → commit → push/pull\nCommit messages should be descriptive!\n\n\nSee also: Advanced Git & GitHub"
  },
  {
    "objectID": "assets/files/is310IntroLLMS.html",
    "href": "assets/files/is310IntroLLMS.html",
    "title": "Introduction to LLMs Notebook",
    "section": "",
    "text": "To use this notebook, you will need to install the following packages:"
  },
  {
    "objectID": "assets/files/is310IntroLLMS.html#install-and-import-libraries",
    "href": "assets/files/is310IntroLLMS.html#install-and-import-libraries",
    "title": "Introduction to LLMs Notebook",
    "section": "Install and Import Libraries",
    "text": "Install and Import Libraries\n\n# Importing libraries\nfrom rich.console import Console\nimport pandas as pd\nfrom tqdm import tqdm\nimport altair as alt\nimport ollama\nimport ast\nimport pandas as pd\nimport time\nimport json\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nimport re\nimport warnings\nwarnings.filterwarnings('ignore')\n\nconsole = Console()"
  },
  {
    "objectID": "assets/files/is310IntroLLMS.html#load-datasets",
    "href": "assets/files/is310IntroLLMS.html#load-datasets",
    "title": "Introduction to LLMs Notebook",
    "section": "Load Datasets",
    "text": "Load Datasets\nYou either have the option of using the premade dataset available in Google Drive (though you will need to change the path to the file) or running the code below to remake the dataset from scratch.\nBe warned, this file is quite large because of the size of the novels, so you may want to use a subset of the novels to test this code.\n\nGoogle Drive Dataset\nYou can download this dataset here https://drive.google.com/file/d/1LkaRtYph_lWtMPRyzZpECuzEMD3WPx26/view?usp=sharing and it’s very larger so make sure you don’t push it up to GitHub.\n\ncombined_novels_nyt_df = pd.read_csv(\"combined_novels_nyt_with_text.csv\")\n\n\ncombined_novels_nyt_genre_df = combined_novels_nyt_df[(combined_novels_nyt_df.cleaned_pg_eng_text.notna()) & (combined_novels_nyt_df.genre != \"na\")][['top_500_rank', 'title', 'author', 'pub_year', 'orig_lang', 'genre',\n       'author_birth', 'author_death', 'author_gender', 'author_primary_lang',\n       'author_nationality', 'author_field_of_activity', 'author_occupation',\n       'oclc_holdings', 'oclc_eholdings', 'oclc_total_editions',\n       'oclc_holdings_rank', 'oclc_editions_rank', 'gr_avg_rating',\n       'gr_num_ratings', 'gr_num_reviews', 'gr_avg_rating_rank',\n       'gr_num_ratings_rank', 'oclc_owi', 'author_viaf', 'gr_url', 'wiki_url',\n       'pg_eng_url', 'pg_orig_url', 'year', 'week', 'rank', 'title_id',\n       'nyt_title', 'pub_year_date', 'pub_date','pg_eng_text_len',\n       'pg_orig_text_len', 'pg_eng_token_len', 'pg_orig_token_len',\n       'cleaned_pg_eng_text', 'cleaned_pg_orig_text']]\n\ncombined_novels_nyt_genre_df['cleaned_pg_eng_text_len'] = combined_novels_nyt_genre_df['cleaned_pg_eng_text'].str.len()\ncombined_novels_nyt_genre_df['cleaned_orig_eng_text_len'] = combined_novels_nyt_genre_df['cleaned_pg_orig_text'].str.len()\n\n\ndef clean_text(text):\n    \"\"\"\n    Cleans the input text by removing unnecessary metadata, special characters, and image references.\n\n    Parameters\n    ----------\n    text : str\n        The raw text string to clean.\n\n    Returns\n    -------\n    str\n        A cleaned version of the input text.\n    \"\"\"\n    # Decode bytes to string if text is in bytes format\n    if isinstance(text, bytes):\n        text = text.decode('utf-8', errors='ignore')\n    \n    # Remove image references like \"bookcover.jpg\" or \"p003.jpg (307K)\"\n    text = re.sub(r'\\b\\w+\\.jpg\\b\\s*\\(\\d+K\\)', '', text)\n    text = re.sub(r'\\b\\w+\\.jpg\\b', '', text)\n\n    # Remove sections like \"Full Size\" which appear frequently\n    text = re.sub(r'\\bFull Size\\b', '', text, flags=re.IGNORECASE)\n    \n    # Remove phrases indicating editor notes or eBook-related content\n    text = re.sub(r'(Ebook Editor’s Note|Project Gutenberg edition)', '', text, flags=re.IGNORECASE)\n    \n    # Remove special character sequences (e.g., \\xe2\\x80\\x9c, etc.)\n    text = re.sub(r'\\\\x[0-9A-Fa-f]{2}', '', text)\n    text = re.sub(r'\\\\[a-zA-Z]', '', text)  # Remove any leftover backslashes\n    \n    # Remove extra newline characters and unnecessary whitespace\n    text = re.sub(r'\\n+', '\\n', text).strip()  # Reduce multiple newlines to one\n    text = re.sub(r'\\s{2,}', ' ', text)  # Replace multiple spaces with a single space\n\n    # Optional: Remove all caps headings if needed, which might signify metadata (like CHAPTER TITLES)\n    text = re.sub(r'^[A-Z\\s]{3,}$', '', text, flags=re.MULTILINE)\n\n    return text\n\ntqdm.pandas(desc=\"Cleaning Text\")\n\ncombined_novels_nyt_genre_df.cleaned_pg_eng_text = combined_novels_nyt_genre_df.cleaned_pg_eng_text.fillna('')\ncombined_novels_nyt_genre_df.cleaned_pg_orig_text = combined_novels_nyt_genre_df.cleaned_pg_orig_text.fillna('')\ncombined_novels_nyt_genre_df['full_cleaned_pg_eng_text'] = combined_novels_nyt_genre_df.cleaned_pg_eng_text.progress_apply(clean_text)\ncombined_novels_nyt_genre_df['full_cleaned_pg_orig_text'] = combined_novels_nyt_genre_df.cleaned_pg_orig_text.progress_apply(clean_text)\n\nCleaning Text: 100%|██████████| 87/87 [00:11&lt;00:00,  7.74it/s]\nCleaning Text: 100%|██████████| 87/87 [00:02&lt;00:00, 31.01it/s]\n\n\n\ncombined_novels_nyt_genre_df.genre.value_counts()\n\nhistory       24\nromance       12\naction        10\nbildung       10\nfantasy        7\npolitical      7\nscifi          5\nmystery        4\nallegories     3\nautobio        3\nhorror         2\nName: genre, dtype: int64\n\n\n\nvectorizer = TfidfVectorizer(stop_words=\"english\", min_df=1, max_df=0.7,)\n\n# Fit and transform the text data\ntfidf_matrix = vectorizer.fit_transform(combined_novels_nyt_genre_df.full_cleaned_pg_eng_text.fillna(''))\n\n# Convert the TF-IDF matrix to a DataFrame for better readability\ntfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n\n# Add the titles to the DataFrame\ntfidf_df['title'] = combined_novels_nyt_genre_df['title'].values\n\n# Melt the DataFrame to get a long format DataFrame with terms and scores\nmelted_tfidf_df = tfidf_df.melt(id_vars=['title'], var_name='term', value_name='score')\n\n# Sort the DataFrame by score in descending order\nsorted_tfidf_no_stopwords_min_max_df = melted_tfidf_df.sort_values(by='score', ascending=False)\n\n# Display the top 10 results\nsorted_tfidf_no_stopwords_min_max_df.head(10)\n\n\n\n\n\n\n\n\ntitle\nterm\nscore\n\n\n\n\n15942353\nThe Jungle\njurgis\n0.949413\n\n\n1660887\nThis Side of Paradise\namory\n0.938376\n\n\n1295317\nThrough the Looking-Glass, and What Alice Foun...\nalice\n0.919699\n\n\n1295257\nAlice's Adventures in Wonderland\nalice\n0.911474\n\n\n22207592\nOf Human Bondage\nphilip\n0.895389\n\n\n10050016\nWhite Fang\nfang\n0.878559\n\n\n31973533\nThe Secret Agent\nverloc\n0.878165\n\n\n6298634\nThe Pilgrim's Progress\nchr\n0.867819\n\n\n15925817\nThe Red & the Black\njulien\n0.857932\n\n\n1253008\nKidnapped: The Adventures of David Balfour\nalan\n0.834603\n\n\n\n\n\n\n\n\n\n# Group the text data by genre and join the texts within each genre\ngrouped_texts = combined_novels_nyt_genre_df.groupby('genre')['full_cleaned_pg_eng_text'].apply(lambda texts: ' '.join(texts)).reset_index()\n\n# Initialize the TfidfVectorizer\nvectorizer = TfidfVectorizer(stop_words=\"english\", min_df=1, max_df=0.7)\n\n# Fit and transform the text data\ntfidf_matrix = vectorizer.fit_transform(grouped_texts['full_cleaned_pg_eng_text'].fillna(''))\n\n# Convert the TF-IDF matrix to a DataFrame for better readability\ntfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n\n# Add the genres to the DataFrame\ntfidf_df['genre'] = grouped_texts['genre'].values\n\n# Melt the DataFrame to get a long format DataFrame with terms and scores\nmelted_tfidf_df = tfidf_df.melt(id_vars=['genre'], var_name='term', value_name='score')\n\n# Sort the DataFrame by score in descending order\nsorted_tfidf_no_stopwords_min_max_df = melted_tfidf_df.sort_values(by='score', ascending=False)\n\n# Display the top 10 results\nsorted_tfidf_no_stopwords_min_max_df.head(10)\n\n\n\n\n\n\n\n\ngenre\nterm\nscore\n\n\n\n\n3403765\nautobio\nswann\n0.688831\n\n\n1469898\nallegories\ngrandet\n0.628138\n\n\n160439\nfantasy\nalice\n0.611279\n\n\n2712973\nscifi\npencroft\n0.597450\n\n\n780566\nhorror\nchristine\n0.594813\n\n\n3043260\naction\nsancho\n0.552047\n\n\n2481151\nautobio\nodette\n0.505621\n\n\n2879536\naction\nquixote\n0.493032\n\n\n1962606\npolitical\njurgis\n0.480073\n\n\n4045793\nfantasy\nwendy\n0.450921\n\n\n\n\n\n\n\n\n# Group by genre and get the top 10 terms for each genre\ntop_terms_by_genre = sorted_tfidf_no_stopwords_min_max_df.groupby('genre').head(10)\n\n# Print the results\nfor genre, group in top_terms_by_genre.groupby('genre'):\n    console.print(f\"In our dataset we have this many novels {len(combined_novels_nyt_genre_df[combined_novels_nyt_genre_df.genre == genre])} for genre: {genre}\", style=\"bright_magenta\")\n    console.print(f\"Top 10 terms for genre: {genre}\", style=\"bright_magenta\")\n    console.print(group[['term', 'score']])\n    console.print(\"\\n\")\n\nIn our dataset we have this many novels 10 for genre: action\n\n\n\nTop 10 terms for genre: action\n\n\n\n                 term     score\n3043260        sancho  0.552047\n2879536       quixote  0.493032\n1978977           kim  0.309052\n1312465          fogg  0.240187\n3105014          seor  0.173718\n2695847  passepartout  0.156205\n1236796          fang  0.150566\n1748571          huck  0.117994\n2007819          lama  0.117008\n1949068           jim  0.105786\n\n\n\n\n\n\nIn our dataset we have this many novels 3 for genre: allegories\n\n\n\nTop 10 terms for genre: allegories\n\n\n\n                term     score\n1469898      grandet  0.628138\n1165825      eugenie  0.408747\n779208           chr  0.407920\n2369181        nanon  0.311669\n1472923     grassins  0.145140\n3051324       saumur  0.137501\n2050291         leni  0.134446\n651443       brstner  0.096251\n916004       cruchot  0.095331\n663741   businessman  0.080393\n\n\n\n\n\n\nIn our dataset we have this many novels 3 for genre: autobio\n\n\n\nTop 10 terms for genre: autobio\n\n\n\n                term     score\n3403765        swann  0.688831\n2481151       odette  0.505621\n2267366          mme  0.224274\n3938530     verdurin  0.207967\n1359668     franoise  0.192370\n819381       combray  0.148177\n1428660     gilberte  0.109183\n3938552    verdurins  0.106583\n1323753  forcheville  0.105283\n1504021   guermantes  0.105283\n\n\n\n\n\n\nIn our dataset we have this many novels 10 for genre: bildung\n\n\n\nTop 10 terms for genre: bildung\n\n\n\n             term     score\n2738717    philip  0.372431\n1960588    julien  0.340148\n2406231  nickleby  0.261137\n205879      amory  0.242298\n3304062   squeers  0.240849\n2405296  nicholas  0.227766\n2237832  micawber  0.216213\n2711437  peggotty  0.193896\n2988967      rnal  0.165203\n2568173    oliver  0.152509\n\n\n\n\n\n\nIn our dataset we have this many novels 7 for genre: fantasy\n\n\n\nTop 10 terms for genre: fantasy\n\n\n\n              term     score\n160439       alice  0.611279\n4045793      wendy  0.450921\n1059821    dorothy  0.334701\n2670485         oz  0.180595\n4186626    woodman  0.172680\n3064527  scarecrow  0.143049\n473048      badger  0.131457\n68336     _yahoos_  0.112021\n2027997  launcelot  0.078783\n2230265     merlin  0.074615\n\n\n\n\n\n\nIn our dataset we have this many novels 24 for genre: history\n\n\n\nTop 10 terms for genre: history\n\n\n\n              term     score\n943189   dartagnan  0.422866\n367977      aramis  0.199584\n437398       athos  0.196951\n2746155     pierre  0.178851\n910398      cristo  0.176063\n2373486     natsha  0.170972\n2787240    porthos  0.166057\n3960511   vinicius  0.157631\n3959125  villefort  0.155174\n937425    danglars  0.145695\n\n\n\n\n\n\nIn our dataset we have this many novels 2 for genre: horror\n\n\n\nTop 10 terms for genre: horror\n\n\n\n               term     score\n780566    christine  0.594813\n2890960       raoul  0.441049\n1157448        erik  0.405930\n2275543  moncharmin  0.211493\n931189         daae  0.208082\n1432283        giry  0.170559\n751823       chagny  0.168853\n2976782     richard  0.149405\n2267370         mme  0.080860\n1965794     justine  0.080163\n\n\n\n\n\n\nIn our dataset we have this many novels 4 for genre: mystery\n\n\n\nTop 10 terms for genre: mystery\n\n\n\n               term     score\n2774878      poirot  0.432187\n96004       ackroyd  0.325706\n571468   betteredge  0.296476\n1359123    franklin  0.288071\n2881754      rachel  0.230026\n651548        bruff  0.205654\n591224        blake  0.197201\n3001885     rosanna  0.183732\n1722354      holmes  0.172843\n3939030    verinder  0.171204\n\n\n\n\n\n\nIn our dataset we have this many novels 7 for genre: political\n\n\n\nTop 10 terms for genre: political\n\n\n\n             term     score\n1962606    jurgis  0.480073\n3939449    verloc  0.279961\n1464229     gould  0.271089\n3735916    tienne  0.204753\n1750185      hugh  0.182292\n791029      clare  0.166596\n2434704  nostromo  0.159203\n485295    barnaby  0.151248\n467805       aziz  0.147374\n1548401  haredale  0.142938\n\n\n\n\n\n\nIn our dataset we have this many novels 12 for genre: romance\n\n\n\nTop 10 terms for genre: romance\n\n\n\n                 term     score\n2058879         levin  0.361453\n606681         boffin  0.316385\n3973979       vronsky  0.211128\n549745          bella  0.186235\n1237333         fanny  0.179559\n4039264          wegg  0.177372\n1120634        elinor  0.175837\n1651692         hetty  0.168472\n385317   arkadyevitch  0.131648\n490675      bathsheba  0.131034\n\n\n\n\n\n\nIn our dataset we have this many novels 5 for genre: scifi\n\n\n\nTop 10 terms for genre: scifi\n\n\n\n               term     score\n2712973    pencroft  0.597450\n1546368     harding  0.320924\n931028        cyrus  0.293458\n3291100     spilett  0.255352\n50555    _nautilus_  0.248632\n1616900     herbert  0.227483\n2381939         neb  0.222443\n2389342        nemo  0.190591\n467499       ayrton  0.172882\n853995      conseil  0.133675\n\n\n\n\n\n\n\n# Filter the dataset for action and romance genres\nfiltered_df = combined_novels_nyt_genre_df[combined_novels_nyt_genre_df['genre'].isin(['action', 'romance'])]\n\n# Vectorize the text data\nvectorizer = TfidfVectorizer(stop_words=\"english\", min_df=1, max_df=0.7)\nX = vectorizer.fit_transform(filtered_df['full_cleaned_pg_eng_text'].fillna(''))\n\n# Encode the genres as binary labels\ny = filtered_df['genre'].apply(lambda x: 1 if x == 'action' else 0)\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Perform logistic regression\nlog_reg = LogisticRegression()\nlog_reg.fit(X_train, y_train)\n\n# Predict on the test set\ny_pred = log_reg.predict(X_test)\n\n# Print classification report\nprint(classification_report(y_test, y_pred, target_names=['romance', 'action']))\n\n# Extract the most distinctive coefficients\nfeature_names = vectorizer.get_feature_names_out()\ncoefficients = log_reg.coef_.flatten()\ncoeff_df = pd.DataFrame({'term': feature_names, 'coefficient': coefficients})\n\n# Sort by absolute value of coefficients to get the most distinctive terms\ncoeff_df['abs_coefficient'] = coeff_df['coefficient'].abs()\nsorted_coeff_df = coeff_df.sort_values(by='abs_coefficient', ascending=False)\n\n# Display the top 10 most distinctive terms\nconsole.print(\"Top 10 most distinctive terms for action vs romance:\")\nconsole.print(sorted_coeff_df.head(10))\n\n# Display the top 10 terms for action\nconsole.print(\"\\nTop 10 terms for action:\")\nconsole.print(sorted_coeff_df[sorted_coeff_df['coefficient'] &gt; 0].head(10))\n\n# Display the top 10 terms for romance\nconsole.print(\"\\nTop 10 terms for romance:\")\nconsole.print(sorted_coeff_df[sorted_coeff_df['coefficient'] &lt; 0].head(10))\n\n              precision    recall  f1-score   support\n\n     romance       0.75      1.00      0.86         3\n      action       1.00      0.50      0.67         2\n\n    accuracy                           0.80         5\n   macro avg       0.88      0.75      0.76         5\nweighted avg       0.85      0.80      0.78         5\n\n\n\nTop 10 most distinctive terms for action vs romance:\n\n\n\n             term  coefficient  abs_coefficient\n81961         mrs    -0.458468         0.458468\n43752        fang     0.348776         0.348776\n70298         kim     0.330332         0.330332\n46368        fogg     0.315881         0.315881\n132712        tom     0.304972         0.304972\n23468        buck     0.302627         0.302627\n73037       levin    -0.231547         0.231547\n43799       fanny    -0.225164         0.225164\n17571   bathsheba    -0.224674         0.224674\n67359      island     0.223476         0.223476\n\n\n\nTop 10 terms for action:\n\n\n\n                term  coefficient  abs_coefficient\n43752           fang     0.348776         0.348776\n70298            kim     0.330332         0.330332\n46368           fogg     0.315881         0.315881\n132712           tom     0.304972         0.304972\n23468           buck     0.302627         0.302627\n67359         island     0.223476         0.223476\n94876   passepartout     0.205433         0.205433\n61866           huck     0.182405         0.182405\n130239          thou     0.155512         0.155512\n71385           lama     0.146316         0.146316\n\n\n\nTop 10 terms for romance:\n\n\n\n             term  coefficient  abs_coefficient\n81961         mrs    -0.458468         0.458468\n73037       levin    -0.231547         0.231547\n43799       fanny    -0.225164         0.225164\n17571   bathsheba    -0.224674         0.224674\n3631         adam    -0.218932         0.218932\n105066  rochester    -0.218286         0.218286\n21858      boffin    -0.203912         0.203912\n39767   elizabeth    -0.203224         0.203224\n11560        anne    -0.199121         0.199121\n69997        kate    -0.186628         0.186628\n\n\n\n\n\n\n# Assuming combined_novels_nyt_genre_df is already defined and contains 'full_cleaned_pg_eng_text' and 'genre' columns\n\n# Vectorize the text data\nvectorizer = TfidfVectorizer(stop_words=\"english\", min_df=1, max_df=0.7)\nX = vectorizer.fit_transform(combined_novels_nyt_genre_df['full_cleaned_pg_eng_text'].fillna(''))\n\n# Encode the genres as labels\ny = combined_novels_nyt_genre_df['genre']\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Perform Random Forest classification\nrf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\nrf_clf.fit(X_train, y_train)\n\n# Predict on the test set\ny_pred = rf_clf.predict(X_test)\n\n# Print classification report\nconsole.print(classification_report(y_test, y_pred))\n\n# Extract feature importances\nfeature_names = vectorizer.get_feature_names_out()\nimportances = rf_clf.feature_importances_\nimportance_df = pd.DataFrame({'term': feature_names, 'importance': importances})\n\n# Sort by importance to get the most distinctive terms\nsorted_importance_df = importance_df.sort_values(by='importance', ascending=False)\n\n# Display the top 10 most distinctive terms\nconsole.print(\"Top 10 most distinctive terms:\")\nconsole.print(sorted_importance_df.head(10))\n\n              precision    recall  f1-score   support\n\n      action       0.00      0.00      0.00         4\n  allegories       0.00      0.00      0.00         1\n     autobio       1.00      1.00      1.00         1\n     bildung       0.00      0.00      0.00         0\n     fantasy       0.00      0.00      0.00         0\n     history       0.42      1.00      0.59         5\n      horror       0.00      0.00      0.00         1\n     mystery       0.00      0.00      0.00         1\n   political       0.00      0.00      0.00         2\n     romance       1.00      0.33      0.50         3\n\n    accuracy                           0.39        18\n   macro avg       0.24      0.23      0.21        18\nweighted avg       0.34      0.39      0.30        18\n\n\n\n\nTop 10 most distinctive terms:\n\n\n\n                term  importance\n78132        comrade    0.002868\n349101        timeto    0.002363\n315531         sugar    0.002211\n270276      rattling    0.002150\n161978          host    0.002132\n249094         pages    0.002034\n162061         hotel    0.001930\n257295         plane    0.001892\n348181       thyself    0.001822\n334056  themountains    0.001754\n\n\n\n\nmarc_records_df = pd.read_csv(\"merged_preidentified_periodicals.csv\")\nconsole.print(f\"Whe have this many marc records {len(marc_records_df)}\")\n\nWhe have this many marc records 66\n\n\n\n\ndef process_marc_record(row: pd.Series) -&gt; pd.Series:\n    \"\"\"\n    Function to process a MARC record row to generate a human-readable description and classify the periodical.\n\n    Parameters\n    ----------\n    row : pd.Series\n        A row of a dataframe with MARC record fields.\n\n    Returns\n    -------\n    pd.Series\n        A series with the human-readable description and classification from the Llama model.\n    \"\"\"\n    time.sleep(1)  # Avoid hitting API rate limits\n    try:\n        console.print(f\"Processing periodical {row['periodical_name']}\")\n        \n        knowledege_check_content = (\n               f\"Based on this periodical name: {row['periodical_name']}, what do you know about this publication? \"\n        )\n        console.print(f\"Knowledge check content: {knowledege_check_content}\", style=\"bold green\")\n        \n        # Prepare the Ollama API request for the knowledge check\n        knowledge_check_response = ollama.chat(\n            model='llama3.2',\n            messages=[\n                {\n                    'role': 'system',\n                    'content': \"You are a librarian with knowledge of periodicals. Return your answer strictly in JSON format like this: {'knowledge': 'Your knowledge here'}. Do not include any other text or explanations outside the JSON format.\"\n                },\n                {\n                    'role': 'user',\n                    'content': knowledege_check_content\n                }\n            ]\n        )\n        console.print(f\"Knowledge check response: {knowledge_check_response}\", style=\"bold green\")\n        # Attempt to parse response as JSON\n        try:\n            knowledge_message = knowledge_check_response['message']['content']\n            row['knowledge'] = ast.literal_eval(knowledge_message).get('knowledge', 'No knowledge available')\n            console.print(f\"Knowledge: {row.knowledge}\", style=\"bold green\")\n        except (ValueError, SyntaxError):\n            console.print(\"Error parsing JSON response for knowledge check.\", style=\"bold red\")\n            row['knowledge'] = \"No valid JSON returned.\"\n        # Compile MARC record fields into a structured text for prompting\n        marc_fields = '\\n'.join([f\"{field}: {value}\" for field, value in row.items() if pd.notna(value) and field != \"publication_type\"])\n        \n\n        # Define the prompt content for generating a human-readable description\n        description_content = (\n            f\"Here is combined data from multiple MARC records and HathiTrust data:\\n{marc_fields}\\nPlease create a brief human-readable description \"\n            f\"including title, author, publication date, and subjects.\"\n        )\n        console.print(f\"Description content: {description_content}\", style=\"bold green\")\n\n        # Prepare the Ollama API request for the description\n        description_response = ollama.chat(\n            model='llama3.2',\n            messages=[\n                {\n                    'role': 'system',\n                    'content': \"You are a librarian skilled in converting metadata from MARC files into user-friendly summaries. Please return your answer in strict JSON format like this: {'description': 'Your description here'}. Do not include any other text or explanations outside the JSON format.\"\n                },\n                {\n                    'role': 'user',\n                    'content': description_content\n                }\n            ]\n        )\n        console.print(f\"Description response: {description_response}\", style=\"bold green\")\n        # Attempt to parse response as JSON\n        description_message = description_response['message']['content']\n        \n        try:\n            # Extract and store the human-readable description\n            row['human_readable_description'] = ast.literal_eval(description_message).get('description', 'No description available')\n            console.print(f\"Description: {row.human_readable_description}\", style=\"bold green\")\n        except (ValueError, SyntaxError):\n            console.print(\"Error parsing JSON response for description.\", style=\"bold red\")\n            row['human_readable_description'] = \"No valid JSON returned.\"\n\n        # Define the prompt content for genre classification\n        classification_content = (\n            f\"Based on this description, classify the periodical into a genre: Information Bulletin, Radical Periodical, News & Politics Magazine.\\n\"\n            f\"Description: {row['human_readable_description']}\"\n        )\n\n        # Prepare the Ollama API request for classification\n        classification_response = ollama.chat(\n            model='llama3.2',\n            messages=[\n                {\n                    'role': 'system',\n                    'content': \"You are a classification expert who assigns genres to periodicals. \"\n                               \"Return your answer strictly in JSON format like this: {'classification': 'Your classification here'}.\"\n                               \"Do not include any other text or explanations outside the JSON format.\"\n                },\n                {\n                    'role': 'user',\n                    'content': classification_content\n                }\n            ]\n        )\n\n        # Attempt to parse response as JSON\n        classification_message = classification_response['message']['content']\n        \n        try:\n            row['classification'] = ast.literal_eval(classification_message).get('classification', 'Unclassified')\n            console.print(f\"Classification: {row.classification}\", style=\"bold green\")\n            console.print(f\"Actual classification: {row['publication_type']}\", style=\"bright_magenta\")\n            console.print(\"\\n\")\n        except (ValueError, SyntaxError):\n            console.print(\"Error parsing JSON response for classification.\", style=\"bold red\")\n            row['classification'] = \"No valid JSON returned.\"\n\n    except Exception as e:\n        console.print(f\"Unexpected error: {e}\", style=\"bold red\")\n        row['human_readable_description'] = None\n        row['classification'] = None\n\n    return row\n\n\n# Process each MARC record\nmarc_df = marc_records_df[0:10].apply(process_marc_record, axis=1)\n\nProcessing periodical AIM information bulletin\n\n\n\nKnowledge check content: Based on this periodical name: AIM information bulletin, what do you know about this \npublication? \n\n\n\nKnowledge check response: {'model': 'llama3.2', 'created_at': '2024-11-14T18:36:26.735061Z', 'message': {'role': \n'assistant', 'content': \"{'knowledge': {'title': 'AIM Information Bulletin', 'publisher': 'American Immunological \nAssociation (AIA)', 'frequency': 'bimonthly', 'circulation': '1,500-3,000', 'history': 'Published from 1948 to 1976\nand revived in 1987.', 'content': 'Original research articles, review articles, case reports, and proceedings of \nAIA conferences on immunology.'}}\"}, 'done_reason': 'stop', 'done': True, 'total_duration': 9893991375, \n'load_duration': 31743208, 'prompt_eval_count': 87, 'prompt_eval_duration': 1915974000, 'eval_count': 91, \n'eval_duration': 7942370000}\n\n\n\nKnowledge: {'title': 'AIM Information Bulletin', 'publisher': 'American Immunological Association (AIA)', \n'frequency': 'bimonthly', 'circulation': '1,500-3,000', 'history': 'Published from 1948 to 1976 and revived in \n1987.', 'content': 'Original research articles, review articles, case reports, and proceedings of AIA conferences \non immunology.'}\n\n\n\nDescription content: Here is combined data from multiple MARC records and HathiTrust data:\nperiodical_name: AIM information bulletin\n001: 2698052.0\nupdated_ht_bib_key: 002698052\nlink: https://hdl.handle.net/2027/uc1.c088753409, https://hdl.handle.net/2027/uc1.31158013169338, \nhttps://hdl.handle.net/2027/inu.30000117848279, https://hdl.handle.net/2027/uc1.l0068269067, \nhttps://hdl.handle.net/2027/uc1.l0068269125, https://hdl.handle.net/2027/uc1.l0068269026, \nhttps://hdl.handle.net/2027/inu.30000117848261, https://hdl.handle.net/2027/uc1.c006804189, \nhttps://hdl.handle.net/2027/inu.30000117848329, https://hdl.handle.net/2027/uc1.l0060235397, \nhttps://hdl.handle.net/2027/inu.30000117848337, https://hdl.handle.net/2027/inu.30000100306202, \nhttps://hdl.handle.net/2027/inu.30000117848295, https://hdl.handle.net/2027/inu.30000100306194, \nhttps://hdl.handle.net/2027/uc1.31158012522107, https://hdl.handle.net/2027/inu.30000117848253, \nhttps://hdl.handle.net/2027/inu.30000117848287\nhtid: inu.30000100306194, inu.30000117848279, inu.30000117848295, inu.30000117848287, uc1.l0068269067, \nuc1.c006804189, uc1.l0068269026, inu.30000117848261, inu.30000117848329, uc1.c088753409, inu.30000100306202, \ninu.30000117848253, uc1.31158013169338, uc1.l0060235397, uc1.31158012522107, inu.30000117848337, uc1.l0068269125\ndate: no.091-122 yr.1984-86, Suppl. no.78, no.102-113 1985, no.079 yr.1983 mi.supp, no.077-079 yr.1982, no.24-25 \n1978, no.79-90 1983, no.12-30 (July 1977-Dec.1978) NO.12-30 (JULY 1977-DEC.1978), no.126-137 1987, no.136 (Nov \n1987), no.45-54 1980, no.91-101 1984, no.075 yr.1982, no.114-125 1986, no.084 yr.1983, no.69 yr.1982 mi.supp., \nno.16-81 inc. 1977-83\noriginal_source: \n                       University of California\n                      , \n                       Indiana University\n                      \nrecord_url: https://catalog.hathitrust.org/Record/002698052\nid: inu.30000100306194, inu.30000117848279, inu.30000117848295, inu.30000117848287, uc1.l0068269067, \nuc1.c006804189, uc1.l0068269026, inu.30000117848261, inu.30000117848329, uc1.c088753409, inu.30000100306202, \ninu.30000117848253, uc1.31158013169338, uc1.l0060235397, uc1.31158012522107, inu.30000117848337, uc1.l0068269125\nmetadata_schema_version: https://schemas.hathitrust.org/EF_Schema_MetadataSubSchema_v_3.0\nenumeration_chronology: no.091-122 yr.1984-86, Suppl. no.78, no.102-113 1985, no.079 yr.1983 mi.supp, no.077-079 \nyr.1982, no.24-25 1978, no.79-90 1983, no.12-30 (July 1977-Dec.1978) NO.12-30 (JULY 1977-DEC.1978), no.126-137 \n1987, no.136 (Nov 1987), no.45-54 1980, no.91-101 1984, no.075 yr.1982, no.114-125 1986, no.084 yr.1983, no.69 \nyr.1982 mi.supp., no.16-81 inc. 1977-83\ntype_of_resource: http://id.loc.gov/ontologies/bibframe/Text\ntitle: AIM information bulletin /\ndate_created: 20200209.0\npub_date: 1984.0, 1985.0, 1986.0, 1987.0, 1988.0, 1978.0, 1980.0, 1982.0, 1983.0\nlanguage: eng\naccess_profile: google\nlccn: 85647346\noclc: 12339773.0\npage_count: 32.0, 354.0, 34.0, 322.0, 72.0, 232.0, 426.0, 332.0, 300.0, 46.0, 528.0, 242.0, 52.0, 20.0, 22.0, 158.0\nfeature_schema_version: https://schemas.hathitrust.org/EF_Schema_FeaturesSubSchema_v_3.0\naccess_rights: und, ic\nalternate_title: ['Mozambique news', 'Agência de Informação de Moçambique information bulletin.', 'A.I.M. \ninformation bulletin.', 'AIM bulletin.']\ncategory: Africa\ngenre_ld: ['http://id.loc.gov/vocabulary/marcgt/doc', 'http://id.loc.gov/vocabulary/marcgt/gov']\ngenre: ['document (computer)', 'government publication']\ncontributor_ld: {'id': 'http://www.viaf.org/viaf/148452547', 'type': \n'http://id.loc.gov/ontologies/bibframe/Organization', 'name': 'Agência de Informação de Moçambique.'}\ncontributor: Agência de Informação de Moçambique.\nhandle_url: http://hdl.handle.net/2027/inu.30000117848295, http://hdl.handle.net/2027/inu.30000117848337, \nhttp://hdl.handle.net/2027/inu.30000117848261, http://hdl.handle.net/2027/inu.30000117848287, \nhttp://hdl.handle.net/2027/uc1.l0068269026, http://hdl.handle.net/2027/uc1.31158012522107, \nhttp://hdl.handle.net/2027/uc1.c006804189, http://hdl.handle.net/2027/inu.30000117848253, \nhttp://hdl.handle.net/2027/inu.30000117848329, http://hdl.handle.net/2027/uc1.31158013169338, \nhttp://hdl.handle.net/2027/inu.30000117848279, http://hdl.handle.net/2027/uc1.l0060235397, \nhttp://hdl.handle.net/2027/uc1.c088753409, http://hdl.handle.net/2027/uc1.l0068269125, \nhttp://hdl.handle.net/2027/uc1.l0068269067, http://hdl.handle.net/2027/inu.30000100306202, \nhttp://hdl.handle.net/2027/inu.30000100306194\nsource_institution_ld: {'type': 'http://id.loc.gov/ontologies/bibframe/Organization', 'name': 'UCLA'}, {'type': \n'http://id.loc.gov/ontologies/bibframe/Organization', 'name': 'INU'}, {'type': \n'http://id.loc.gov/ontologies/bibframe/Organization', 'name': 'UCBK'}\nsource_institution: INU, UCBK, UCLA\nlcc: DT451.A4\ntype: ['DataFeedItem', 'PublicationVolume']\nis_part_of: {'id': 'http://www.worldcat.org/oclc/12339773', 'type': 'CreativeWorkSeries', 'journalTitle': 'AIM \ninformation bulletin (Maputo, Mozambique : 1977)'}\nlast_rights_update_date: 20190721.0, 20180417.0, 20171207.0, 20180528.0, 20190805.0\npub_place_ld: {'id': 'http://id.loc.gov/vocabulary/countries/mz', 'type': \n'http://id.loc.gov/ontologies/bibframe/Place', 'name': 'Mozambique'}\npub_place: Mozambique\nmain_entity_of_page: ['https://catalog.hathitrust.org/Record/002698052', \n'http://catalog.hathitrust.org/api/volumes/brief/oclc/12339773.json', \n'http://catalog.hathitrust.org/api/volumes/full/oclc/12339773.json']\npublisher_ld: {'id': 'http://catalogdata.library.illinois.edu/lod/entities/ProvisionActivityAgent/ht/The%20Agency',\n'type': 'http://id.loc.gov/ontologies/bibframe/Organization', 'name': 'The Agency'}\npublisher: The Agency\nlowercase_periodical_name: aim_information_bulletin\npublication_directory: datasets/ht_ef_datasets/aim_information_bulletin\nvolume_directory: inu_30000100306202, uc1_31158012522107, inu_30000117848295, inu_30000100306194, uc1_l0068269026, \ninu_30000117848279, uc1_l0068269125, inu_30000117848337, uc1_31158013169338, uc1_c088753409, uc1_c006804189, \ninu_30000117848287, inu_30000117848253, inu_30000117848329, inu_30000117848261, uc1_l0068269067, uc1_l0060235397\nht_bib_key: 2698052.0\npublishDates: 1977.0\n974_y: 1983.0\nlastUpdate: 20240710.0, 20231211.0, 20231212.0, 20240109.0, 20240716.0, 20230331.0\nenumcron: no.091-122 yr.1984-86, Suppl. no.78, no.102-113 1985, no.079 yr.1983 mi.supp, no.077-079 yr.1982, \nno.24-25 1978, no.79-90 1983, no.12-30 (July 1977-Dec.1978) NO.12-30 (JULY 1977-DEC.1978), no.126-137 1987, no.136 \n(Nov 1987), no.45-54 1980, no.91-101 1984, no.075 yr.1982, no.114-125 1986, no.084 yr.1983, no.69 yr.1982 mi.supp.,\nno.16-81 inc. 1977-83\nmin_year: 1977.0\nmax_year: 1987.0\nextracted_year: 1984.0, 1985.0, 1986.0, 1987.0, 1977.0, 1978.0, 1980.0, 1982.0, 1983.0\nfinalized_year: 1984.0, 1985.0, 1986.0, 1987.0, 1977.0, 1978.0, 1980.0, 1982.0, 1983.0\nprocessed_finalized_year: 1980-01-01, 1983-01-01, 1987-01-01, 1978-01-01, 1984-01-01, 1982-01-01, 1986-01-01, \n1977-01-01, 1985-01-01\nknowledge: {'title': 'AIM Information Bulletin', 'publisher': 'American Immunological Association (AIA)', \n'frequency': 'bimonthly', 'circulation': '1,500-3,000', 'history': 'Published from 1948 to 1976 and revived in \n1987.', 'content': 'Original research articles, review articles, case reports, and proceedings of AIA conferences \non immunology.'}\nPlease create a brief human-readable description including title, author, publication date, and subjects.\n\n\n\nDescription response: {'model': 'llama3.2', 'created_at': '2024-11-14T18:36:46.914611Z', 'message': {'role': \n'assistant', 'content': 'Here is a brief human-readable description based on the provided metadata:\\n\\n**Title:** \nAIM Information Bulletin\\n**Author:** American Immunological Association (AIA)\\n**Publication Date:** \n1977-1987\\n**Subjects:** Immunology, Research Articles, Review Articles, Case Reports, AIA Conferences\\n\\nNote: The\npublication dates range from 1977 to 1987, but the exact dates for each issue are not provided in the metadata. If \nyou need more specific information, please let me know!'}, 'done_reason': 'stop', 'done': True, 'total_duration': \n20117827250, 'load_duration': 50812666, 'prompt_eval_count': 1026, 'prompt_eval_duration': 10822634000, \n'eval_count': 104, 'eval_duration': 9161351000}\n\n\n\nError parsing JSON response for description.\n\n\n\nClassification: Radical Periodical\n\n\n\nActual classification: Information Bulletin\n\n\n\n\n\n\nProcessing periodical Africanist news and views\n\n\n\nKnowledge check content: Based on this periodical name: Africanist news and views, what do you know about this \npublication? \n\n\n\nKnowledge check response: {'model': 'llama3.2', 'created_at': '2024-11-14T18:36:58.831176Z', 'message': {'role': \n'assistant', 'content': '{\\'knowledge\\':\\n   {\"publication_type\": \"magazine\", \\n    \"focusing_area\": \n\"Africa-related topics\", \\n    \"geographicFocus\": \"Africa\",\\n    \"genre\": \"News analysis and opinion\",\\n    \n\"language\": \"English\",\\n    \"frequency\": \"Monthly\",\\n    \"circulation\": \"Limited\",\\n    \"available Formats\": \n[\"Print\", \"Digital\"],\\n    \"target audience\": \"Academics, researchers, Africanists\"}\\n}'}, 'done_reason': 'stop', \n'done': True, 'total_duration': 9061780333, 'load_duration': 31473375, 'prompt_eval_count': 89, \n'prompt_eval_duration': 854034000, 'eval_count': 94, 'eval_duration': 8173412000}\n\n\n\nKnowledge: {'publication_type': 'magazine', 'focusing_area': 'Africa-related topics', 'geographicFocus': 'Africa', \n'genre': 'News analysis and opinion', 'language': 'English', 'frequency': 'Monthly', 'circulation': 'Limited', \n'available Formats': ['Print', 'Digital'], 'target audience': 'Academics, researchers, Africanists'}\n\n\n\nDescription content: Here is combined data from multiple MARC records and HathiTrust data:\nperiodical_name: Africanist news and views\n001: 8867156.0\nupdated_ht_bib_key: 008867156\nlink: https://babel.hathitrust.org/cgi/pt?id=inu.30000122781960, \nhttps://babel.hathitrust.org/cgi/pt?id=inu.30000122781986\nhtid: inu.30000122781960, inu.30000122781986\ndate: no.1-4 1966, no.6-7 1968\noriginal_source: Indiana University\nrecord_url: https://catalog.hathitrust.org/Record/008867156\nid: inu.30000122781960\nmetadata_schema_version: https://schemas.hathitrust.org/EF_Schema_MetadataSubSchema_v_3.0\nenumeration_chronology: no.1-4 1966\ntype_of_resource: http://id.loc.gov/ontologies/bibframe/Text\ntitle: Africanist news and views /\ndate_created: 20200209.0\npub_date: 1966.0\nlanguage: eng\naccess_profile: google\nlccn: sn 82004378\noclc: 8541914.0\npage_count: 66.0\nfeature_schema_version: https://schemas.hathitrust.org/EF_Schema_FeaturesSubSchema_v_3.0\naccess_rights: ic\ngenre_ld: http://id.loc.gov/vocabulary/marcgt/doc\ngenre: ['document (computer)']\ncontributor_ld: {'id': 'http://www.viaf.org/viaf/136634124', 'type': \n'http://id.loc.gov/ontologies/bibframe/Organization', 'name': 'Pan Africanist Congress.'}\ncontributor: Pan Africanist Congress.\nhandle_url: http://hdl.handle.net/2027/inu.30000122781960\nsource_institution_ld: {'type': 'http://id.loc.gov/ontologies/bibframe/Organization', 'name': 'INU'}\nsource_institution: INU\ntype: ['DataFeedItem', 'PublicationVolume']\nis_part_of: {'id': 'http://www.worldcat.org/oclc/8541914', 'type': 'CreativeWorkSeries', 'journalTitle': \n'Africanist news and views /'}\nlast_rights_update_date: 20180416.0\npub_place_ld: {'id': 'http://id.loc.gov/vocabulary/countries/ua', 'type': \n'http://id.loc.gov/ontologies/bibframe/Place', 'name': 'Egypt'}\npub_place: Egypt\nmain_entity_of_page: ['https://catalog.hathitrust.org/Record/008867156', \n'http://catalog.hathitrust.org/api/volumes/brief/oclc/8541914.json', \n'http://catalog.hathitrust.org/api/volumes/full/oclc/8541914.json']\npublisher_ld: {'id': \n'http://catalogdata.library.illinois.edu/lod/entities/ProvisionActivityAgent/ht/The%20Congress%3F', 'type': \n'http://id.loc.gov/ontologies/bibframe/Organization', 'name': 'The Congress?'}\npublisher: The Congress?\nlowercase_periodical_name: africanist_news_and_views\npublication_directory: datasets/ht_ef_datasets/africanist_news_and_views\nvolume_directory: inu_30000122781986, inu_30000122781960\nht_bib_key: 8867156.0\npublishDates: 1966.0\n974_y: 1966.0\nlastUpdate: 20240113.0, 20240604.0\nenumcron: no.1-4 1966, no.6-7 1968\nmin_year: 1966.0\nmax_year: 1968.0\nextracted_year: 1968.0, 1966.0\nfinalized_year: 1968.0, 1966.0\nprocessed_finalized_year: 1968-01-01, 1966-01-01\nknowledge: {'publication_type': 'magazine', 'focusing_area': 'Africa-related topics', 'geographicFocus': 'Africa', \n'genre': 'News analysis and opinion', 'language': 'English', 'frequency': 'Monthly', 'circulation': 'Limited', \n'available Formats': ['Print', 'Digital'], 'target audience': 'Academics, researchers, Africanists'}\nPlease create a brief human-readable description including title, author, publication date, and subjects.\n\n\n\nDescription response: {'model': 'llama3.2', 'created_at': '2024-11-14T18:37:17.140452Z', 'message': {'role': \n'assistant', 'content': \"{'description': 'Africanist News and Views is a magazine published by the Pan Africanist \nCongress, focusing on Africa-related topics, with monthly issues released from 1966 to 1968. The English-language \npublication covers news analysis and opinion.'}\"}, 'done_reason': 'stop', 'done': True, 'total_duration': \n18279357417, 'load_duration': 35433542, 'prompt_eval_count': 1054, 'prompt_eval_duration': 13272852000, \n'eval_count': 52, 'eval_duration': 4961151000}\n\n\n\nDescription: Africanist News and Views is a magazine published by the Pan Africanist Congress, focusing on \nAfrica-related topics, with monthly issues released from 1966 to 1968. The English-language publication covers news\nanalysis and opinion.\n\n\n\nClassification: Radical Periodical\n\n\n\nActual classification: Information Bulletin\n\n\n\n\n\n\nProcessing periodical Afrique Documents\n\n\n\nKnowledge check content: Based on this periodical name: Afrique Documents, what do you know about this publication?\n\n\n\nKnowledge check response: {'model': 'llama3.2', 'created_at': '2024-11-14T18:37:26.228155Z', 'message': {'role': \n'assistant', 'content': '{\\'knowledge\\':\\n  {\"Afrique Documents\": \"A French-language magazine that provides \nin-depth analysis and commentary on African politics, economics, culture, and development.\\n  Coverage includes \nnews updates from across the continent,\\n  analysis of regional and international trends,\\n  feature articles on \nkey figures and events,\\n  country overviews and expert opinions.\"}}'}, 'done_reason': 'stop', 'done': True, \n'total_duration': 6191203625, 'load_duration': 32680250, 'prompt_eval_count': 87, 'prompt_eval_duration': \n901576000, 'eval_count': 69, 'eval_duration': 5252984000}\n\n\n\nError parsing JSON response for knowledge check.\n\n\n\nDescription content: Here is combined data from multiple MARC records and HathiTrust data:\nperiodical_name: Afrique Documents\n001: 8569258.0\nupdated_ht_bib_key: 008569258\nlink: https://hdl.handle.net/2027/inu.30000117844955, https://hdl.handle.net/2027/inu.30000117844963, \nhttps://hdl.handle.net/2027/inu.30000117844997, https://hdl.handle.net/2027/inu.30000117844948, \nhttps://hdl.handle.net/2027/uc1.b3451703, https://hdl.handle.net/2027/inu.30000117844971, \nhttps://hdl.handle.net/2027/inu.30000117844989\nhtid: inu.30000117844948, inu.30000117844997, inu.30000117844971, inu.30000117844989, inu.30000117844963, \nuc1.b3451703, inu.30000117844955\ndate: no.101-106 1969, no.96-100 1968, no.84-89 1966, no.90-95 1967, no.75-78 1964-1965, no.81-83 1965, no.96-106 \n(1968-69)\noriginal_source: \n                       University of California\n                      , \n                       Indiana University\n                      \nrecord_url: https://catalog.hathitrust.org/Record/008569258\nid: inu.30000117844948, inu.30000117844997, inu.30000117844971, inu.30000117844989, inu.30000117844963, \nuc1.b3451703, inu.30000117844955\nmetadata_schema_version: https://schemas.hathitrust.org/EF_Schema_MetadataSubSchema_v_3.0\nenumeration_chronology: no.101-106 1969, no.96-100 1968, no.84-89 1966, no.90-95 1967, no.75-78 1964-1965, no.81-83\n1965, no.96-106 (1968-69)\ntype_of_resource: http://id.loc.gov/ontologies/bibframe/Text\ntitle: Afrique documents., Afrique Documents\ndate_created: 20200209.0\npub_date: 1965.0, 1966.0, 1967.0, 1968.0, 1969.0\nlanguage: fre\naccess_profile: google\nissn: 0568-1766\nlccn: sn 85022670\noclc: 5969711.0\npage_count: 290.0, 764.0, 142.0, 208.0, 400.0, 370.0, 380.0\nfeature_schema_version: https://schemas.hathitrust.org/EF_Schema_FeaturesSubSchema_v_3.0\naccess_rights: ic\ngenre_ld: http://id.loc.gov/vocabulary/marcgt/doc\ngenre: ['document (computer)']\nhandle_url: http://hdl.handle.net/2027/inu.30000117844997, http://hdl.handle.net/2027/inu.30000117844989, \nhttp://hdl.handle.net/2027/inu.30000117844971, http://hdl.handle.net/2027/inu.30000117844955, \nhttp://hdl.handle.net/2027/inu.30000117844948, http://hdl.handle.net/2027/uc1.b3451703, \nhttp://hdl.handle.net/2027/inu.30000117844963\nsource_institution_ld: {'type': 'http://id.loc.gov/ontologies/bibframe/Organization', 'name': 'NRLF'}, {'type': \n'http://id.loc.gov/ontologies/bibframe/Organization', 'name': 'INU'}\nsource_institution: INU, NRLF\ntype: ['DataFeedItem', 'PublicationVolume']\nis_part_of: {'id': 'http://www.worldcat.org/oclc/5969711', 'type': 'CreativeWorkSeries', 'journalTitle': 'Afrique \ndocuments.'}, {'id': '_:b010100315', 'type': 'CreativeWorkSeries', 'journalTitle': 'Afrique Documents'}\nlast_rights_update_date: 20180419.0, 20191007.0\npub_place_ld: {'id': 'http://id.loc.gov/vocabulary/countries/sg', 'type': \n'http://id.loc.gov/ontologies/bibframe/Place', 'name': 'Senegal'}, {'id': \n'http://id.loc.gov/vocabulary/countries/fr', 'type': 'http://id.loc.gov/ontologies/bibframe/Place', 'name': \n'France'}\npub_place: France, Senegal\nmain_entity_of_page: ['https://catalog.hathitrust.org/Record/010100315'], \n['https://catalog.hathitrust.org/Record/008569258', \n'http://catalog.hathitrust.org/api/volumes/brief/oclc/5969711.json', \n'http://catalog.hathitrust.org/api/volumes/full/oclc/5969711.json']\npublisher_ld: {'id': \n'http://catalogdata.library.illinois.edu/lod/entities/ProvisionActivityAgent/ht/Afrique%20Documents', 'type': \n'http://id.loc.gov/ontologies/bibframe/Organization', 'name': 'Afrique Documents'}\npublisher: Afrique Documents\nlowercase_periodical_name: afrique_documents\npublication_directory: datasets/ht_ef_datasets/afrique_documents\nvolume_directory: uc1_b3451703, inu_30000117844971, inu_30000117844997, inu_30000117844963, inu_30000117844955, \ninu_30000117844948, inu_30000117844989\nht_bib_key: 8569258.0\npublishDates: 1960.0\n974_y: 1969.0\nlastUpdate: 20230321.0, 20240108.0, 20231221.0\nenumcron: no.101-106 1969, no.96-100 1968, no.84-89 1966, no.90-95 1967, no.75-78 1964-1965, no.81-83 1965, \nno.96-106 (1968-69)\nmin_year: 1964.0\nmax_year: 1969.0\nextracted_year: 1964.0, 1965.0, 1966.0, 1967.0, 1968.0, 1969.0\nfinalized_year: 1964.0, 1965.0, 1966.0, 1967.0, 1968.0, 1969.0\nprocessed_finalized_year: 1967-01-01, 1969-01-01, 1965-01-01, 1968-01-01, 1964-01-01, 1966-01-01\nknowledge: No valid JSON returned.\nPlease create a brief human-readable description including title, author, publication date, and subjects.\n\n\n\nDescription response: {'model': 'llama3.2', 'created_at': '2024-11-14T18:37:52.51246Z', 'message': {'role': \n'assistant', 'content': \"{'description': 'Afrique Documents is a series of French-language documents about Africa. \nThe publications cover various topics such as politics, society, culture, and history. The first issue was \npublished in 1964 and the most recent in 1969.'}\"}, 'done_reason': 'stop', 'done': True, 'total_duration': \n26246050417, 'load_duration': 32415958, 'prompt_eval_count': 1673, 'prompt_eval_duration': 20663724000, \n'eval_count': 53, 'eval_duration': 5536505000}\n\n\n\nDescription: Afrique Documents is a series of French-language documents about Africa. The publications cover \nvarious topics such as politics, society, culture, and history. The first issue was published in 1964 and the most \nrecent in 1969.\n\n\n\nClassification: Information Bulletin\n\n\n\nActual classification: Information Bulletin\n\n\n\n\n\n\nProcessing periodical Afro Asian Bulletin\n\n\n\nKnowledge check content: Based on this periodical name: Afro Asian Bulletin, what do you know about this \npublication? \n\n\n\nKnowledge check response: {'model': 'llama3.2', 'created_at': '2024-11-14T18:38:03.314835Z', 'message': {'role': \n'assistant', 'content': '{\\'knowledge\\':\\n  {\"title\": \"Afro-Asian Bulletin\", \"type\": \"periodical\", \"description\": \n\"A quarterly journal of news, analysis and opinion on the relations between AfroAsia and other regions.\", \n\"publisher\": \"Research Institute for International Affairs (RIIA)\", \"founding_date\": \"1963\", \"country_of_origin\": \n\"Japan\"}}'}, 'done_reason': 'stop', 'done': True, 'total_duration': 7846059084, 'load_duration': 30328834, \n'prompt_eval_count': 87, 'prompt_eval_duration': 463746000, 'eval_count': 75, 'eval_duration': 7348257000}\n\n\n\nKnowledge: {'title': 'Afro-Asian Bulletin', 'type': 'periodical', 'description': 'A quarterly journal of news, \nanalysis and opinion on the relations between AfroAsia and other regions.', 'publisher': 'Research Institute for \nInternational Affairs (RIIA)', 'founding_date': '1963', 'country_of_origin': 'Japan'}\n\n\n\nDescription content: Here is combined data from multiple MARC records and HathiTrust data:\nperiodical_name: Afro Asian Bulletin\n001: 0055797.0\nupdated_ht_bib_key: 000055797\nlink: https://hdl.handle.net/2027/inu.32000013032935, https://hdl.handle.net/2027/inu.32000013032885, \nhttps://hdl.handle.net/2027/inu.32000013032836, https://hdl.handle.net/2027/inu.32000013032893, \nhttps://hdl.handle.net/2027/wau.39352050623487, https://hdl.handle.net/2027/mdp.39015061285188, \nhttps://hdl.handle.net/2027/inu.32000013032901, https://hdl.handle.net/2027/inu.32000013032919, \nhttps://hdl.handle.net/2027/inu.32000013032828, https://hdl.handle.net/2027/inu.32000013032927, \nhttps://hdl.handle.net/2027/mdp.39015061286350, https://hdl.handle.net/2027/mdp.39015061285097\nhtid: mdp.39015061285097, inu.32000013032901, wau.39352050623487, inu.32000013032893, inu.32000013032836, \nmdp.39015061285188, inu.32000013032885, inu.32000013032935, inu.32000013032919, mdp.39015061286350, \ninu.32000013032828, inu.32000013032927\ndate: v.8,no.1-2 1966, v.9,no.1-6 1967, v.10,no.9-12 1967, v.4 1962, v.4 no.1-2 1962, v.5 no.1-4 1963, v.7 1965, \nv.6 1964, v.3,no.7-12 1961, v.5 1963, v.3 no.5-12 1961\noriginal_source: \n                       University of Michigan\n                      , \n                       University of Washington\n                      , \n                       Indiana University\n                      \nrecord_url: https://catalog.hathitrust.org/Record/000055797\nid: mdp.39015061285097, inu.32000013032901, inu.32000013032893, inu.32000013032836, mdp.39015061285188, \ninu.32000013032885, inu.32000013032935, inu.32000013032919, mdp.39015061286350, inu.32000013032828, \ninu.32000013032927\nmetadata_schema_version: https://schemas.hathitrust.org/EF_Schema_MetadataSubSchema_v_3.0\nenumeration_chronology: v.8,no.1-2 1966, v.9,no.1-6 1967, v.10,no.9-12 1967, v.4 1962, v.4 no.1-2 1962, v.5 no.1-4 \n1963, v.7 1965, v.6 1964, v.3,no.7-12 1961, v.5 1963, v.3 no.5-12 1961\ntype_of_resource: http://id.loc.gov/ontologies/bibframe/Text\ntitle: Afro-Asian bulletin :  monthly journal of the Permanent Secretariat of Afro-Asian Solidarity.\ndate_created: 20200209.0\npub_date: 1961.0, 1962.0, 1963.0, 1964.0, 1965.0, 1966.0, 1967.0\nlanguage: eng\naccess_profile: google\nlccn: ['sn 89044037', '64001216']\noclc: 2257026.0\npage_count: 224.0, 298.0, 74.0, 78.0, 208.0, 276.0, 216.0, 94.0, 88.0, 124.0, 350.0\nfeature_schema_version: https://schemas.hathitrust.org/EF_Schema_FeaturesSubSchema_v_3.0\naccess_rights: ic\ncategory: Asia\ngenre_ld: http://id.loc.gov/vocabulary/marcgt/doc\ngenre: ['document (computer)']\ncontributor_ld: {'id': 'http://www.viaf.org/viaf/156453588', 'type': \n'http://id.loc.gov/ontologies/bibframe/Organization', 'name': \"Permanent Organization for Afro-Asian Peoples' \nSolidarity. Permanent Secretariat.\"}\ncontributor: Permanent Organization for Afro-Asian Peoples' Solidarity. Permanent Secretariat.\nhandle_url: http://hdl.handle.net/2027/inu.32000013032828, http://hdl.handle.net/2027/mdp.39015061285188, \nhttp://hdl.handle.net/2027/inu.32000013032927, http://hdl.handle.net/2027/mdp.39015061286350, \nhttp://hdl.handle.net/2027/inu.32000013032919, http://hdl.handle.net/2027/inu.32000013032935, \nhttp://hdl.handle.net/2027/inu.32000013032893, http://hdl.handle.net/2027/inu.32000013032901, \nhttp://hdl.handle.net/2027/mdp.39015061285097, http://hdl.handle.net/2027/inu.32000013032836, \nhttp://hdl.handle.net/2027/inu.32000013032885\nsource_institution_ld: {'type': 'http://id.loc.gov/ontologies/bibframe/Organization', 'name': 'INU'}, {'type': \n'http://id.loc.gov/ontologies/bibframe/Organization', 'name': 'MIU'}\nsource_institution: INU, MIU\nlcc: DS35.N251\ntype: ['DataFeedItem', 'PublicationVolume']\nis_part_of: {'id': 'http://www.worldcat.org/oclc/2257026', 'type': 'CreativeWorkSeries', 'journalTitle': \n'Afro-Asian bulletin :  monthly journal of the Permanent Secretariat of Afro-Asian Solidarity.'}\nlast_rights_update_date: 20191225.0, 20181113.0, 20180601.0, 20180310.0\npub_place_ld: {'id': 'http://id.loc.gov/vocabulary/countries/ua', 'type': \n'http://id.loc.gov/ontologies/bibframe/Place', 'name': 'Egypt'}\npub_place: Egypt\nmain_entity_of_page: ['https://catalog.hathitrust.org/Record/000055797', \n'http://catalog.hathitrust.org/api/volumes/brief/oclc/2257026.json', \n'http://catalog.hathitrust.org/api/volumes/full/oclc/2257026.json']\npublisher_ld: {'id': \n'http://catalogdata.library.illinois.edu/lod/entities/ProvisionActivityAgent/ht/The%20Secretariat', 'type': \n'http://id.loc.gov/ontologies/bibframe/Organization', 'name': 'The Secretariat'}\npublisher: The Secretariat\nlowercase_periodical_name: afro_asian_bulletin\npublication_directory: datasets/ht_ef_datasets/afro_asian_bulletin\nvolume_directory: inu_32000013032836, inu_32000013032927, inu_32000013032893, inu_32000013032919, \ninu_32000013032885, inu_32000013032828, inu_32000013032935, mdp_39015061285188, mdp_39015061286350, \ninu_32000013032901, mdp_39015061285097\nht_bib_key: 55797.0\npublishDates: 1958.0\n974_y: 1963.0\nlastUpdate: 20230305.0, 20241001.0, 20230223.0, 20230227.0, 20220119.0\nenumcron: v.8,no.1-2 1966, v.9,no.1-6 1967, v.10,no.9-12 1967, v.4 1962, v.4 no.1-2 1962, v.5 no.1-4 1963, v.7 \n1965, v.6 1964, False, v.3,no.7-12 1961, v.5 1963, v.3 no.5-12 1961\nmin_year: 1961.0\nmax_year: 1967.0\nextracted_year: 1961.0, 1962.0, 1963.0, 1964.0, 1965.0, 1966.0, 1967.0\nfinalized_year: 1961.0, 1962.0, 1963.0, 1964.0, 1965.0, 1966.0, 1967.0\nprocessed_finalized_year: 1967-01-01, 1965-01-01, 1963-01-01, 1964-01-01, 1966-01-01, 1961-01-01, 1962-01-01\nknowledge: {'title': 'Afro-Asian Bulletin', 'type': 'periodical', 'description': 'A quarterly journal of news, \nanalysis and opinion on the relations between AfroAsia and other regions.', 'publisher': 'Research Institute for \nInternational Affairs (RIIA)', 'founding_date': '1963', 'country_of_origin': 'Japan'}\nPlease create a brief human-readable description including title, author, publication date, and subjects.\n\n\n\nDescription response: {'model': 'llama3.2', 'created_at': '2024-11-14T18:38:24.249517Z', 'message': {'role': \n'assistant', 'content': 'Here is a brief human-readable description:\\n\\n**Afro-Asian Bulletin**\\n\\nPublished by the\nResearch Institute for International Affairs (RIIA) in Japan, the Afro-Asian Bulletin is a quarterly journal \ncovering news, analysis, and opinion on relations between AfroAsia and other regions. First published in 1958, it \nprovides insights into international affairs, politics, economy, culture, and social issues affecting the \nAfro-Asian region and its connections with the world.'}, 'done_reason': 'stop', 'done': True, 'total_duration': \n20879316208, 'load_duration': 31994583, 'prompt_eval_count': 1026, 'prompt_eval_duration': 12072136000, \n'eval_count': 92, 'eval_duration': 8761600000}\n\n\n\nError parsing JSON response for description.\n\n\n\nClassification: Information Bulletin\n\n\n\nActual classification: News/politics magazines\n\n\n\n\n\n\nProcessing periodical Afro Asian Peoples\n\n\n\nKnowledge check content: Based on this periodical name: Afro Asian Peoples, what do you know about this \npublication? \n\n\n\nKnowledge check response: {'model': 'llama3.2', 'created_at': '2024-11-14T18:38:31.446366Z', 'message': {'role': \n'assistant', 'content': \"{'knowledge': 'Afro Asian Peoples is a monthly magazine published in London, UK. It \nfocuses on news and issues relevant to African and Asian communities in the UK and worldwide. The magazine covers \ntopics such as politics, culture, arts, and social justice.'}\"}, 'done_reason': 'stop', 'done': True, \n'total_duration': 4413567458, 'load_duration': 71654583, 'prompt_eval_count': 87, 'prompt_eval_duration': \n439834000, 'eval_count': 54, 'eval_duration': 3889129000}\n\n\n\nKnowledge: Afro Asian Peoples is a monthly magazine published in London, UK. It focuses on news and issues relevant\nto African and Asian communities in the UK and worldwide. The magazine covers topics such as politics, culture, \narts, and social justice.\n\n\n\nDescription content: Here is combined data from multiple MARC records and HathiTrust data:\nperiodical_name: Afro Asian Peoples\n001: 6064527.0\nupdated_ht_bib_key: 006064527\nlink: https://hdl.handle.net/2027/inu.32000013032851, https://hdl.handle.net/2027/uc1.c2693406, \nhttps://hdl.handle.net/2027/inu.32000013032844\nhtid: inu.32000013032844, uc1.c2693406, inu.32000013032851\ndate: 1969-70, v.12,no.1,3,5 1970, v.11,no.2-4 1969\noriginal_source: \n                       University of California\n                      , \n                       Indiana University\n                      \nrecord_url: https://catalog.hathitrust.org/Record/006064527\nid: inu.32000013032844, uc1.c2693406, inu.32000013032851\nmetadata_schema_version: https://schemas.hathitrust.org/EF_Schema_MetadataSubSchema_v_3.0\nenumeration_chronology: 1969-70, v.12,no.1,3,5 1970, v.11,no.2-4 1969\ntype_of_resource: http://id.loc.gov/ontologies/bibframe/Text\ntitle: Afro-Asian peoples.\ndate_created: 20200209.0\npub_date: 1969.0, 1970.0\nlanguage: eng\naccess_profile: google\nissn: 0002-0648\nlccn: 75640482/NE\noclc: 2240391.0\npage_count: 110.0, 306.0, 150.0\nfeature_schema_version: https://schemas.hathitrust.org/EF_Schema_FeaturesSubSchema_v_3.0\naccess_rights: ic\ncategory: Asia\ngenre_ld: ['http://id.loc.gov/vocabulary/marcgt/doc', 'http://id.loc.gov/vocabulary/marcgt/gov']\ngenre: ['document (computer)', 'government publication']\ncontributor_ld: {'id': 'http://www.viaf.org/viaf/128446232', 'type': \n'http://id.loc.gov/ontologies/bibframe/Organization', 'name': \"Permanent Organization for Afro-Asian Peoples' \nSolidarity.\"}\ncontributor: Permanent Organization for Afro-Asian Peoples' Solidarity.\nhandle_url: http://hdl.handle.net/2027/uc1.c2693406, http://hdl.handle.net/2027/inu.32000013032851, \nhttp://hdl.handle.net/2027/inu.32000013032844\nsource_institution_ld: {'type': 'http://id.loc.gov/ontologies/bibframe/Organization', 'name': 'NRLF'}, {'type': \n'http://id.loc.gov/ontologies/bibframe/Organization', 'name': 'INU'}\nsource_institution: INU, NRLF\nlcc: DS33.3.A34\ntype: ['DataFeedItem', 'PublicationVolume']\nis_part_of: {'id': 'http://www.worldcat.org/oclc/2240391', 'type': 'CreativeWorkSeries', 'journalTitle': \n'Afro-Asian peoples.'}\nlast_rights_update_date: 20181112.0, 20180601.0, 20180629.0\npub_place_ld: {'id': 'http://id.loc.gov/vocabulary/countries/ua', 'type': \n'http://id.loc.gov/ontologies/bibframe/Place', 'name': 'Egypt'}\npub_place: Egypt\nmain_entity_of_page: ['https://catalog.hathitrust.org/Record/006064527', \n'http://catalog.hathitrust.org/api/volumes/brief/oclc/2240391.json', \n'http://catalog.hathitrust.org/api/volumes/full/oclc/2240391.json']\nlowercase_periodical_name: afro_asian_peoples\npublication_directory: datasets/ht_ef_datasets/afro_asian_peoples\nvolume_directory: inu_32000013032844, inu_32000013032851, uc1_c2693406\nht_bib_key: 6064527.0\npublishDates: 1964.0\n974_y: 1969.0\nlastUpdate: 20230305.0, 20230227.0, 20240427.0\nenumcron: 1969-70, v.12,no.1,3,5 1970, v.11,no.2-4 1969\nmin_year: 1969.0\nmax_year: 1970.0\nextracted_year: 1969.0, 1970.0\nfinalized_year: 1969.0, 1970.0\nprocessed_finalized_year: 1969-01-01, 1970-01-01\nknowledge: Afro Asian Peoples is a monthly magazine published in London, UK. It focuses on news and issues relevant\nto African and Asian communities in the UK and worldwide. The magazine covers topics such as politics, culture, \narts, and social justice.\nPlease create a brief human-readable description including title, author, publication date, and subjects.\n\n\n\nDescription response: {'model': 'llama3.2', 'created_at': '2024-11-14T18:38:49.944094Z', 'message': {'role': \n'assistant', 'content': \"{'description': 'Afro-Asian Peoples is a monthly magazine that focuses on news and issues \nrelevant to African and Asian communities in the UK and worldwide. It covers topics such as politics, culture, \narts, and social justice, published from 1969-1970. The Permanent Organization for Afro-Asian Peoples'' Solidarity \nis the main contributor.'}\"}, 'done_reason': 'stop', 'done': True, 'total_duration': 18469203625, 'load_duration': \n31440375, 'prompt_eval_count': 1211, 'prompt_eval_duration': 12022795000, 'eval_count': 73, 'eval_duration': \n6408576000}\n\n\n\nDescription: Afro-Asian Peoples is a monthly magazine that focuses on news and issues relevant to African and Asian\ncommunities in the UK and worldwide. It covers topics such as politics, culture, arts, and social justice, \npublished from 1969-1970. The Permanent Organization for Afro-Asian Peoples Solidarity is the main contributor.\n\n\n\nClassification: News & Politics Magazine\n\n\n\nActual classification: News/politics magazines\n\n\n\n\n\n\nProcessing periodical Afro Asian Women\n\n\n\nKnowledge check content: Based on this periodical name: Afro Asian Women, what do you know about this publication? \n\n\n\nKnowledge check response: {'model': 'llama3.2', 'created_at': '2024-11-14T18:39:00.116937Z', 'message': {'role': \n'assistant', 'content': '{\\'knowledge\\':\\n  \"Afro Asian Women is a quarterly magazine that focuses on issues of \ninterest to Black women and women of African descent living in Asia. It was founded by Sowon Park in 1986 and \ncovers topics such as social justice, politics, culture, and lifestyle. The publication features articles, \nprofiles, and reviews, as well as news from the Asian diaspora community.\"}'}, 'done_reason': 'stop', 'done': True,\n'total_duration': 6993233958, 'load_duration': 32001875, 'prompt_eval_count': 87, 'prompt_eval_duration': \n993704000, 'eval_count': 81, 'eval_duration': 5965059000}\n\n\n\nKnowledge: Afro Asian Women is a quarterly magazine that focuses on issues of interest to Black women and women of \nAfrican descent living in Asia. It was founded by Sowon Park in 1986 and covers topics such as social justice, \npolitics, culture, and lifestyle. The publication features articles, profiles, and reviews, as well as news from \nthe Asian diaspora community.\n\n\n\nDescription content: Here is combined data from multiple MARC records and HathiTrust data:\nperiodical_name: Afro Asian Women\n001: 6064525.0, 6064524.0\nupdated_ht_bib_key: 006064525, 006064524\nlink: https://hdl.handle.net/2027/inu.32000013032984, https://hdl.handle.net/2027/inu.32000013032992, \nhttps://hdl.handle.net/2027/inu.32000013032976\nhtid: inu.32000013032984, inu.32000013032992, inu.32000013032976\ndate: 1968 May, 1967 Jan., 1964 Mar.\noriginal_source: \n                       Indiana University\n                      \nrecord_url: https://catalog.hathitrust.org/Record/006064525, https://catalog.hathitrust.org/Record/006064524\nid: inu.32000013032984, inu.32000013032992, inu.32000013032976\nmetadata_schema_version: https://schemas.hathitrust.org/EF_Schema_MetadataSubSchema_v_3.0\nenumeration_chronology: 1968 May, 1967 Jan., 1964 Mar.\ntype_of_resource: http://id.loc.gov/ontologies/bibframe/Text\ntitle: The Afro-Asian woman's bulletin., The Afro-Asian woman.\ndate_created: 20200209.0\npub_date: 1968.0, 1964.0, 1967.0\nlanguage: eng\naccess_profile: google\nlccn: sn 85020861, sn 85020860\noclc: 9670156.0, 2257029.0\npage_count: 56.0, 50.0, 42.0\nfeature_schema_version: https://schemas.hathitrust.org/EF_Schema_FeaturesSubSchema_v_3.0\naccess_rights: ic\ngenre_ld: http://id.loc.gov/vocabulary/marcgt/doc\ngenre: ['document (computer)']\ncontributor_ld: {'id': 'http://www.viaf.org/viaf/124395362', 'type': \n'http://id.loc.gov/ontologies/bibframe/Organization', 'name': \"Permanent Organization for Afro-Asian Peoples' \nSolidarity. Woman's Bureau\"}\ncontributor: Permanent Organization for Afro-Asian Peoples' Solidarity. Woman's Bureau\nhandle_url: http://hdl.handle.net/2027/inu.32000013032976, http://hdl.handle.net/2027/inu.32000013032992, \nhttp://hdl.handle.net/2027/inu.32000013032984\nsource_institution_ld: {'type': 'http://id.loc.gov/ontologies/bibframe/Organization', 'name': 'INU'}\nsource_institution: INU\ntype: ['DataFeedItem', 'PublicationVolume']\nis_part_of: {'id': 'http://www.worldcat.org/oclc/9670156', 'type': 'CreativeWorkSeries', 'journalTitle': \"The \nAfro-Asian woman's bulletin.\"}, {'id': 'http://www.worldcat.org/oclc/2257029', 'type': 'CreativeWorkSeries', \n'journalTitle': 'The Afro-Asian woman.'}\nlast_rights_update_date: 20181112.0, 20181021.0, 20181022.0\npub_place_ld: {'id': 'http://id.loc.gov/vocabulary/countries/ua', 'type': \n'http://id.loc.gov/ontologies/bibframe/Place', 'name': 'Egypt'}\npub_place: Egypt\nmain_entity_of_page: ['https://catalog.hathitrust.org/Record/006064525', \n'http://catalog.hathitrust.org/api/volumes/brief/oclc/9670156.json', \n'http://catalog.hathitrust.org/api/volumes/full/oclc/9670156.json'], \n['https://catalog.hathitrust.org/Record/006064524', \n'http://catalog.hathitrust.org/api/volumes/brief/oclc/2257029.json', \n'http://catalog.hathitrust.org/api/volumes/full/oclc/2257029.json']\npublisher_ld: {'id': \n\"http://catalogdata.library.illinois.edu/lod/entities/ProvisionActivityAgent/ht/Woman's%20Bureau%20of%20the%20Afro-\nAsian%20Peoples'%20Solidarity%20Organization\", 'type': 'http://id.loc.gov/ontologies/bibframe/Organization', \n'name': \"Woman's Bureau of the Afro-Asian Peoples' Solidarity Organization\"}, {'id': \n\"http://catalogdata.library.illinois.edu/lod/entities/ProvisionActivityAgent/ht/Woman's%20Bureau%20of%20the%20Afro-\nAsian%20Solidarity%20Organization\", 'type': 'http://id.loc.gov/ontologies/bibframe/Organization', 'name': \"Woman's \nBureau of the Afro-Asian Solidarity Organization\"}\npublisher: Woman's Bureau of the Afro-Asian Solidarity Organization, Woman's Bureau of the Afro-Asian Peoples' \nSolidarity Organization\nlowercase_periodical_name: afro_asian_women\npublication_directory: datasets/ht_ef_datasets/afro_asian_women\nvolume_directory: inu_32000013032992, inu_32000013032984, inu_32000013032976\nht_bib_key: 6064524.0, 6064525.0\npublishDates: 1960.0, 1900.0\n974_y: 1968.0, 1964.0\nlastUpdate: 20240120.0, 20240109.0, 20230303.0\nenumcron: 1968 May, 1967 Jan., 1964 Mar.\nmin_year: 1968.0, 1964.0\nmax_year: 1968.0, 1967.0\nextracted_year: 1968.0, 1964.0, 1967.0\nfinalized_year: 1968.0, 1964.0, 1967.0\nprocessed_finalized_year: 1967-01-01, 1968-01-01, 1964-01-01\nknowledge: Afro Asian Women is a quarterly magazine that focuses on issues of interest to Black women and women of \nAfrican descent living in Asia. It was founded by Sowon Park in 1986 and covers topics such as social justice, \npolitics, culture, and lifestyle. The publication features articles, profiles, and reviews, as well as news from \nthe Asian diaspora community.\nPlease create a brief human-readable description including title, author, publication date, and subjects.\n\n\n\nDescription response: {'model': 'llama3.2', 'created_at': '2024-11-14T18:39:20.306601Z', 'message': {'role': \n'assistant', 'content': \"{'description': 'The Afro-Asian woman is a quarterly magazine focusing on social justice, \npolitics, culture, and lifestyle issues relevant to Black women and women of African descent living in Asia. It was\nfounded by Sowon Park in 1986 and covers topics such as the Asian diaspora community.'}\"}, 'done_reason': 'stop', \n'done': True, 'total_duration': 20155797625, 'load_duration': 30336166, 'prompt_eval_count': 1530, \n'prompt_eval_duration': 14896195000, 'eval_count': 63, 'eval_duration': 5221139000}\n\n\n\nDescription: The Afro-Asian woman is a quarterly magazine focusing on social justice, politics, culture, and \nlifestyle issues relevant to Black women and women of African descent living in Asia. It was founded by Sowon Park \nin 1986 and covers topics such as the Asian diaspora community.\n\n\n\nClassification: Radical Periodical\n\n\n\nActual classification: News/politics magazines\n\n\n\n\n\n\nProcessing periodical Afro Asian and World Affairs\n\n\n\nKnowledge check content: Based on this periodical name: Afro Asian and World Affairs, what do you know about this \npublication? \n\n\n\nKnowledge check response: {'model': 'llama3.2', 'created_at': '2024-11-14T18:39:27.761936Z', 'message': {'role': \n'assistant', 'content': '{\\'knowledge\\':\\n {\"Afro-Asian and World Affairs\": \"A peer-reviewed English-language \njournal published by Cambridge University Press that focuses on contemporary issues of the African, Asian, and \nMiddle Eastern regions, offering analysis from a broad range of disciplinary perspectives.\"}}'}, 'done_reason': \n'stop', 'done': True, 'total_duration': 4412874375, 'load_duration': 65924792, 'prompt_eval_count': 89, \n'prompt_eval_duration': 783755000, 'eval_count': 52, 'eval_duration': 3560743000}\n\n\n\nKnowledge: {'Afro-Asian and World Affairs': 'A peer-reviewed English-language journal published by Cambridge \nUniversity Press that focuses on contemporary issues of the African, Asian, and Middle Eastern regions, offering \nanalysis from a broad range of disciplinary perspectives.'}\n\n\n\nDescription content: Here is combined data from multiple MARC records and HathiTrust data:\nperiodical_name: Afro Asian and World Affairs\n001: 0495647.0\nupdated_ht_bib_key: 000495647\nlink: https://hdl.handle.net/2027/mdp.39015035518110, https://hdl.handle.net/2027/mdp.39015035518128, \nhttps://hdl.handle.net/2027/mdp.39015035518102\nhtid: mdp.39015035518110, mdp.39015035518128, mdp.39015035518102\ndate: v.5 no.1 1968, v.4 1967, v.1-3 1964\noriginal_source: \n                       University of Michigan\n                      \nrecord_url: https://catalog.hathitrust.org/Record/000495647\nid: mdp.39015035518110, mdp.39015035518128, mdp.39015035518102\nmetadata_schema_version: https://schemas.hathitrust.org/EF_Schema_MetadataSubSchema_v_3.0\nenumeration_chronology: v.5 no.1 1968, v.4 1967, v.1-3 1964\ntype_of_resource: http://id.loc.gov/ontologies/bibframe/Text\ntitle: Afro-Asian and world affairs.\ndate_created: 20200209.0\npub_date: 1968.0, 1964.0, 1967.0\nlanguage: eng\naccess_profile: google\nissn: 0002-0605\nlccn: sa 66007754\noclc: 1773608.0\npage_count: 1144.0, 466.0, 106.0\nfeature_schema_version: https://schemas.hathitrust.org/EF_Schema_FeaturesSubSchema_v_3.0\naccess_rights: ic\ncategory: Asia\ngenre_ld: http://id.loc.gov/vocabulary/marcgt/doc\ngenre: ['document (computer)']\ncontributor_ld: {'id': 'http://www.viaf.org/viaf/134777751', 'type': \n'http://id.loc.gov/ontologies/bibframe/Organization', 'name': 'Institute of Afro-Asian and World Affairs.'}\ncontributor: Institute of Afro-Asian and World Affairs.\nhandle_url: http://hdl.handle.net/2027/mdp.39015035518110, http://hdl.handle.net/2027/mdp.39015035518102, \nhttp://hdl.handle.net/2027/mdp.39015035518128\nsource_institution_ld: {'type': 'http://id.loc.gov/ontologies/bibframe/Organization', 'name': 'MIU'}\nsource_institution: MIU\nlcc: DS1.A28\ntype: ['DataFeedItem', 'PublicationVolume']\nis_part_of: {'id': 'http://www.worldcat.org/oclc/1773608', 'type': 'CreativeWorkSeries', 'journalTitle': \n'Afro-Asian and world affairs.'}\nlast_rights_update_date: 20161130.0, 20161020.0, 20170130.0\npub_place_ld: {'id': 'http://id.loc.gov/vocabulary/countries/ii', 'type': \n'http://id.loc.gov/ontologies/bibframe/Place', 'name': 'India'}\npub_place: India\nmain_entity_of_page: ['https://catalog.hathitrust.org/Record/000495647', \n'http://catalog.hathitrust.org/api/volumes/brief/oclc/1773608.json', \n'http://catalog.hathitrust.org/api/volumes/full/oclc/1773608.json']\npublisher_ld: {'id': \n'http://catalogdata.library.illinois.edu/lod/entities/ProvisionActivityAgent/ht/Institute%20of%20Afro-Asian%20and%2\n0World%20Affairs.', 'type': 'http://id.loc.gov/ontologies/bibframe/Organization', 'name': 'Institute of Afro-Asian \nand World Affairs.'}\npublisher: Institute of Afro-Asian and World Affairs.\nlowercase_periodical_name: afro_asian_and_world_affairs\npublication_directory: datasets/ht_ef_datasets/afro_asian_and_world_affairs\nvolume_directory: mdp_39015035518110, mdp_39015035518102, mdp_39015035518128\nht_bib_key: 495647.0\npublishDates: 1964.0\n974_y: 1968.0\nlastUpdate: 20221217.0, 20230417.0, 20221215.0\nenumcron: v.5 no.1 1968, v.4 1967, v.1-3 1964\nmin_year: 1964.0\nmax_year: 1968.0\nextracted_year: 1968.0, 1964.0, 1967.0\nfinalized_year: 1968.0, 1964.0, 1967.0\nprocessed_finalized_year: 1967-01-01, 1968-01-01, 1964-01-01\nknowledge: {'Afro-Asian and World Affairs': 'A peer-reviewed English-language journal published by Cambridge \nUniversity Press that focuses on contemporary issues of the African, Asian, and Middle Eastern regions, offering \nanalysis from a broad range of disciplinary perspectives.'}\nPlease create a brief human-readable description including title, author, publication date, and subjects.\n\n\n\nDescription response: {'model': 'llama3.2', 'created_at': '2024-11-14T18:39:46.375509Z', 'message': {'role': \n'assistant', 'content': '{\"description\": \"Afro-Asian and World Affairs is a peer-reviewed English-language journal \npublished by Cambridge University Press that focuses on contemporary issues of the African, Asian, and Middle \nEastern regions. Published from 1964 to 1968, it offers analysis from a broad range of disciplinary perspectives. \nTopics include politics, economy, culture, and social issues affecting these regions.\"}'}, 'done_reason': 'stop', \n'done': True, 'total_duration': 18586694709, 'load_duration': 32313000, 'prompt_eval_count': 1273, \n'prompt_eval_duration': 12100109000, 'eval_count': 77, 'eval_duration': 6448663000}\n\n\n\nDescription: Afro-Asian and World Affairs is a peer-reviewed English-language journal published by Cambridge \nUniversity Press that focuses on contemporary issues of the African, Asian, and Middle Eastern regions. Published \nfrom 1964 to 1968, it offers analysis from a broad range of disciplinary perspectives. Topics include politics, \neconomy, culture, and social issues affecting these regions.\n\n\n\nClassification: Information Bulletin\n\n\n\nActual classification: News/politics magazines\n\n\n\n\n\n\nProcessing periodical Arab Observer and The Scribe\n\n\n\nKnowledge check content: Based on this periodical name: Arab Observer and The Scribe, what do you know about this \npublication? \n\n\n\nKnowledge check response: {'model': 'llama3.2', 'created_at': '2024-11-14T18:39:57.179685Z', 'message': {'role': \n'assistant', 'content': '{\\'knowledge\\':\\n{\\n\"Arab Observer\": {\\n\"Type\": \"Newspaper\",\\n\"Language\": \n\"English\",\\n\"Circulation\": \"Regional\",\\n\"Frequency\": \"Weekly\",\\n\"Focus\": \"Middle East news, analysis and \nopinion\"\\n},\\n\"The Scribe\": {\\n\"Type\": \"Magazine\",\\n\"Language\": \"English\",\\n\"Circulation\": \n\"Regional\",\\n\"Frequency\": \"Monthly\",\\n\"Focus\": \"Culture, lifestyle and social issues in the Middle East\"\\n}\\n}}'}, \n'done_reason': 'stop', 'done': True, 'total_duration': 7871306625, 'load_duration': 35827916, 'prompt_eval_count': \n90, 'prompt_eval_duration': 827027000, 'eval_count': 99, 'eval_duration': 7005037000}\n\n\n\nKnowledge: {'Arab Observer': {'Type': 'Newspaper', 'Language': 'English', 'Circulation': 'Regional', 'Frequency': \n'Weekly', 'Focus': 'Middle East news, analysis and opinion'}, 'The Scribe': {'Type': 'Magazine', 'Language': \n'English', 'Circulation': 'Regional', 'Frequency': 'Monthly', 'Focus': 'Culture, lifestyle and social issues in the\nMiddle East'}}\n\n\n\nDescription content: Here is combined data from multiple MARC records and HathiTrust data:\nperiodical_name: Arab Observer and The Scribe\n001: 0679918.0, 6064523.0\nupdated_ht_bib_key: 006064523, 000679918\nlink: https://hdl.handle.net/2027/uc1.l0105076285, https://hdl.handle.net/2027/mdp.39015056038071, \nhttps://hdl.handle.net/2027/uc1.l0063473003, https://hdl.handle.net/2027/mdp.39015056038089, \nhttps://hdl.handle.net/2027/mdp.39015056038386, https://hdl.handle.net/2027/uc1.l0082987116, \nhttps://hdl.handle.net/2027/inu.32000013025194, https://hdl.handle.net/2027/mdp.39015056038253, \nhttps://hdl.handle.net/2027/inu.32000013025046, https://hdl.handle.net/2027/uc1.l0082986993, \nhttps://hdl.handle.net/2027/uc1.l0063472849, https://hdl.handle.net/2027/mdp.39015056038410, \nhttps://hdl.handle.net/2027/mdp.39015056038212, https://hdl.handle.net/2027/uc1.l0082987033, \nhttps://hdl.handle.net/2027/mdp.39015056038063, https://hdl.handle.net/2027/mdp.39015056038402, \nhttps://hdl.handle.net/2027/mdp.39015056038204, https://hdl.handle.net/2027/uc1.l0082987074, \nhttps://hdl.handle.net/2027/mdp.39015056038220, https://hdl.handle.net/2027/inu.32000013025095, \nhttps://hdl.handle.net/2027/inu.32000013025129, https://hdl.handle.net/2027/mdp.39015056038378, \nhttps://hdl.handle.net/2027/inu.32000013025111, https://hdl.handle.net/2027/inu.32000013024635, \nhttps://hdl.handle.net/2027/uc1.l0082987157, https://hdl.handle.net/2027/mdp.39015056038394, \nhttps://hdl.handle.net/2027/inu.32000013025087, https://hdl.handle.net/2027/inu.32000013025137, \nhttps://hdl.handle.net/2027/uc1.l0063472831, https://hdl.handle.net/2027/mdp.39015056038246, \nhttps://hdl.handle.net/2027/inu.32000013025079, https://hdl.handle.net/2027/inu.32000013025152, \nhttps://hdl.handle.net/2027/inu.32000013025160, https://hdl.handle.net/2027/inu.32000013025145, \nhttps://hdl.handle.net/2027/mdp.39015056038352, https://hdl.handle.net/2027/inu.32000013025061, \nhttps://hdl.handle.net/2027/inu.32000013025103, https://hdl.handle.net/2027/uc1.l0073177743, \nhttps://hdl.handle.net/2027/mdp.39015056038238, https://hdl.handle.net/2027/inu.32000013024627, \nhttps://hdl.handle.net/2027/inu.32000013025053\nhtid: mdp.39015056038204, inu.32000013025061, uc1.l0082987116, inu.32000013025152, inu.32000013024627, \nmdp.39015056038386, mdp.39015056038378, mdp.39015056038212, uc1.l0082986993, inu.32000013025160, uc1.l0063473003, \ninu.32000013025079, uc1.l0063472831, mdp.39015056038352, inu.32000013025137, inu.32000013025053, \nmdp.39015056038089, inu.32000013025145, inu.32000013025111, mdp.39015056038253, uc1.l0082987033, \ninu.32000013025087, mdp.39015056038246, inu.32000013025095, inu.32000013025129, uc1.l0082987074, \ninu.32000013024635, mdp.39015056038238, uc1.l0073177743, uc1.l0105076285, mdp.39015056038071, mdp.39015056038394, \nmdp.39015056038220, mdp.39015056038063, mdp.39015056038402, mdp.39015056038410, inu.32000013025194, \ninu.32000013025046, uc1.l0063472849, uc1.l0082987157, inu.32000013025103\ndate: no.315-326 1966, 1963 Jan-Mar, no.237-249 1965, 1962 Jul-Sep, 1963 Apr-Jun, 1964 Jan-Apr, 1963 Oct-Dec, \nyr.1962 mo.JAN-JUN, yr.1961 mo.JUL-AUG, no.133-144 1963, 1966 May-Aug, no.172-184 1963, no.211-223 1964, no.276-288\n1965, yr.1962 mo.JULY-DEC, no.185-197 1964, 1963 Jul-Sep, 1966 Sep-Oct, no.250-262 1965, 1965 Jan-Apr, no.224-236 \n1964, 1961 Dec, no.263-275 1965, 1962 Oct-Dec, no.289-301 1966, yr.1960 mo.OCT30-NOV13, no.327-331 1966, yr.1961 \nmo.FEB26-MAR12, yr.1961 mo.SEP, 1965 May-Aug, yr.1961 mo.JUL, 1962 Jan-Jun, yr.1960 mo.OCT2, no.80-92 1962, \nno.302-314 1966, yr.1961 mo.JAN 15-22, no.198-210 inc. 1964, 1964 Sep-Dec, 1964 May-Aug, yr.1961 mo.JAN1-8, \nno.93-105 1962\noriginal_source: \n                       University of California\n                      , \n                       University of Michigan\n                      , \n                       Indiana University\n                      \nrecord_url: https://catalog.hathitrust.org/Record/000679918, https://catalog.hathitrust.org/Record/006064523\nid: mdp.39015056038204, inu.32000013025061, inu.32000013025152, inu.32000013024627, mdp.39015056038386, \nmdp.39015056038378, mdp.39015056038212, uc1.l0082986993, inu.32000013025160, uc1.l0063473003, inu.32000013025079, \nuc1.l0063472831, mdp.39015056038352, inu.32000013025137, inu.32000013025053, mdp.39015056038089, \ninu.32000013025145, inu.32000013025111, mdp.39015056038253, uc1.l0082987033, inu.32000013025087, \nmdp.39015056038246, inu.32000013025095, inu.32000013025129, inu.32000013024635, mdp.39015056038238, \nuc1.l0073177743, mdp.39015056038071, mdp.39015056038394, mdp.39015056038220, mdp.39015056038063, \nmdp.39015056038402, mdp.39015056038410, inu.32000013025194, inu.32000013025046, inu.32000013025103\nmetadata_schema_version: https://schemas.hathitrust.org/EF_Schema_MetadataSubSchema_v_3.0\nenumeration_chronology: no.315-326 1966, 1963 Jan-Mar, no.237-249 1965, 1962 Jul-Sep, 1963 Apr-Jun, 1964 Jan-Apr, \n1963 Oct-Dec, yr.1962 mo.JAN-JUN, yr.1961 mo.JUL-AUG, no.133-144 1963, 1966 May-Aug, no.172-184 1963, no.211-223 \n1964, no.276-288 1965, no.185-197 1964, 1963 Jul-Sep, 1966 Sep-Oct, no.250-262 1965, 1965 Jan-Apr, no.224-236 1964,\n1961 Dec, no.263-275 1965, 1962 Oct-Dec, no.289-301 1966, yr.1960 mo.OCT30-NOV13, no.327-331 1966, 1965 May-Aug, \nyr.1961 mo.JUL, 1962 Jan-Jun, yr.1960 mo.OCT2, no.80-92 1962, no.302-314 1966, no.198-210 inc. 1964, 1964 Sep-Dec, \n1964 May-Aug, no.93-105 1962\ntype_of_resource: http://id.loc.gov/ontologies/bibframe/Text\ntitle: Arab observer., Arab observer and the scribe.\ndate_created: 20200209.0\npub_date: 1960.0, 1961.0, 1962.0, 1963.0, 1964.0, 1965.0, 1966.0\nlanguage: eng\naccess_profile: google\nlccn: ne 64000529, sc 83008239\noclc: 4927825.0, 6616932.0\npage_count: 650.0, 670.0, 680.0, 938.0, 44.0, 946.0, 818.0, 948.0, 312.0, 698.0, 700.0, 962.0, 708.0, 964.0, 838.0,\n966.0, 712.0, 714.0, 76.0, 78.0, 722.0, 728.0, 474.0, 480.0, 740.0, 742.0, 746.0, 114.0, 116.0, 1020.0\nfeature_schema_version: https://schemas.hathitrust.org/EF_Schema_FeaturesSubSchema_v_3.0\naccess_rights: ic\nalternate_title: ['Arab observer']\ncategory: Periodicals\ngenre_ld: ['http://id.loc.gov/vocabulary/marcgt/doc', 'urn:uuid:dde6e056-0a8c-488f-9909-82d90383c265'], \n['http://id.loc.gov/vocabulary/marcgt/doc', 'urn:uuid:97f77231-deef-4dd7-ab4b-833ffd9284b4'], \n['http://id.loc.gov/vocabulary/marcgt/doc', 'urn:uuid:10bb298c-0a6e-4b8a-bd3e-fa5cc656add8'], \n['http://id.loc.gov/vocabulary/marcgt/doc', 'urn:uuid:b73810cb-0155-43e9-9c6a-135b993577b9'], \n['http://id.loc.gov/vocabulary/marcgt/doc', 'urn:uuid:c23a173b-6a52-451e-85af-7532a75f865a'], \n['http://id.loc.gov/vocabulary/marcgt/doc', 'urn:uuid:01f00e43-4ead-4cd4-93ed-92c4c2627bf8'], \n['http://id.loc.gov/vocabulary/marcgt/doc', 'urn:uuid:872c566b-32f5-4044-8c8d-387ad6fa9a52'], \n['http://id.loc.gov/vocabulary/marcgt/doc', 'urn:uuid:e9b7664d-1ff5-4f71-b29d-59d159b43079'], \n['http://id.loc.gov/vocabulary/marcgt/doc', 'urn:uuid:4f571384-8e67-4237-b1a7-84e67c024857'], \n['http://id.loc.gov/vocabulary/marcgt/doc', 'urn:uuid:08d83de6-c94a-4222-919a-e071a7c71220'], \n['http://id.loc.gov/vocabulary/marcgt/doc', 'urn:uuid:5bf42491-3399-4883-a027-b9df7a86a606'], \n['http://id.loc.gov/vocabulary/marcgt/doc', 'urn:uuid:17f87339-8756-474a-87da-b29cc91fd5a0'], \n['http://id.loc.gov/vocabulary/marcgt/doc', 'urn:uuid:aa9d7bf0-242f-4a40-bfad-8f6abf3ba819'], \n['http://id.loc.gov/vocabulary/marcgt/doc', 'urn:uuid:e7d03221-8e6c-4cdd-8044-489f3b4665c2'], \nhttp://id.loc.gov/vocabulary/marcgt/doc, ['http://id.loc.gov/vocabulary/marcgt/doc', \n'urn:uuid:68f7e5d1-f495-40d0-8bbc-5a57456238e1'], ['http://id.loc.gov/vocabulary/marcgt/doc', \n'urn:uuid:14d4afcc-adc1-4b8b-9d20-ff97650d6e12'], ['http://id.loc.gov/vocabulary/marcgt/doc', \n'urn:uuid:11af7d8b-8f03-498f-8ed2-994485c93244'], ['http://id.loc.gov/vocabulary/marcgt/doc', \n'urn:uuid:4c05ef15-2310-4790-a18b-f54216a9abbd'], ['http://id.loc.gov/vocabulary/marcgt/doc', \n'urn:uuid:f588c2dc-0251-4031-9de1-9705d22c0ad7'], ['http://id.loc.gov/vocabulary/marcgt/doc', \n'urn:uuid:28919d4f-de30-412f-9285-e9780ffd2938'], ['http://id.loc.gov/vocabulary/marcgt/doc', \n'urn:uuid:06e758f9-c6f6-4305-b108-b7e663e11fd5'], ['http://id.loc.gov/vocabulary/marcgt/doc', \n'urn:uuid:b34fbfb9-18cb-441a-b8ef-e83fc0c714a2'], ['http://id.loc.gov/vocabulary/marcgt/doc', \n'urn:uuid:2957cd85-8e5b-45b7-b0e7-e16db98553cf'], ['http://id.loc.gov/vocabulary/marcgt/doc', \n'urn:uuid:fde45dea-1616-4930-832c-8fd5e64f78bd'], ['http://id.loc.gov/vocabulary/marcgt/doc', \n'urn:uuid:0d21d812-ced4-4aa4-b673-09d85e2c6c4e'], ['http://id.loc.gov/vocabulary/marcgt/doc', \n'urn:uuid:0477eabb-fbec-4439-af4f-f0ddfb15a5fe'], ['http://id.loc.gov/vocabulary/marcgt/doc', \n'urn:uuid:eead994f-9e5c-47ad-8d66-b08837706625'], ['http://id.loc.gov/vocabulary/marcgt/doc', \n'urn:uuid:5f43aa6d-e275-4e2b-9a69-33e41acb7bd7'], ['http://id.loc.gov/vocabulary/marcgt/doc', \n'urn:uuid:47549c35-6e34-472b-b98f-14bf623071ea'], ['http://id.loc.gov/vocabulary/marcgt/doc', \n'urn:uuid:05ad3e90-5302-400e-a487-d3389d735979']\ngenre: ['document (computer)', 'urn:uuid:5bf42491-3399-4883-a027-b9df7a86a606'], ['document (computer)', \n'urn:uuid:e7d03221-8e6c-4cdd-8044-489f3b4665c2'], ['document (computer)', \n'urn:uuid:05ad3e90-5302-400e-a487-d3389d735979'], ['document (computer)', \n'urn:uuid:872c566b-32f5-4044-8c8d-387ad6fa9a52'], ['document (computer)'], ['document (computer)', \n'urn:uuid:47549c35-6e34-472b-b98f-14bf623071ea'], ['document (computer)', \n'urn:uuid:b34fbfb9-18cb-441a-b8ef-e83fc0c714a2'], ['document (computer)', \n'urn:uuid:4f571384-8e67-4237-b1a7-84e67c024857'], ['document (computer)', \n'urn:uuid:c23a173b-6a52-451e-85af-7532a75f865a'], ['document (computer)', \n'urn:uuid:01f00e43-4ead-4cd4-93ed-92c4c2627bf8'], ['document (computer)', \n'urn:uuid:aa9d7bf0-242f-4a40-bfad-8f6abf3ba819'], ['document (computer)', \n'urn:uuid:0d21d812-ced4-4aa4-b673-09d85e2c6c4e'], ['document (computer)', \n'urn:uuid:f588c2dc-0251-4031-9de1-9705d22c0ad7'], ['document (computer)', \n'urn:uuid:2957cd85-8e5b-45b7-b0e7-e16db98553cf'], ['document (computer)', \n'urn:uuid:5f43aa6d-e275-4e2b-9a69-33e41acb7bd7'], ['document (computer)', \n'urn:uuid:eead994f-9e5c-47ad-8d66-b08837706625'], ['document (computer)', \n'urn:uuid:06e758f9-c6f6-4305-b108-b7e663e11fd5'], ['document (computer)', \n'urn:uuid:97f77231-deef-4dd7-ab4b-833ffd9284b4'], ['document (computer)', \n'urn:uuid:14d4afcc-adc1-4b8b-9d20-ff97650d6e12'], ['document (computer)', \n'urn:uuid:08d83de6-c94a-4222-919a-e071a7c71220'], ['document (computer)', \n'urn:uuid:dde6e056-0a8c-488f-9909-82d90383c265'], ['document (computer)', \n'urn:uuid:fde45dea-1616-4930-832c-8fd5e64f78bd'], ['document (computer)', \n'urn:uuid:28919d4f-de30-412f-9285-e9780ffd2938'], ['document (computer)', \n'urn:uuid:e9b7664d-1ff5-4f71-b29d-59d159b43079'], ['document (computer)', \n'urn:uuid:4c05ef15-2310-4790-a18b-f54216a9abbd'], ['document (computer)', \n'urn:uuid:11af7d8b-8f03-498f-8ed2-994485c93244'], ['document (computer)', \n'urn:uuid:17f87339-8756-474a-87da-b29cc91fd5a0'], ['document (computer)', \n'urn:uuid:68f7e5d1-f495-40d0-8bbc-5a57456238e1'], ['document (computer)', \n'urn:uuid:b73810cb-0155-43e9-9c6a-135b993577b9'], ['document (computer)', \n'urn:uuid:10bb298c-0a6e-4b8a-bd3e-fa5cc656add8'], ['document (computer)', \n'urn:uuid:0477eabb-fbec-4439-af4f-f0ddfb15a5fe']\nhandle_url: http://hdl.handle.net/2027/inu.32000013025053, http://hdl.handle.net/2027/inu.32000013025061, \nhttp://hdl.handle.net/2027/mdp.39015056038402, http://hdl.handle.net/2027/mdp.39015056038089, \nhttp://hdl.handle.net/2027/uc1.l0073177743, http://hdl.handle.net/2027/mdp.39015056038246, \nhttp://hdl.handle.net/2027/inu.32000013024635, http://hdl.handle.net/2027/inu.32000013025103, \nhttp://hdl.handle.net/2027/mdp.39015056038071, http://hdl.handle.net/2027/mdp.39015056038238, \nhttp://hdl.handle.net/2027/mdp.39015056038212, http://hdl.handle.net/2027/inu.32000013025087, \nhttp://hdl.handle.net/2027/inu.32000013025129, http://hdl.handle.net/2027/uc1.l0082986993, \nhttp://hdl.handle.net/2027/mdp.39015056038394, http://hdl.handle.net/2027/mdp.39015056038410, \nhttp://hdl.handle.net/2027/mdp.39015056038352, http://hdl.handle.net/2027/inu.32000013025137, \nhttp://hdl.handle.net/2027/inu.32000013025194, http://hdl.handle.net/2027/inu.32000013025111, \nhttp://hdl.handle.net/2027/inu.32000013025046, http://hdl.handle.net/2027/mdp.39015056038204, \nhttp://hdl.handle.net/2027/mdp.39015056038253, http://hdl.handle.net/2027/mdp.39015056038063, \nhttp://hdl.handle.net/2027/uc1.l0063473003, http://hdl.handle.net/2027/inu.32000013025079, \nhttp://hdl.handle.net/2027/inu.32000013024627, http://hdl.handle.net/2027/inu.32000013025095, \nhttp://hdl.handle.net/2027/inu.32000013025152, http://hdl.handle.net/2027/inu.32000013025160, \nhttp://hdl.handle.net/2027/mdp.39015056038386, http://hdl.handle.net/2027/uc1.l0063472831, \nhttp://hdl.handle.net/2027/mdp.39015056038220, http://hdl.handle.net/2027/mdp.39015056038378, \nhttp://hdl.handle.net/2027/uc1.l0082987033, http://hdl.handle.net/2027/inu.32000013025145\nsource_institution_ld: {'type': 'http://id.loc.gov/ontologies/bibframe/Organization', 'name': 'UCLA'}, {'type': \n'http://id.loc.gov/ontologies/bibframe/Organization', 'name': 'INU'}, {'type': \n'http://id.loc.gov/ontologies/bibframe/Organization', 'name': 'MIU'}\nsource_institution: INU, UCLA, MIU\nlcc: AP9.A68\ntype: ['DataFeedItem', 'PublicationVolume']\nis_part_of: {'id': 'http://www.worldcat.org/oclc/4927825', 'type': 'CreativeWorkSeries', 'journalTitle': 'Arab \nobserver.'}, {'id': 'http://www.worldcat.org/oclc/6616932', 'type': 'CreativeWorkSeries', 'journalTitle': 'Arab \nobserver and the scribe.'}\nlast_rights_update_date: 20180312.0, 20171015.0, 20170108.0, 20181020.0, 20191022.0, 20191024.0, 20200112.0, \n20181111.0, 20200115.0, 20200117.0, 20180310.0, 20161015.0, 20181112.0, 20180601.0, 20191129.0, 20170107.0, \n20191228.0, 20181022.0\npub_place_ld: {'id': 'http://id.loc.gov/vocabulary/countries/ua', 'type': \n'http://id.loc.gov/ontologies/bibframe/Place', 'name': 'Egypt'}\npub_place: Egypt\nmain_entity_of_page: ['https://catalog.hathitrust.org/Record/000679918820599-1', \n'http://catalog.hathitrust.org/api/volumes/brief/oclc/4927825.json', \n'http://catalog.hathitrust.org/api/volumes/full/oclc/4927825.json'], \n['https://catalog.hathitrust.org/Record/006064523', \n'http://catalog.hathitrust.org/api/volumes/brief/oclc/6616932.json', \n'http://catalog.hathitrust.org/api/volumes/full/oclc/6616932.json']\npublisher_ld: {'id': \n'http://catalogdata.library.illinois.edu/lod/entities/ProvisionActivityAgent/ht/National%20Publications%20House', \n'type': 'http://id.loc.gov/ontologies/bibframe/Organization', 'name': 'National Publications House'}\npublisher: National Publications House\nlowercase_periodical_name: arab_observer_and_the_scribe\npublication_directory: datasets/ht_ef_datasets/arab_observer_and_the_scribe\nvolume_directory: mdp_39015056038238, inu_32000013025111, uc1_l0063472831, inu_32000013025079, inu_32000013025061, \ninu_32000013025103, uc1_l0063473003, inu_32000013025129, inu_32000013024635, mdp_39015056038378, \ninu_32000013025160, inu_32000013025087, inu_32000013025046, mdp_39015056038253, mdp_39015056038063, \ninu_32000013024627, uc1_l0082987033, mdp_39015056038071, mdp_39015056038089, inu_32000013025053, \nmdp_39015056038220, mdp_39015056038410, mdp_39015056038394, inu_32000013025194, inu_32000013025152, \nmdp_39015056038386, mdp_39015056038246, uc1_l0082986993, inu_32000013025145, mdp_39015056038402, \ninu_32000013025095, inu_32000013025137, mdp_39015056038352, uc1_l0073177743, mdp_39015056038212, mdp_39015056038204\nht_bib_key: 6064523.0, 679918.0\npublishDates: 1960.0, 1965.0\n974_y: 1966.0\nlastUpdate: 20240911.0, 20230812.0, 20221215.0, 20230303.0, 20230305.0, 20240818.0, 20230707.0, 20240820.0, \n20230709.0, 20240819.0, 20230713.0, 20230329.0, 20231228.0, 20240317.0, 20231229.0, 20240318.0, 20231121.0, \n20230227.0, 20230618.0, 20241002.0, 20241009.0, 20241010.0, 20241012.0, 20241015.0, 20240120.0, 20230911.0\nenumcron: no.315-326 1966, 1963 Jan-Mar, no.237-249 1965, 1962 Jul-Sep, 1963 Apr-Jun, 1964 Jan-Apr, 1963 Oct-Dec, \nyr.1962 mo.JAN-JUN, yr.1961 mo.JUL-AUG, no.133-144 1963, 1966 May-Aug, no.172-184 1963, no.211-223 1964, no.276-288\n1965, yr.1962 mo.JULY-DEC, no.185-197 1964, 1963 Jul-Sep, 1966 Sep-Oct, no.250-262 1965, 1965 Jan-Apr, no.224-236 \n1964, 1961 Dec, no.263-275 1965, 1962 Oct-Dec, no.289-301 1966, yr.1960 mo.OCT30-NOV13, no.327-331 1966, yr.1961 \nmo.FEB26-MAR12, yr.1961 mo.SEP, 1965 May-Aug, yr.1961 mo.JUL, 1962 Jan-Jun, yr.1960 mo.OCT2, no.80-92 1962, \nno.302-314 1966, yr.1961 mo.JAN 15-22, no.198-210 inc. 1964, 1964 Sep-Dec, 1964 May-Aug, yr.1961 mo.JAN1-8, \nno.93-105 1962\nmin_year: 1960.0, 1965.0\nmax_year: 1966.0\nextracted_year: 1960.0, 1961.0, 1962.0, 1963.0, 1964.0, 1965.0, 1966.0\nfinalized_year: 1960.0, 1961.0, 1962.0, 1963.0, 1964.0, 1965.0, 1966.0\nprocessed_finalized_year: 1960-01-01, 1965-01-01, 1963-01-01, 1964-01-01, 1966-01-01, 1961-01-01, 1962-01-01\nknowledge: {'Arab Observer': {'Type': 'Newspaper', 'Language': 'English', 'Circulation': 'Regional', 'Frequency': \n'Weekly', 'Focus': 'Middle East news, analysis and opinion'}, 'The Scribe': {'Type': 'Magazine', 'Language': \n'English', 'Circulation': 'Regional', 'Frequency': 'Monthly', 'Focus': 'Culture, lifestyle and social issues in the\nMiddle East'}}\nPlease create a brief human-readable description including title, author, publication date, and subjects.\n\n\n\nDescription response: {'model': 'llama3.2', 'created_at': '2024-11-14T18:40:16.261692Z', 'message': {'role': \n'assistant', 'content': 'Here is a brief human-readable description based on the provided data:\\n\\n**Title:** \nMiddle East Newspaper Archive\\n**Author:** Unknown\\n**Publication Date:** 1960-2026 (varies)\\n**Subjects:**\\n\\n* \nNews from the Middle East region\\n* Analysis and opinion on regional issues\\n* Culture, lifestyle, and social \nissues in the Middle East\\n\\nNote that the publication date range is quite broad, indicating that the archive \ncontains articles from various years between 1960 and 2026.'}, 'done_reason': 'stop', 'done': True, \n'total_duration': 19018725416, 'load_duration': 35491666, 'prompt_eval_count': 1026, 'prompt_eval_duration': \n10928021000, 'eval_count': 102, 'eval_duration': 8041269000}\n\n\n\nError parsing JSON response for description.\n\n\n\nClassification: Radical Periodical\n\n\n\nActual classification: News/politics magazines\n\n\n\n\n\n\nProcessing periodical Asia and Africa today\n\n\n\nKnowledge check content: Based on this periodical name: Asia and Africa today, what do you know about this \npublication? \n\n\n\nKnowledge check response: {'model': 'llama3.2', 'created_at': '2024-11-14T18:40:26.493854Z', 'message': {'role': \n'assistant', 'content': '{\\'knowledge\\':\\n  {\\n    \"publication\": \"Asia and Africa Today\",\\n    \"category\": \n\"Newspaper\",\\n    \"language\": \"English\",\\n    \"focus\": \"Current events, politics, business, and culture from Asia \nand Africa\",\\n    \"circulation\": \"Online only\",\\n    \"frequency\": \"Daily\",\\n    \"publisher\": \"Institute for Global \nGovernance\",\\n    \"country\": \"South Africa\"\\n  }\\n}'}, 'done_reason': 'stop', 'done': True, 'total_duration': \n7374463875, 'load_duration': 35275166, 'prompt_eval_count': 88, 'prompt_eval_duration': 837273000, 'eval_count': \n89, 'eval_duration': 6499162000}\n\n\n\nKnowledge: {'publication': 'Asia and Africa Today', 'category': 'Newspaper', 'language': 'English', 'focus': \n'Current events, politics, business, and culture from Asia and Africa', 'circulation': 'Online only', 'frequency': \n'Daily', 'publisher': 'Institute for Global Governance', 'country': 'South Africa'}\n\n\n\nDescription content: Here is combined data from multiple MARC records and HathiTrust data:\nperiodical_name: Asia and Africa today\n001: 8561767.0\nupdated_ht_bib_key: 008561767\nlink: https://hdl.handle.net/2027/inu.30000004511980, https://hdl.handle.net/2027/inu.30000123002556, \nhttps://hdl.handle.net/2027/uc1.c040585625, https://hdl.handle.net/2027/uc1.l0073938409, \nhttps://hdl.handle.net/2027/uc1.b4938581, https://hdl.handle.net/2027/uc1.b4938582, \nhttps://hdl.handle.net/2027/inu.30000123002523, https://hdl.handle.net/2027/uc1.l0073935298, \nhttps://hdl.handle.net/2027/inu.30000123002549, https://hdl.handle.net/2027/umn.31951p001472021, \nhttps://hdl.handle.net/2027/inu.30000028466609, https://hdl.handle.net/2027/uc1.c050190445, \nhttps://hdl.handle.net/2027/uc1.l0054919444, https://hdl.handle.net/2027/inu.30000123002515, \nhttps://hdl.handle.net/2027/inu.30000123002507, https://hdl.handle.net/2027/inu.30000123002499\nhtid: inu.30000004511980, uc1.l0073935298, uc1.c050190445, inu.30000123002507, uc1.c040585625, uc1.b4938582, \ninu.30000123002556, uc1.l0054919444, inu.30000123002549, umn.31951p001472021, inu.30000123002515, \ninu.30000028466609, inu.30000123002523, inu.30000123002499, uc1.b4938581, uc1.l0073938409\ndate: 1977 inc., 1978, yr.1987-88, 1990, no.1-2 yr.1991, 1979, 1988, 1991, no.4-6 yr.1991, 1984, 1980, 1987, \n1991:1-4, 1983, 1981, 1989\noriginal_source: \n                       University of California\n                      , \n                       Indiana University\n                      , \n                       University of Minnesota\n                      \nrecord_url: https://catalog.hathitrust.org/Record/008561767\nid: inu.30000004511980, uc1.l0073935298, uc1.c050190445, inu.30000123002507, uc1.c040585625, uc1.b4938582, \ninu.30000123002556, uc1.l0054919444, inu.30000123002549, umn.31951p001472021, inu.30000123002515, \ninu.30000028466609, inu.30000123002523, inu.30000123002499, uc1.b4938581, uc1.l0073938409\nmetadata_schema_version: https://schemas.hathitrust.org/EF_Schema_MetadataSubSchema_v_3.0\nenumeration_chronology: 1977 inc., 1978, yr.1987-88, 1990, no.1-2 yr.1991, 1979, 1988, 1991, no.4-6 yr.1991, 1984, \n1980, 1987, 1991:1-4, 1983, 1981, 1989\ntype_of_resource: http://id.loc.gov/ontologies/bibframe/Text\ntitle: Asia and Africa today.\ndate_created: 20200209.0\npub_date: 1984.0, 1988.0, 1989.0, 1990.0, 1991.0, 1977.0, 1978.0, 1979.0, 1980.0, 1981.0, 1983.0\nlanguage: eng, und\naccess_profile: google\nissn: 0134-451X\nlccn: 80649602 //r85\noclc: 3825150.0\npage_count: 352.0, 640.0, 418.0, 416.0, 612.0, 424.0, 1256.0, 494.0, 208.0, 624.0, 312.0\nfeature_schema_version: https://schemas.hathitrust.org/EF_Schema_FeaturesSubSchema_v_3.0\naccess_rights: und, ic\ncategory: Asia\ngenre_ld: http://id.loc.gov/vocabulary/marcgt/doc\ngenre: ['document (computer)']\ncontributor_ld: [{'id': 'http://www.viaf.org/viaf/146036771', 'type': \n'http://id.loc.gov/ontologies/bibframe/Organization', 'name': 'Institut Afriki (Akademii͡a nauk SSSR)'}, {'id': \n'http://www.viaf.org/viaf/124863923', 'type': 'http://id.loc.gov/ontologies/bibframe/Organization', 'name': \n'Institut vostokovedenii͡a (Akademii͡a nauk SSSR)'}, {'id': 'http://www.viaf.org/viaf/152428799', 'type': \n'http://id.loc.gov/ontologies/bibframe/Organization', 'name': 'Sovetskiĭ komitet solidarnosti stran Azii i \nAfriki.'}]\ncontributor: ['Institut Afriki (Akademii͡a nauk SSSR)', 'Institut vostokovedenii͡a (Akademii͡a nauk SSSR)', 'Sovetskiĭ\nkomitet solidarnosti stran Azii i Afriki.']\nhandle_url: http://hdl.handle.net/2027/inu.30000123002549, http://hdl.handle.net/2027/uc1.b4938581, \nhttp://hdl.handle.net/2027/uc1.l0073938409, http://hdl.handle.net/2027/inu.30000123002556, \nhttp://hdl.handle.net/2027/uc1.b4938582, http://hdl.handle.net/2027/inu.30000028466609, \nhttp://hdl.handle.net/2027/inu.30000123002507, http://hdl.handle.net/2027/uc1.l0054919444, \nhttp://hdl.handle.net/2027/inu.30000004511980, http://hdl.handle.net/2027/uc1.l0073935298, \nhttp://hdl.handle.net/2027/uc1.c040585625, http://hdl.handle.net/2027/uc1.c050190445, \nhttp://hdl.handle.net/2027/inu.30000123002523, http://hdl.handle.net/2027/umn.31951p001472021, \nhttp://hdl.handle.net/2027/inu.30000123002499, http://hdl.handle.net/2027/inu.30000123002515\nsource_institution_ld: {'type': 'http://id.loc.gov/ontologies/bibframe/Organization', 'name': 'UMN'}, {'type': \n'http://id.loc.gov/ontologies/bibframe/Organization', 'name': 'UCLA'}, {'type': \n'http://id.loc.gov/ontologies/bibframe/Organization', 'name': 'UCBK'}, {'type': \n'http://id.loc.gov/ontologies/bibframe/Organization', 'name': 'INU'}, {'type': \n'http://id.loc.gov/ontologies/bibframe/Organization', 'name': 'NRLF'}\nsource_institution: NRLF, UMN, UCLA, INU, UCBK\nlcc: DS1.A4714\ntype: ['DataFeedItem', 'Book'], ['DataFeedItem', 'PublicationVolume']\nis_part_of: {'id': 'http://www.worldcat.org/oclc/3825150', 'type': 'CreativeWorkSeries', 'journalTitle': 'Asia and \nAfrica today.'}\nlast_rights_update_date: 20180416.0, 20181122.0, 20180520.0, 20181129.0, 20181228.0, 20171116.0, 20180409.0, \n20191004.0, 20181213.0\npub_place_ld: {'id': 'http://id.loc.gov/vocabulary/countries/rur', 'type': \n'http://id.loc.gov/ontologies/bibframe/Place', 'name': 'Russian S.F.S.R.'}, {'id': \n'http://id.loc.gov/vocabulary/countries/xx', 'type': 'http://id.loc.gov/ontologies/bibframe/Place', 'name': 'No \nplace, unknown, or undetermined'}\npub_place: Russian S.F.S.R., No place, unknown, or undetermined\nmain_entity_of_page: ['https://catalog.hathitrust.org/Record/008561767', \n'http://catalog.hathitrust.org/api/volumes/brief/oclc/3825150.json', \n'http://catalog.hathitrust.org/api/volumes/full/oclc/3825150.json'], \n['https://catalog.hathitrust.org/Record/010754220']\npublisher_ld: {'id': \n'http://catalogdata.library.illinois.edu/lod/entities/ProvisionActivityAgent/ht/Asia%20and%20Africa%20Today', \n'type': 'http://id.loc.gov/ontologies/bibframe/Organization', 'name': 'Asia and Africa Today'}\npublisher: Asia and Africa Today\nlowercase_periodical_name: asia_and_africa_today\npublication_directory: datasets/ht_ef_datasets/asia_and_africa_today\nvolume_directory: uc1_l0073935298, uc1_l0054919444, inu_30000123002556, inu_30000123002507, inu_30000004511980, \nuc1_l0073938409, uc1_c040585625, uc1_c050190445, uc1_b4938582, inu_30000123002515, uc1_b4938581, \ninu_30000123002499, inu_30000123002549, inu_30000028466609, inu_30000123002523, umn_31951p001472021\nht_bib_key: 8561767.0\npublishDates: 1900.0\n974_y: 1991.0\nlastUpdate: 20231202.0, 20240105.0, 20230825.0, 20240813.0, 20240209.0, 20230903.0, 20240509.0\nenumcron: 1977 inc., 1978, yr.1987-88, 1990, no.1-2 yr.1991, 1979, 1988, 1991, no.4-6 yr.1991, 1984, 1980, 1987, \n1991:1-4, 1983, 1981, 1989\nmin_year: 1977.0\nmax_year: 1991.0\nextracted_year: 1984.0, 1987.0, 1988.0, 1989.0, 1990.0, 1991.0, 1977.0, 1978.0, 1979.0, 1980.0, 1981.0, 1983.0\nfinalized_year: 1984.0, 1987.0, 1988.0, 1989.0, 1990.0, 1991.0, 1977.0, 1978.0, 1979.0, 1980.0, 1981.0, 1983.0\nprocessed_finalized_year: 1980-01-01, 1983-01-01, 1989-01-01, 1981-01-01, 1979-01-01, 1987-01-01, 1978-01-01, \n1991-01-01, 1984-01-01, 1977-01-01, 1988-01-01, 1990-01-01\nknowledge: {'publication': 'Asia and Africa Today', 'category': 'Newspaper', 'language': 'English', 'focus': \n'Current events, politics, business, and culture from Asia and Africa', 'circulation': 'Online only', 'frequency': \n'Daily', 'publisher': 'Institute for Global Governance', 'country': 'South Africa'}\nPlease create a brief human-readable description including title, author, publication date, and subjects.\n\n\n\nDescription response: {'model': 'llama3.2', 'created_at': '2024-11-14T18:40:46.015096Z', 'message': {'role': \n'assistant', 'content': \"Here is a brief human-readable description based on the provided metadata:\\n\\n**Title:** \nAsia and Africa Today\\n**Author:** Institute for Global Governance\\n**Publication Date:** [Assumed to be the \nearliest publication date mentioned in the data, which is 1900]\\n**Subjects:**\\n\\n* Current events from Asia and \nAfrica\\n* Politics\\n* Business\\n* Culture from Asia and Africa\\n\\nNote that the author's name is not explicitly \nstated in the metadata, so I had to infer it based on the publisher's name. If you have more accurate information \nabout the author, please let me know!\"}, 'done_reason': 'stop', 'done': True, 'total_duration': 19475862375, \n'load_duration': 42959375, 'prompt_eval_count': 1026, 'prompt_eval_duration': 9828037000, 'eval_count': 121, \n'eval_duration': 9589919000}\n\n\n\nError parsing JSON response for description.\n\n\n\nClassification: Information Bulletin\n\n\n\nActual classification: News/politics magazines\n\n\n\n\n\n\nProcessing periodical Azania news\n\n\n\nKnowledge check content: Based on this periodical name: Azania news, what do you know about this publication? \n\n\n\nKnowledge check response: {'model': 'llama3.2', 'created_at': '2024-11-14T18:40:54.488072Z', 'message': {'role': \n'assistant', 'content': '{\\'knowledge\\':\\n \"Azania News is a South African weekly newspaper that focuses on current\nevents, politics, business, and social issues affecting the African continent. Published by The Independent Media \nGroup in Johannesburg, it has been in circulation since 2002. It aims to provide an alternative perspective to \nmainstream media by highlighting underreported stories and investigative journalism.\"}'}, 'done_reason': 'stop', \n'done': True, 'total_duration': 5771230625, 'load_duration': 53678458, 'prompt_eval_count': 87, \n'prompt_eval_duration': 787066000, 'eval_count': 71, 'eval_duration': 4907559000}\n\n\n\nKnowledge: Azania News is a South African weekly newspaper that focuses on current events, politics, business, and \nsocial issues affecting the African continent. Published by The Independent Media Group in Johannesburg, it has \nbeen in circulation since 2002. It aims to provide an alternative perspective to mainstream media by highlighting \nunderreported stories and investigative journalism.\n\n\n\nDescription content: Here is combined data from multiple MARC records and HathiTrust data:\nperiodical_name: Azania news\n001: 2707548.0\nupdated_ht_bib_key: 002707548\nlink: https://babel.hathitrust.org/cgi/pt?id=inu.30000004998252, \nhttps://babel.hathitrust.org/cgi/pt?id=uiug.30112107803469, \nhttps://babel.hathitrust.org/cgi/pt?id=inu.30000046781955, \nhttps://babel.hathitrust.org/cgi/pt?id=inu.30000028478695, \nhttps://babel.hathitrust.org/cgi/pt?id=inu.30000028509796, \nhttps://babel.hathitrust.org/cgi/pt?id=inu.30000028509788, \nhttps://babel.hathitrust.org/cgi/pt?id=inu.30000028478687, \nhttps://babel.hathitrust.org/cgi/pt?id=inu.30000004998245, \nhttps://babel.hathitrust.org/cgi/pt?id=inu.30000028478703, \nhttps://babel.hathitrust.org/cgi/pt?id=inu.30000028478711, \nhttps://babel.hathitrust.org/cgi/pt?id=inu.30000028509804\nhtid: inu.30000028509796, uiug.30112107803469, inu.30000004998245, inu.30000046781955, inu.30000028478711, \ninu.30000028509804, inu.30000028478695, inu.30000028478703, inu.30000028478687, inu.30000028509788, \ninu.30000004998252\ndate: v.18,no.8 1982, v.8,no.7 1973, v.2,no.9 1967, v.3,no.15 1968, v.3,no.3-9 1968, v.18,no.11 1982, v.11:4-V.21:1\n(INCOMPL.), v.26,no.9 1991, v.3,no.11-12 1968, v.9-13 inc. 1974-1978, v.8,no.1-5 1973\noriginal_source: Indiana University, University of Illinois at Urbana-Champaign\nrecord_url: https://catalog.hathitrust.org/Record/002707548\nid: inu.30000028478687\nmetadata_schema_version: https://schemas.hathitrust.org/EF_Schema_MetadataSubSchema_v_3.0\nenumeration_chronology: v.2,no.9 1967\ntype_of_resource: http://id.loc.gov/ontologies/bibframe/Text\ntitle: Azania news.\ndate_created: 20200209.0\npub_date: 1967.0\nlanguage: eng\naccess_profile: google\nissn: 0572-2853\nlccn: sn 91028162\noclc: 2038783.0\npage_count: 80.0\nfeature_schema_version: https://schemas.hathitrust.org/EF_Schema_FeaturesSubSchema_v_3.0\naccess_rights: ic\ncategory: Africa\ngenre_ld: http://id.loc.gov/vocabulary/marcgt/doc\ngenre: ['document (computer)']\ncontributor_ld: [{'id': 'http://www.viaf.org/viaf/136322235', 'type': \n'http://id.loc.gov/ontologies/bibframe/Organization', 'name': 'Pan Africanist Congress of Azania. Revolutionary \nCommand.'}, {'id': 'http://www.viaf.org/viaf/131418472', 'type': \n'http://id.loc.gov/ontologies/bibframe/Organization', 'name': 'Pan Africanist Congress of Azania.'}]\ncontributor: ['Pan Africanist Congress of Azania. Revolutionary Command.', 'Pan Africanist Congress of Azania.']\nhandle_url: http://hdl.handle.net/2027/inu.30000028478687\nsource_institution_ld: {'type': 'http://id.loc.gov/ontologies/bibframe/Organization', 'name': 'INU'}\nsource_institution: INU\nlcc: DT1177.A92\ntype: ['DataFeedItem', 'PublicationVolume']\nis_part_of: {'id': 'http://www.worldcat.org/oclc/2038783', 'type': 'CreativeWorkSeries', 'journalTitle': 'Azania \nnews (Pan Africanist Congress of Azania)'}\nlast_rights_update_date: 20180823.0\npub_place_ld: {'id': 'http://id.loc.gov/vocabulary/countries/za', 'type': \n'http://id.loc.gov/ontologies/bibframe/Place', 'name': 'Zambia'}\npub_place: Zambia\nmain_entity_of_page: ['https://catalog.hathitrust.org/Record/002707548', \n'http://catalog.hathitrust.org/api/volumes/brief/oclc/2038783.json', \n'http://catalog.hathitrust.org/api/volumes/full/oclc/2038783.json']\npublisher_ld: {'id': \n'http://catalogdata.library.illinois.edu/lod/entities/ProvisionActivityAgent/ht/Pan%20Africanist%20Congress%20of%20\nAzania%20(S.A.)', 'type': 'http://id.loc.gov/ontologies/bibframe/Organization', 'name': 'Pan Africanist Congress of\nAzania (S.A.)'}\npublisher: Pan Africanist Congress of Azania (S.A.)\nlowercase_periodical_name: azania_news\npublication_directory: datasets/ht_ef_datasets/azania_news\nvolume_directory: inu_30000028478695, inu_30000028478711, inu_30000046781955, inu_30000004998245, \nuiug_30112107803469, inu_30000028478703, inu_30000028509788, inu_30000004998252, inu_30000028509796, \ninu_30000028509804, inu_30000028478687\nht_bib_key: 2707548.0\npublishDates: 1966.0\n974_y: 1967.0\nlastUpdate: 20240328.0, 20230331.0, 20240109.0, 20240222.0\nenumcron: v.18,no.8 1982, v.8,no.7 1973, v.2,no.9 1967, v.3,no.15 1968, v.3,no.3-9 1968, v.18,no.11 1982, \nv.11:4-V.21:1 (INCOMPL.), v.26,no.9 1991, v.3,no.11-12 1968, v.9-13 inc. 1974-1978, v.8,no.1-5 1973\nmin_year: 1967.0\nmax_year: 1991.0\nextracted_year: 1991.0, 1967.0, 1968.0, 1973.0, 1974.0, 1982.0\nfinalized_year: 1991.0, 1967.0, 1968.0, 1973.0, 1974.0, 1982.0\nprocessed_finalized_year: 1967-01-01, 1991-01-01, 1982-01-01, 1973-01-01, 1968-01-01, 1974-01-01\nknowledge: Azania News is a South African weekly newspaper that focuses on current events, politics, business, and \nsocial issues affecting the African continent. Published by The Independent Media Group in Johannesburg, it has \nbeen in circulation since 2002. It aims to provide an alternative perspective to mainstream media by highlighting \nunderreported stories and investigative journalism.\nPlease create a brief human-readable description including title, author, publication date, and subjects.\n\n\n\nDescription response: {'model': 'llama3.2', 'created_at': '2024-11-14T18:41:17.363398Z', 'message': {'role': \n'assistant', 'content': \"{'description': 'Azania news is a South African weekly newspaper focusing on current \nevents, politics, business, and social issues affecting the African continent.'}\"}, 'done_reason': 'stop', 'done': \nTrue, 'total_duration': 22538215375, 'load_duration': 42897583, 'prompt_eval_count': 1827, 'prompt_eval_duration': \n18423767000, 'eval_count': 32, 'eval_duration': 4064491000}\n\n\n\nDescription: Azania news is a South African weekly newspaper focusing on current events, politics, business, and \nsocial issues affecting the African continent.\n\n\n\nClassification: News & Politics Magazine\n\n\n\nActual classification: Information Bulletin"
  }
]